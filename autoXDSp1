#!/home/sdcsoftware/linux/tools/bin/perl -w

#The MIT License (MIT)

#Copyright (c) 2015 Qingping Xu

#Permission is hereby granted, free of charge, to any person obtaining a copy
#of this software and associated documentation files (the "Software"), to deal
#in the Software without restriction, including without limitation the rights
#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
#copies of the Software, and to permit persons to whom the Software is
#furnished to do so, subject to the following conditions:

#The above copyright notice and this permission notice shall be included in
#all copies or substantial portions of the Software.

#THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
#THE SOFTWARE.
#
#
#
# (C) Qingping Xu @ Stanford Synchrotron Radiation Laboratory Stanford University
#
# bug report should be sent to qxu@slac.stanford.edu
# 
# version history
#    0.1a 03/30/04 working prototype to integration
#    0.1b 04/02/04 working prototype to XSCALE.INP
#    0.1c 04/02/04 fix data structure for easier futher development
#                  added subroutine for calculate resolution shells for xscale
#    0.1d 04/04/04 added XSCALE and XDSCONV subroutines and fixed other bugs
#    0.1e 04/05/04 added shelx stuff
#    0.1f 04/06/04 added user input support and sequence parser
#    0.1g 04/09/04 finished adding autoSHARP/SHELX interfaces
#         important issues: 1) get the best data; 
#                           2) select best data for structure determination
#    0.2  04/11/04 many revisions
#    0.3  04/18/04 adding jcsg upload conversions: using scala
#    0.4  04/21/04 changed how the spot ranges are defined
#    0.5  04/23/04 uploaded scaling log parser, 
#         todo: need a way to deal with enantimorph selection
#    0.5a 04/25/04 turn off 0-dose extraction (probably no good for MAD purpose), 
#                  increase shelx resolution search radius
#    0.5b 04/26/04 added integration in p1; now the old way of doing things is -slow
#                  fixed bug in parseSeq so that it can deal with image numbers >999
#                  added -XDScommands -highres -noanomalous flag
#         todo: how to reject bad data in MAD phasing? 
#    0.5c 05/17/04 fixed a nasty bug in parseSeq        
#    0.5d 05/19/04 1. global variables so that certain parameters can be defined by users
#                  2. user can now modify the collect information for each run: COLLECT.NFO.
#         todo: how to recover in case of failure?
#    0.5e 05/31/04 added simple recovery mechanism         
#                  added RESOLVE functions (based on autoSHARP results)
#    0.5f 06/01/04 changed script name to autoXDS from XDSauto
#    0.5g 06/03/04 fixed a few bugs
#         issue: how to deal with cases with lots of sites (tm0077 ~72 sites)
#    0.6a 06/29/04 need to fix jcsg upload to use the correct space group for MAD data
#    0.6b  07/14/04 added second pass of CORRECT after INTEGRATION to avoid cutting resolution to 
#                  aggressively 
#          08/06/04 Note: 1). in some cases, it seems use all three wavelengths for shelxd 
#                             hurts shelxd signal, need to try PEAK,INFL-REMO,PEAK-REMO combo
#                         2). need to deal with spread multiple parallel process across cpus
#    0.7a 04/06/05 added irp and dm/warp functionality; replaced rotaprep with combat; other changes
#    0.7b 04/08    major update, added support for p222*, data conversion change, start data harvesting etc
#    0.7c 04/10    add code to analys shelxd site occupancy distribution, other small changes
#    0.7d 04/14/05 modify create XDS to allow highest resol being modified
#                     this deals with integration in p1 often crashes due to not enough memory
#                  Need to do: consider systematic absence; try infl-remo kind of combination
#    0.7e 05/06/05 use shelxd results to recalculate #nmol asu and solvent content
#         06/06/05 support parsing data/images from multiple directories
#    0.7f 10/13/05 adjusted nomerge option since XDS changed its format for no merge (added one column)
#    0.7g 01/05/06 modified shelx section to take advantage of new features
#    0.8a 02/20/06 parallelize autoXDS with Forkmanager and LSF batch system
#         02/22/06 Need to do: need take care rerun xscale in case reindex was carried out in later stages
#                              eg in p22121 etc cases
#    0.8b 04/21/06 added XML support and xsolve CollectData.xml hookup for STSS upload
#    0.8c 11/06/09 added parrot & buccaneer tracing
#    0.8d 09/01/10 a version for execution on a single machine
#    0.8e 06/24/11 added tracing using phenix.autobuild
#
    
use strict;
use Data::Dumper;
use File::Path;
use File::Copy;
use File::Find;
use Getopt::Long;
use Parallel::ForkManager;
use LWP::Simple;
use XML::Simple;
use Net::SMTP;
use DBI;
use Cwd;

my $version='autoXDS 0.8e June-24-2011';

# define some constants, these are global varibles!  # NOTE: no need to modify in most cases
my $NPROCESSOR=6; # xds_par broken for marccd325, so use 1 in xds but use 2 in xscale; how many processors on each machine to use? 
my $NMACHINES=1; # how many machines to distribute jobs
my $NMACHINES2=6; # how many machines to distribute jobs
my $SIGMAMAD=1.5; # i/sigma I cutoff for MAD dataset 
my $RESOL4SPG=3.0; # defined resolution for determine spg
my $RESOL4SPG2=3.0; # defined resolution for determine spg
my $CUTOFF_CORRELATION_MAD = 0.90; # cc=0.9 define acceptable correlation for a run to be included into MAD set
my $REINDEX_CORRELATION = 0.90; # cc=0.9 to reference data, decide whether reindex is needed.
my $CUTOFF_RMERGE_MAD = 22.0; # define acceptable Rmerge for a run to be included into MAD set
my $CUTOFF_RSYM = 20.0; # percent, define the Rsym/Rmerge cutoff to accept spg with data from 30A - $RESOL4SPG
my $SHELXD_CUTOFF_MAD = 40.0; # percent, CC for SHELXD cutoff for MAD case
my $SHELXD_CUTOFF_SAD = 35.0; # percent, CC for SHELXD cutoff for SAD case
my $ANOMALOUS =1; # assume anomalous by default
my $SHELXD_CYCLES=100;
my $SHELXE_CYCLES=20;
my $SHELXE_FLA_CYCLES=400;
my $MINIMAGESPERRUN=15; # minimum images per run, runs with images less than that will be ignored
my $MINIMAGESFORINDEX=20; # minimum images per run, runs with images less than that will be ignored
my $SPOTRANGE=3.0;      # degree, spot range is 3 degree of data
my $CUTOFF_LOWRESOL=50.0; # low resolution cutoff
# non essential, used to keep track protein/xtal
my $PROJECT = 'PRJNAME'; # non essential, used to keep track protein/xtal
my $SLEEP=1; # in seconds, how long to wait between job submission. A walkaround feature for SLAC LSF 

# handle user interupt
$SIG{INT} = \&STOP; 

# define location of programs used
my $ostype=$ENV{OSTYPE};
my $PROGROOT = "/home/sdcsoftware";
# XDS programs
my $XDS=     "$PROGROOT/$ostype/XDS-cluster/xds_par"; checkprog($XDS,"XDS");
my $XDSPAR=     "$PROGROOT/$ostype/XDS-cluster/xds_par"; checkprog($XDSPAR,"xds_par");
#my $XDSCONV= "$PROGROOT/$ostype/XDS/-clusterxdsconv";
my $XSCALE=  "$PROGROOT/$ostype/XDS-cluster/xscale_par";
# home cooked image header dumper
my $IMGHEADER= "/home/sdcsoftware/linux/bin/imgheader_lnx"; checkprog($IMGHEADER,"imgheader");
# SHELX programs
my $SHELXC=  "$PROGROOT/$ostype/SHELX/shelxc"; checkprog($SHELXC,"shelxc");
my $SHELXD=  "$PROGROOT/$ostype/SHELX/shelxd";
my $SHELXE=  "$PROGROOT/$ostype/SHELX/shelxe";
# CCP4 programs
#my $TRUNCATE=`which truncate`; chop $TRUNCATE; checkprog($TRUNCATE,"truncate");
my $TRUNCATE="/home/sdcsoftware/linux/ccp4-6.2.0/bin/truncate";
#my $SCALA=`whch scala`; chop $SCALA; checkprog($SCALA,"scala");
my $SCALA="/home/sdcsoftware/linux/ccp4-6.2.0/bin/scala";
#my $SORTMTZ=`which sortmtz`; chop $SORTMTZ; checkprog($SORTMTZ,"sortmtz");
my $SORTMTZ="/home/sdcsoftware/linux/ccp4-6.2.0/bin/sortmtz";
#my $COMBAT=`which combat`; chop $COMBAT; checkprog($COMBAT,"combat");
my $COMBAT="/home/sdcsoftware/linux/ccp4-6.2.0/bin/combat";
#my $MATTHEWS_COEF=`which matthews_coef`; chop $MATTHEWS_COEF;
my $MATTHEWS_COEF="/home/sdcsoftware/linux/ccp4-6.2.0/bin/matthews_coef";
#my $F2MTZ=`which f2mtz`; chop $F2MTZ;
my $F2MTZ="/home/sdcsoftware/linux/ccp4-6.2.0/bin/f2mtz";
#my $MTZ2VARIOUS=`which mtz2various`; chop $MTZ2VARIOUS;
my $MTZ2VARIOUS="/home/sdcsoftware/linux/ccp4-6.2.0/bin/mtz2various";
#my $REINDEX=`which reindex`; chop $REINDEX;
my $REINDEX="/home/sdcsoftware/linux/ccp4-6.2.0/bin/reindex";
#my $DMX = `which dm`; chop $DMX; checkprog($DMX,"dm");
my $DMX = "/home/sdcsoftware/linux/ccp4-6.2.0/bin/dm";
# RESOLVE needed
my $RESOLVE='/home/sdcsoftware/linux/solve/bin/resolve_huge';  checkprog($RESOLVE,"resolve");
my $IRPSCRIPT='/home/sdcsoftware/linux/jcsgscripts/resolve_build.csh';
my $PARROT='/home/sdcsoftware/linux/ccp4-6.2.0/bin/cparrot';
my $BUCCANEERSCRIPT='/home/sdcsoftware/linux/ccp4-6.2.0/bin/buccaneer_pipeline';
#my $ARP = `which auto_warp.sh`; chop $ARP; checkprog($ARP,"auto_warp.sh");
my $ARP = '/home/sdcsoftware/linux/arp_warp_7.2/bin/bin-x86_64-Linux/auto_tracing.sh'; #chop $ARP; checkprog($ARP,"auto_tracing.sh");
my $PHENIXAUTOBUILD = '/home/sdcsoftware/linux/phenix-1.7.2-869/build/intel-linux-2.6-x86_64/bin/phenix.autobuild';
# SHARP installation
my $BDG_home='/home/sdcsoftware/linux/SHARP2009/';
$ENV{'BDG_home'} = '/home/sdcsoftware/linux/SHARP2009/';
if  ($ostype eq 'osf1') { 
	print "Warning: running autoSHARP run blcpu's is not tested.\n\n";
	$BDG_home="/home/sharp"; 
} 
my $MACHINE = ''; # machine to run autoSHARP on
my $PDB_EXTRACT='/home/sdcsoftware/linux/ccp4-6.2.0/bin/pdb_extract';
my $POINTLESS='/home/sdcsoftware/linux/ccp4-6.2.0/bin/pointless';
my $CAD='/home/sdcsoftware/linux/ccp4-6.2.0/bin/cad';

my $start_time=time();

#print "\n\nIntegration/Scaling programs used: XDS/XSCALE (version Dec-2003) by Wolfgang Kabsch.\n";
print "autoXDS $version by Qingping Xu\n\n";
COLLECTINFO:
# Parse user input
my $href_userInputs=getUserInputs();
# set number of parallel processes: # of cpu to be used
if ( exists $href_userInputs->{parallel} ) { $NMACHINES=$href_userInputs->{parallel}; } 
if ( exists $href_userInputs->{project} ) { $PROJECT=$href_userInputs->{project}; } 
if ( exists $href_userInputs->{rsym} ) { $CUTOFF_RSYM=$href_userInputs->{rsym}; } 
if ( exists $href_userInputs->{CCreindex} ) { $REINDEX_CORRELATION=$href_userInputs->{CCreindex}; } 
if ( exists $href_userInputs->{CCmad} ) { $SHELXD_CUTOFF_MAD=$href_userInputs->{CCmad}; } 
if ( exists $href_userInputs->{CCsad} ) { $SHELXD_CUTOFF_SAD=$href_userInputs->{CCsad}; } 
# print help
if ( exists $href_userInputs->{help} ) { usage(); exit(0); }

# reading xsolve CollectData.xml
my $XMLDATA; 
if ( exists $href_userInputs->{xsolve} )  { #path to xsolve_directory
   my $xml = new XML::Simple;
   my $xsolvepath=$href_userInputs->{xsolve};
   my $collectdataxml="$xsolvepath/bic/collect/CollectData.xml";
   if (! -e $collectdataxml) {
     print "File $collectdataxml does not exist.\n";
     exit;
   }
   $XMLDATA = $xml->XMLin($collectdataxml,KeepRoot => 1, forcearray => [ 'Data' ]);
   
   #my @p=split /\//,$collectdataxml;
   #pop(@p); pop(@p); pop(@p); # jcsg specific
   #my $seqpath=join("/",@p);
   #my $seqxml="$seqpath/target.xml";
   my $seqxml="$xsolvepath/target.xml";
   if(! -e $seqxml) {
       print "File $seqxml does not exist. Check Xsolve.\n";
       exit;
   }

   my $TARGETXML=$xml->XMLin($seqxml,KeepRoot => 1);
   my $seq= $TARGETXML->{TARGET}->{SEQUENCE};
   open(SEQ,">seq.dat");
   print SEQ $seq,"\n";
   close(SEQ);
}

# data structure, all reference to more complex ds
my $href_target;
my $aref_collect;
my $href_dataset;
my $href_MADdataset;
my $href_sharpPhaseSet;

if ( exists $href_userInputs->{help} ) {
   usage();
}

my $dir_info=cwd."/info"; # global variable, directory to save processing information
if (! -e $dir_info) {
   mkpath($dir_info);
}

print "\n\nExtracting sequence information...\n";
# get sequence information for protein
$href_target=updateTarget($href_userInputs);

if ( exists $href_userInputs->{recover} ) {
  if (-e "$dir_info/TARGET.NFO") {
    $href_target=do "$dir_info/TARGET.NFO" or die "can't recreate href_target: $! $@";
  }

  if ( -e "$dir_info/PHASE.NFO" ) {
    print "Note: recovering at TRACING STEP.\n";
    $href_dataset=do "$dir_info/DATASETS.NFO" or die "can't recreate href_dataset: $! $@";
    $aref_collect=do "$dir_info/INTEG.NFO" or die "can't recreate aref_collect: $! $@";
    $href_MADdataset=do "$dir_info/MAD.NFO" or die "can't recreate href_MADdataset: $! $@";
    $href_sharpPhaseSet=do "$dir_info/PHASE.NFO" or die "can't recreate href_sharpPhaseSet: $! $@";
    goto TRACING;
  }

  if ( -e "$dir_info/MAD.NFO" ) {
    print "Note: recovering at TRACING STEP.\n";
    $href_dataset=do "$dir_info/DATASETS.NFO" or die "can't recreate href_dataset: $! $@";
    $aref_collect=do "$dir_info/INTEG.NFO" or die "can't recreate aref_collect: $! $@";
    $href_MADdataset=do "$dir_info/MAD.NFO" or die "can't recreate href_MADdataset: $! $@";
    goto SHARP;
  }

  if ( -e "$dir_info/DATASETS.NFO" ) {
    print "Note: recovering at MAD PHASING.\n";
    $href_dataset=do "$dir_info/DATASETS.NFO" or die "can't recreate href_dataset: $! $@";
    $aref_collect=do "$dir_info/INTEG.NFO" or die "can't recreate aref_collect: $! $@";
    goto PHASING;
  }

  if (-e "$dir_info/INTEG.NFO") {
    print "Note: recovering at SCALING.\n";
    $aref_collect=do "$dir_info/INTEG.NFO" or die "can't recreate aref_collect: $! $@";
    goto SCALING;
  }

  if ( -e "$dir_info/COLLECT.NFO") {
    print "NOTE: File COLLECT.NFO exists, will use this file instead to run XDS.\n";
    # read in user modified run info
    $aref_collect=do "$dir_info/COLLECT.NFO" or die "can't recreate aref_collect: $! $@";
    goto INTEGRATION;
  }

}

###########################
# Data processing section
# #########################
if ( exists $href_userInputs->{xsolve} ) { 
  if ( -e "./images" ) {
     print "Warning: directory ./images already exists, -xsolve is not needed.\n"; 
     rmtree("./images");
  }
  my $directory = $XMLDATA->{CollectData}->{ImageDirectory};
  my $ok=createLinks($directory);
  if (!$ok) {
     print "Failed to create images links.\n";
     print "Please check where you directory exists, readable to me. Aborting.\n";
     exit(1);
  }
  # guess project name from full path, assuming last two path are target name and xtal_id
  if ($PROJECT eq 'PRJNAME') {
     my @tmparr=split(/\//,$directory);
     if ($tmparr[-1] eq '') { 
       pop @tmparr;
       $PROJECT=$tmparr[-2];
     } else {
       $PROJECT=$tmparr[-2];
     }
  }
} elsif ( exists $href_userInputs->{data} ) {
  if ( -e "./images" ) {
     print "Warning: directory ./images already exists, -data is not needed.\n"; 
  }
  #my $directory = $href_userInputs->{data};
  my $href_dir=$href_userInputs->{data};
  print "Creating sym links to images to ./images directory ...\n";
  foreach my $directory (@$href_dir) {
    my $ok=createLinks($directory);
    if (!$ok) {
       print "Failed to create images links.\n";
       print "Please check where you directory exists, readable to me. Aborting.\n";
       exit(1);
    }
   
    # guess project name from full path, assuming last two path are target name and xtal_id
    if ($PROJECT eq 'PRJNAME') {
       my @tmparr=split(/\//,$directory);
       if ($tmparr[-1] eq '') { 
         pop @tmparr;
         $PROJECT=$tmparr[-2];
       } else {
         $PROJECT=$tmparr[-2];
       }
    }

  }

} elsif (! is_empty_or_notexist("./images") ) {
     print "Will process images in ./images directory.\n";
} else {
     usage();
     exit(0);
}

#get sequence information for protein
#$href_target=updateTarget($href_userInputs);
################################################

# parse images dir and figure out run information
print "\n\n>>1.<<Parse images and headers to determine parameters.\n";
$aref_collect=parseSeq(); 

# using run informaton collected above parse the header
# and update run information, now all information needed
# can are stored in a array of hash which will be later
# by following steps
$aref_collect=parseImgHeader($aref_collect);
printSummary($aref_collect);

# let user to specify their prior knowledge
$aref_collect=updateCollectionfromUserInput1($aref_collect,$href_userInputs);
$aref_collect=updateCollectionfromUserInput2($aref_collect,$href_userInputs);
# combine CollectData.xml information into aref_collect
if ( exists $href_userInputs->{xsolve} ) {
   $aref_collect=updateCollectionfromCollectDataXML($aref_collect, $XMLDATA);
}

print "generating input file for Xia2...\n";
createXIA2($aref_collect,$href_target);

if ( exists $href_userInputs->{collectinfo} ) { 
    open(COLLECT,">$dir_info/COLLECT.NFO"); print COLLECT Dumper $aref_collect; close(COLLECT);
    createXDS($aref_collect);
    exit(0); 
}

my $end_time=time();
print "autoXDS time used (finished parsing):  ",$end_time-$start_time, " seconds.\n\n";


INTEGRATION:
if ( ! exists $href_userInputs->{fast} ) { # planA: do things regularly
  print "\n\n>>2.<<index and space group determination.\n";
  print     "  NOTE: XDS will sometimes fail/die here due to memory limitation.\n";
  print     "        If this happens: wait until the machine is less loaded, or find \n";
  print     "        a machine with more memory, or decrease number of parallel processing used.\n\n";
  my $success=runIndex1($aref_collect);
  $end_time=time();
  print "autoXDS time used (finished indexing & Laue group determination):  ",$end_time-$start_time, " seconds.\n\n";
  print "\n\n>>4.<<Integration.\n";
  runInteg($aref_collect);
  $end_time=time();
  print "autoXDS time used (finished integration):  ",$end_time-$start_time, " seconds.\n\n";
} else { # experimental use p1 integration for everything, need to make sure it works
  print "\n\n>>3.<<Will integrate everything in reduced cell in spg p1.\n";
  print "Note: then figure out space group and run CORRECT in new space group.\n";
  runIntegP1($aref_collect);
  $end_time=time();
  print "autoXDS time used (finished integration in p1):  ",$end_time-$start_time, " seconds.\n\n";
}

print "\n\n>>5.<<Reindex to insure everything can be scale together.\n";
runReindex($aref_collect);
print "\n\n>>6.<<Calcualte Vm, assigning fp and fpp and Dumping integration summary.\n\n";
$aref_collect=updateCollectionfromUserInput3($aref_collect,$href_userInputs,$href_target);
# save integration information and target information for restart
open(SAVEINTEG,">$dir_info/INTEG.NFO"); print SAVEINTEG Dumper $aref_collect; close(SAVEINTEG);
open(SAVETARGET,">$dir_info/TARGET.NFO"); print SAVETARGET Dumper $href_target; close(SAVETARGET);
$end_time=time();
print "autoXDS time used (finished reindex):  ",$end_time-$start_time, " seconds.\n\n";

SCALING: 
print "\n\n>>7.<<Calculate average unit cell and generating XSCALE.INP files for each wavelength.\n";
if ( exists $href_userInputs->{anomalous} ) {
   $ANOMALOUS=$href_userInputs->{anomalous};
}
$href_dataset=createXSCALE($aref_collect);
runXSCALE($href_dataset);
$end_time=time();
print "autoXDS time used (finished scaling):  ",$end_time-$start_time, " seconds.\n\n";

open(SAVEDATA,">$dir_info/DATASETS.NFO"); print SAVEDATA Dumper $href_dataset; close(SAVEDATA);
if (! $ANOMALOUS) { 
  print "\n\n>>8.<<Prepare native data for upload....\n";
  createSummary($href_dataset,$href_MADdataset,$href_target);
  $end_time=time();
  print "autoXDS time used (finished data summary):  ",$end_time-$start_time, " seconds.\n\n";
  exit(0);
}

PHASING:
# only do following task if it anomalous data
print "\n\n>>8.<<Now scaling across wavelength and generating files for following phasing step.\n";
my $maxres;
($href_MADdataset, $maxres)=createMADXSCALE($aref_collect);
createMADXDSCONV2($href_MADdataset,0); # convert the resulting reflections files to various format for following steps
$end_time=time();
print "autoXDS time used (finished MAD scaling):  ",$end_time-$start_time, " seconds.\n\n";

###########################
# SHELX section
###########################
print "\n\n>>9.<<run SHELX C/D/E.\n";
$href_sharpPhaseSet=createPhaseSet();
 
# if default don't work, try resolution systematically
my $sitefound=0;
my $shelxdres=$maxres+0.5; # default shelxd resolution
if($shelxdres <=2.0) { $shelxdres=2.0; } # don't look for HA below 2.0A 
if (!$sitefound) {
    while ($shelxdres<=5.5) {
      print "\n\nRun SHELX at resolution $shelxdres A ...\n";
      $sitefound=runSHELX($href_MADdataset,$shelxdres,$href_sharpPhaseSet);
      if ($sitefound) { 
        print "Site found at $shelxdres!\n"; 
        last; 
      } 
      $shelxdres=$shelxdres+0.5; # search resolution range with 0.5A step
    }  
}
open(SAVEMAD,">$dir_info/MAD.NFO"); print SAVEMAD Dumper $href_MADdataset; close(SAVEMAD);

$end_time=time();
print "autoXDS time used (finished shelx):  ",$end_time-$start_time, " seconds.\n\n";
if ( lc($href_userInputs->{stopafter}) eq 'shelx') {  
    print "Stopped after SHELX.\n";
    exit(0);
} 

SHARP:
###########################
# SHARP section
###########################
print "\n\n>>10.<<Create autoSHARP files and run it.\n";
## rerun conversion procedure for MAD scaled data for following steps
#  update space group and reindex
createMADXDSCONV2($href_MADdataset,1); 

createAutoSHARP($href_MADdataset,$href_target,$maxres);
# sharp result:
find(\&FinalSharpDir,cwd); # update SHARP results

open(PHASE,">$dir_info/PHASE.NFO"); print PHASE Dumper $href_sharpPhaseSet; close(PHASE);
$end_time=time();
print "autoXDS time used (finished sharp):  ",$end_time-$start_time, " seconds.\n\n";

TRACING:

#####################################
# RESOLVE / DM AND WARP
#    all dependent on sharp phases
#####################################

print "\n\n>>11.<<Iterative Resolve building on Sharp experimental map (VERY SLOW!!).\n";
print     "       DM and wARP on Sharp experimental map (faster).\n";
print     " NOTE: Both jobs will be run in background.\n\n";
createResolveWarpBuccaneer($href_sharpPhaseSet,$href_target,$href_MADdataset);

# insert data into sdc processing db
if ( exists $href_userInputs->{xsolve} )  { #path to xsolve_directory
   my $startxml=$href_userInputs->{xsolve}."/start1.xml";
   if(! -e $startxml) {
       print "File $startxml does not exist. Check Xsolve.\n";
       exit;
   }
    
   my $xml = new XML::Simple;
   my $STARTXML=$xml->XMLin($startxml,KeepRoot => 1);
   my $dataid= $STARTXML->{start}->{param}->{datasetID};
   updateSDCPROC($dataid, $href_sharpPhaseSet,$href_target,$href_MADdataset);
}

#################################
# Create Summaries  and uploads
#################################
createSummary($href_dataset,$href_MADdataset,$href_target);

#print "Find and gzip INTEGRATE.HKL:\n";
#find(sub { m/^INTEGRATE\.HKL$/ and print "gzip $File::Find::name\n" and system("gzip $File::Find::name")},cwd);
# generate mmcif files for eventual pdb deposition and tables for publication etc
createSTSS($href_dataset,$href_MADdataset,$href_sharpPhaseSet,$href_target); 

$end_time=time();
print "\n\nGood Luck! End of autoXDS session.\n";
print "autoXDS session finished in ",$end_time-$start_time, " seconds.\n\n";

exit(0);

###########################
# END OF MAIN PROGRAM
###########################

## subroutine doing various task starting here

sub parseSeq {
    # figure out how the run is defined based on images names in ./images

    my ($name, $dir, %hash, @list, @runlist, @run, @num, @seq, @ext,@a,$el);
    my ($nrun, $nimg, $nimg_per_run);
    my @a_collect; # an array of hash to store collect information
    
    $dir = "./images";
    my $imagepatten = '(\d{3,4}\.)|(\.\d{3,4})';
    
    print "parsing images in $dir now ...\n";
    
    opendir(DATA, $dir) || die "Error: cannot open directory $dir.\n";
    
    while ( $name = readdir(DATA))  {
       if ( $name =~ m/$imagepatten/ ) { push(@list,$name); }
    }
    closedir(DATA);
   
    # sort the filenames so that the following process would be easier
    # my @flist = sort  @list; # sort file list, 
    # this cannot deal with cases in which numbers >999, ex. 'T2732_1_999.img', 'T2732_1_1000.img'
    # sort by length of filename, then alphabetically
    my @flist = sort { length($a) <=> length($b) || $a cmp $b } @list;

    foreach $el (@flist) {
        $el =~ m/$imagepatten/;
        push(@run, $`);
        push(@num, $&);
        push(@ext, $');  
        # use a temporary hash, use run as key, as a result, the last digit 
        # of each run will be assigned to the hash
        $hash{$`} = $&; 
    }
    
    my $minnum=length($num[0])-1;
    my $maxnum=length($num[-1])-1;
    foreach $el (@num) {
      my $tmp = $el;
      $tmp  =~ s/\.//g;
      push @seq, sprintf ("%0$maxnum.0f", $tmp);
    }

    if ( $maxnum != $minnum )  {
       print "Warning: images file names are not uniform. Will try to fix it.\n";
       my $nfile=@flist;
       for (my $i=0;$i<$nfile; $i++) {
          my $newnum=$seq[$i].".";
          if( $num[$i] ne $newnum ) {
            rename($dir."/".$run[$i].$num[$i].$ext[$i],$dir."/".$run[$i].$newnum.$ext[$i]);
            $num[$i] = $newnum;  # update name
          }
       }
    } 

    # figure out how many runs is present in the images
    @runlist= sort ( keys %hash);
    
    $nrun = scalar (@runlist);
    $nimg = scalar (@run);

    my $endposition = 0;
    for(my $i=0;$i<$nrun;$i++)  {
       my $nimg_per_run = 0;
       for(my $j=0;$j<$nimg;$j++)  {
          if ( $run[$j] eq $runlist[$i])  {
             $nimg_per_run = $nimg_per_run + 1;
             $endposition = $j;
          }
       } 
      
       # check whether there are breaks in a run
       my $start= $endposition-$nimg_per_run+1;
       my $end= $endposition;

       my @a=();
       push(@a,$start);
       for(my $k=$start; $k<$end;$k++) {
         if($seq[$k]+1 != $seq[$k+1]) {
            push(@a,$k);
            push(@a,$k+1);
         }
       }
       push(@a,$end);
       #### 
       # values: @a store first and end numbers in each segment
       $hash{$runlist[$i]} = \@a;
    }

    # save all run information into an array of hashes
    my $totalruns=0;
    foreach $el (@runlist) {
	my $ref= $hash{$el}; # reference to the array stored the sweeps is retrieved
	my @a=@$ref;          # the content is assigned to a new array
        my $nsweeps= scalar(@a)/2;
       
        #guess crystalid from image names
        my @tempnames=split(/[\_]/,$el);
        my $crystalid=$tempnames[0];

        my $imageprefix=substr($el,0,length($el)-1); # image prefix, ignore last separator character such as _,- etc

        for(my $i=0; $i<$nsweeps;$i++) {
           my $st = shift(@a); # start of sweep
           my $en = shift(@a); # end of sweep
           my $totalimg = $en-$st+1;
           
           # next if ($totalimg <10); # ignore the run if the total # images is less than 10
           $totalruns=$totalruns+1;
           # figuring out template for file names
           my $temp = $num[$st];
           #$temp =~ s/\d/\#/g;
           $temp =~ s/\d/\?/g;
           my $template=$el.$temp.$ext[$st]; 
           
           # first and last image for the run
           my $first_img= $run[$st].$num[$st].$ext[$st];
           my $last_img = $run[$en].$num[$en].$ext[$en];
           #save into an array of hash
           # creating a hash for each run
           my $href=createRun();
           $href->{directory}=$dir;
           $href->{template}=$template;
           $href->{first_image}=$first_img;
           $href->{last_image}=$last_img;
           $href->{startseq}=$seq[$st];
           $href->{endseq}=$seq[$en];
           $href->{totalimages}=$totalimg;
           $href->{crystalid}=$crystalid;
           $href->{imgprefix}=$imageprefix; 
           push @a_collect,  $href;
        }
    } # end of writing run information to file

    # sort @a_collect so that good ones are listed first
    my @sorted_collect = sort { $a->{first_image} cmp $b->{first_image} } @a_collect;
    # print Dumper \@sorted_collect;
    return \@sorted_collect;

    # return the reference to array of hash
    # return \@a_collect;
} # end of sub seqparser

sub parseImgHeader {
    my $aref_collect=$_[0];
  
    # extract beam position when 2theta=0 and store them in $beamx0 and $beamy0, these values 
    # stay the same for every run, this is for XDS 
    my $beamx0=0;
    my $beamy0=0;
    foreach my $href (@{$aref_collect}) {
        next if ( $href->{process}==0);
        my $dire=$href->{directory};
        my $fimg=$href->{first_image};

        my $infodir=$href->{dir_info};
        my $header=dumpHeader($dire."/".$fimg, "$infodir/$fimg.header");

        my $pixel_size = ImgHeaderSearch($header, "PIXEL_SIZE=");
        my $twotheta = ImgHeaderSearch($header, "TWOTHETA=");
        my $sizex =ImgHeaderSearch($header, "SIZE1=");
 
        if($twotheta < 0.1) {
          # ALS beam center
          my $beamx= ImgHeaderSearch($header, "DENZO.*X.*BEAM=");
          my $beamy= ImgHeaderSearch($header, "DENZO.*Y.*BEAM=");
          #
          if ( ! $beamx ) {
             # $x,$y are beam position for ADXV
             # for denzo/mosflm: beamx=width-$y beamy=$x
             my $x= ImgHeaderSearch($header, "BEAM_CENTER_X=");
             my $y= ImgHeaderSearch($header, "BEAM_CENTER_Y=");
             $beamx= $sizex*$pixel_size - $y;
             $beamy= $x;
             ## for SSRL since $x and $y are always @ the center
             ##    so it does not matter
          }
          $beamx0=$beamx;
          $beamy0=$beamy;
          last; # found what we need and quit the loop
          # please notice we keep x y order here (MOSFLM/DENZO), 
          # for XDS they need to be swapped
        }
    }

    # check that beamx0 and beamy0 are not nonsense
    if ($beamx0+$beamy0 < 10.0) {
       print "ERROR: needs direct beam position when 2theta=0 in order to continue.\n";
       exit(1);
    }

    # now process every run
    my $irun=0;
    foreach my $href (@{$aref_collect}) {
        $irun++;
        my $dire=$href->{directory};
        my $fimg=$href->{first_image};
        my $temp=$href->{template};
        my $limg=$href->{last_image};
        my $sseq=$href->{startseq};
        my $eseq=$href->{endseq};
        my $nimg=$href->{totalimages};

        # Dumping Header file for each sweep to an ascii file
        my $infodir=$href->{dir_info};
        my $header=dumpHeader($dire."/".$fimg, "$infodir/$fimg.header");

        my $distance = ImgHeaderSearch($header, "DISTANCE=");
        my $wavelength = ImgHeaderSearch($header, "WAVELENGTH=");
        my $twotheta = ImgHeaderSearch($header, "TWOTHETA=");
        if (! $twotheta ) { $twotheta='0.0'; }
        my $osc_range = ImgHeaderSearch($header, "OSC_RANGE=");
        my $osc_start = ImgHeaderSearch($header, "PHI=");
        my $pixel_size = ImgHeaderSearch($header, "PIXEL_SIZE=");
        #
        my $detector= ImgHeaderSearch($header,"DETECTOR=");
        my $sizex =ImgHeaderSearch($header, "SIZE1=");
        my $sizey =ImgHeaderSearch($header, "SIZE2=");

        # ALS beam center
        my $beamx= ImgHeaderSearch($header, "DENZO.*X.*BEAM=");
        my $beamy= ImgHeaderSearch($header, "DENZO.*Y.*BEAM=");
        #
        if ( ! $beamx ) {
           # $x,$y are beam position for ADXV
           # for denzo/mosflm: beamx=width-$y beamy=$x
           my $x= ImgHeaderSearch($header, "BEAM_CENTER_X=");
           my $y= ImgHeaderSearch($header, "BEAM_CENTER_Y=");
           $beamx= $sizex*$pixel_size - $y;
           $beamy= $x;
           ## for SSRL since $x and $y are always @ the center 
           ##    so it does not matter 
        }

        if ( exists $href_userInputs->{beam} ) { # if user defined his own beam center, use it anyway
           my @beamxy=split /,|\s+/, $href_userInputs->{beam};
           $beamx=$beamxy[0];
           $beamy=$beamxy[1];
        }

        if ( exists $href_userInputs->{gmcat} ) { # APS GMCAT ID 23
           my $x= ImgHeaderSearch($header, "BEAM_CENTER_X=");
           my $y= ImgHeaderSearch($header, "BEAM_CENTER_Y=");
           #$beamx= $y;
           #$beamy= $x;
           #$beamx0= $y;
           #$beamy0= $x;
           #$beamx0= 157.2;
           #$beamy0= 148.7;
           #$beamx= 157.2;
           #$beamy= 148.7;
           #$beamx0= 152.929;
           #$beamy0= 152.343;
           #$beamx= 152.929;
           #$beamy= 152.343;
           $beamx0= 154.8;
           $beamy0= 148.4;
           $beamx= 154.8;
           $beamy= 148.4;
        }

        #
        my $axis= ImgHeaderSearch($header, "AXIS=");
        if (! $axis ) { $axis='phi'; }

        # get beamcenter in pixels (used by XDS/D*TREK etc)
        my $beamx_pixel=$beamx/$pixel_size;
        my $beamy_pixel=$beamy/$pixel_size;

        my $orgx=$beamy0/$pixel_size;
        my $orgy=$beamx0/$pixel_size;
       
        # which directory to keep  processing files
        my $process_rt=cwd();
        # a unique runid for each run
        my $runid= "s".$irun."dist".substr($distance,0,3);
        # wavelength significant only to 4 digits (scale together wavelengths with only slight energy difference 
        my $wave4=sprintf("%.4f",$wavelength); 
        my $process_dir=$process_rt."/w".$wave4."/".$runid;
        my $scale_dir=$process_rt."/w".$wave4;

        # update collection information based on header information parsed
        $href->{distance}=$distance;
        $href->{beamx}=$beamx;
        $href->{beamy}=$beamy;
        $href->{twotheta}=$twotheta;
        $href->{osc_range}=$osc_range;
        $href->{osc_start}=$osc_start;
        $href->{wavelength}=$wavelength;
        $href->{wavelength4}=$wave4;
        $href->{sizex}=$sizex;
        $href->{sizey}=$sizey;
        $href->{pixel_size}=$pixel_size;
        $href->{beamx_pixel}=$beamx_pixel;
        $href->{beamy_pixel}=$beamy_pixel;
        $href->{rotation_axis}=$axis;
        $href->{detector}=$detector;
        $href->{processing_directory}=$process_dir;
        $href->{processing_root}=$process_rt;
        $href->{dir_info}=$process_rt."/info";
        $href->{scaling_directory}=$scale_dir;
        $href->{xds_orgx}=$orgx;
        $href->{xds_orgy}=$orgy;
        $href->{runid}=$runid;
    }

    # final pass, remove runs with fewer than $MINIMAGESPERRUN images
    my (@new_collect);
    foreach my $href (@{$aref_collect}) {
        my $nimg=$href->{totalimages};
        next if $nimg<$MINIMAGESPERRUN;
        push @new_collect, $href;
    }

    return \@new_collect;
}

sub dumpHeader {
    # dump Header of images into a text file
    # usuage dumpHeader($filein, $fileout) 
    my $filein = $_[0];
    my $fileout = $_[1];

    system("$IMGHEADER $filein >$fileout");
    open(HEADER,$fileout);
    local $/="\f";
    my $log=<HEADER>;
    close(HEADER);
    return $log;

} # end of dumping header information

sub createXDS {
    # create or update XDS.INP file using the data structure collected above
    my $aref_collect=$_[0];

    foreach my $href ( @{$aref_collect}) {
        next if ($href->{process}==0);

        my $dire=$href->{directory};
        my $fimg=$href->{first_image};
        my $temp=$href->{template};
        my $limg=$href->{last_image};
        my $sseq=$href->{startseq};
        my $eseq=$href->{endseq};
        my $nimg=$href->{totalimages};
        my $wave=$href->{wavelength};
        my $twos=$href->{twotheta};
        my $dist=$href->{distance};
        my $sizx=$href->{sizex};
        my $sizy=$href->{sizey};
        my $psiz=$href->{pixel_size};
        my $bmxp=$href->{beamx_pixel};
        my $bmyp=$href->{beamy_pixel};
        my $oscr=$href->{osc_range};
        my $oscs=$href->{osc_start};
        my $firt=$href->{startseq};
        my $last=$href->{endseq};
        my $orgx=$href->{xds_orgx};
        my $orgy=$href->{xds_orgy};
        my $pdir=$href->{processing_directory};
        my $strongpixel=$href->{strongpixel};
        my $xdscmds=$href->{XDScommands};
        my $highres=$href->{highres};
        my $detector=$href->{detector};

	if (! -e $pdir ) { mkpath($pdir) || die "cannot mkdir $pdir: $!"; }
	open(XDS,"> $pdir/XDS.INP") || die "cannot create file: $pdir/XDS.INP: $!";
        print XDS "!==>DETECTOR SECTION\n";
        my $detector4=substr($detector,0,4);
        my $imgformat="DIRECT  TIFF";
        if($detector4 eq "ADSC") {       
           print XDS "DETECTOR=ADSC  MINIMUM_VALID_PIXEL_VALUE=1  OVERLOAD= 65300\n";
           $imgformat="SMV DIRECT";
           print XDS "NX=",$sizx," NY=",$sizy," QX=",$psiz," QY=",$psiz, "\n";
        } elsif ($detector4 eq "MARC") {
           print XDS "DETECTOR=CCDCHESS        MINIMUM_VALID_PIXEL_VALUE=0     OVERLOAD=65000\n";
           $imgformat="DIRECT  TIFF";
           print XDS "NX=",$sizx," NY=",$sizy," QX=",$psiz," QY=",$psiz, "\n";
        } elsif ($detector4 eq "PILA") {
           print XDS "DETECTOR=PILATUS MINIMUM_VALID_PIXEL_VALUE=0  OVERLOAD=1048500\n";
           print XDS "SENSOR_THICKNESS=0.32        !SILICON=-1.0\n";
           $imgformat="";
           print XDS "NX=",$sizy," NY=",$sizx," QX=",$psiz," QY=",$psiz, "\n";
        } else {
           print "ERROR: Detector not yet supporteded. Please contact the author of the script to add support. Abort.\n";
           exit(1);
        }

        # dealing with 2theta angle
        my $two_theta= ($twos*3.1415926)/180.0;
        my $cost=cos($two_theta);
        my $sint=sin($two_theta);
        print XDS "! DIRECTION_OF_DETECTOR_X-AXIS= $cost 0.0 $sint\n";
        print XDS "! DIRECTION_OF_DETECTOR_Y-AXIS= 0.0 1.0 0.0\n";
        # x=1 0 0 y=[ 0 cos(2theta) sin(twotheta)] worked for ALS 8.3.1
        print XDS "! two_theta= (5*3.1415926)/180.0 cost=cos(two_theta) sint=sin(two_theta) \n";
        print XDS "! x=1 0 0 y=[ 0 cos(2theta) sin(twotheta)] worked for ALS 8.3.1\n";
        print XDS "DIRECTION_OF_DETECTOR_X-AXIS= 1.0 0.0 0.0\n";
        print XDS "DIRECTION_OF_DETECTOR_Y-AXIS= 0.0 $cost $sint\n";
        print XDS "!Half corner is ~1.2, Full CCD 1.4, 1.0 for no corner\n";
        print XDS "TRUSTED_REGION=0.0 1.2\n";
        #print XDS "NX=",$sizx," NY=",$sizy," QX=",$psiz," QY=",$psiz, "\n";

        print XDS "\n!==>JOB CONTROL SECTION\n";
        print XDS "!JOB= !XYCORR INIT COLSPOT IDXREF DEFPIX INTEGRATE CORRECT\n";
        print XDS "JOB=",$href->{xdsjobs}, "\n";
        print XDS "!TEST=1\n";
        print XDS "!MINUTE=0\n";
        print XDS "MAXIMUM_NUMBER_OF_PROCESSORS=$NPROCESSOR\n";
        print XDS "MAXIMUM_NUMBER_OF_JOBS= $NMACHINES \n";

        print XDS "\n!==>GEOMETRY SECTION\n";
        print XDS "! ORGX=   $bmyp  ORGY=   $bmxp !twotheta is $twos\n";
        print XDS "ORGX=   $orgx  ORGY=   $orgy !twotheta is 0 \n";
        print XDS "X-RAY_WAVELENGTH=$wave\n";
        print XDS "DETECTOR_DISTANCE=$dist !(mm)\n";
        print XDS "ROTATION_AXIS= 1.0  0.0 0.0\n";
        print XDS "OSCILLATION_RANGE=$oscr\n";
        print XDS "POLARIZATION_PLANE_NORMAL= 0.0 1.0 0.0\n";
        print XDS "INCIDENT_BEAM_DIRECTION=0.0 0.0 1.0\n";
        print XDS "FRACTION_OF_POLARIZATION=0.92 !SSRL ! 0.5 for unpolarized beam\n";
        print XDS "!AIR=0.001                     !Air absorption coefficient of x-rays\n";
        print XDS "\n";


        print XDS "\n!==>CRYSTAL SECTION\n";
        print XDS "\nMAX_FAC_Rmeas=3.\n";
        if ( !$href->{integ_spg} ) { # if space group is not defined, use 0
           print XDS "SPACE_GROUP_NUMBER=0 !0 for unknown crystals; cell constants are ignored\n";
           print XDS "!UNIT_CELL_CONSTANTS=  85.832  85.832  42.785  90.000  90.000 120.000\n";
           print XDS "!TEST_RESOLUTION_RANGE= 10.0 5 \n";
        } else {
           print XDS "SPACE_GROUP_NUMBER=",$href->{integ_spg},"\n";
           print XDS "UNIT_CELL_CONSTANTS=",$href->{cell}, "\n";
        };
        print XDS "!REIDX=   0  0 -1  0  0 -1  0  0 -1  0  0  0\n";
        print XDS "\n";

        print XDS "\n!==>STRATEGY SECTION\n";
        print XDS "FRIEDEL'S_LAW=FALSE !Default is TRUE.\n";
        print XDS "STARTING_ANGLE= $oscs  STARTING_FRAME=1\n";
        if ( $href->{reference_dataset}  ) {
           print XDS "REFERENCE_DATA_SET= ", $href->{reference_dataset}," !Name of a reference data set\n";
        }
        print XDS "!TOTAL_SPINDLE_ROTATION_RANGES=30.0 120 15\n";
        print XDS "!RESOLUTION_SHELLS=10 6 5 4 3 2 1.5 1.3 1.2\n";
        print XDS "!Default:  STARTING_ANGLE=  0 at STARTING_FRAME=first data image\n";

        print XDS "\n!==>IMAGE DATA SECTION\n";
        print XDS "NAME_TEMPLATE_OF_DATA_FRAMES= ../../images/$temp  $imgformat\n";
        print XDS "DATA_RANGE= $firt  $last\n";
        print XDS spotrange2($sseq,$eseq,$oscr);

        print XDS "\n!==>INDEX SECTION\n";
        print XDS "!Never forget to check this, since the default 0 0 0 is almost always correct!\n";
        print XDS "!INDEX_ORIGIN= 0 0 0 ! used by IDXREF to add an index offset\n";
        print XDS "!Additional parameters for fine tuning that rarely need to be changed\n";
        print XDS "!SEPMIN=2.0 CLUSTER_RADIUS=3\n";
        print XDS "!INDEX_ERROR=0.05 INDEX_MAGNITUDE=8 INDEX_QUALITY=0.8\n";
        print XDS "!MAXIMUM_ERROR_OF_SPOT_POSITION=3.0\n";

        print XDS "\n!==>CRITERIA FOR ACCEPTING REFLECTIONS SECTION\n";
        print XDS "VALUE_RANGE_FOR_TRUSTED_DETECTOR_PIXELS= 6000 30000\n";
        print XDS "INCLUDE_RESOLUTION_RANGE= $CUTOFF_LOWRESOL  $highres !Angstroem; used by DEFPIX,INTEGRATE,CORRECT\n";
        print XDS "!used by CORRECT to exclude ice-reflections\n";
        print XDS "!EXCLUDE_RESOLUTION_RANGE= 3.93 3.87 !ice-ring at 3.897 Angstrom\n";
        print XDS "!EXCLUDE_RESOLUTION_RANGE= 3.70 3.64 !ice-ring at 3.669 Angstrom\n";
        print XDS "!EXCLUDE_RESOLUTION_RANGE= 3.47 3.41 !ice-ring at 3.441 Angstrom\n";
        print XDS "!EXCLUDE_RESOLUTION_RANGE= 2.70 2.64 !ice-ring at 2.671 Angstrom\n";
        print XDS "!EXCLUDE_RESOLUTION_RANGE= 2.29 2.23 !ice-ring at 2.26 Angstrom\n";
        print XDS "!EXCLUDE_RESOLUTION_RANGE= 2.10 2.04 !ice-ring at 2.07 Angstrom\n";
        print XDS "!EXCLUDE_RESOLUTION_RANGE= 1.95 1.89 !ice-ring at 1.92 Angstrom\n";
        print XDS "!EXCLUDE_RESOLUTION_RANGE= 1.56 1.50 !ice-ring at 1.53 Angstrom\n";
        print XDS "!WFAC1=1.0  !This controls the number of rejected MISFITS in CORRECT; larger value leads to fewer rejections\n";

        print XDS "\n!==>INTEGRATION AND PEAK PROFILE PARAMETERS\n";
        print XDS "!Specification of the peak profile parameters below overrides the automatic\n";
        print XDS "!determination from the images specified in PROFILE_RANGE=\n";
        print XDS "!Suggested values are listed near the end of INTEGRATE.LP\n";
        print XDS "!BEAM_DIVERGENCE=  0.473  !arctan(spot diameter/DETECTOR_DISTANCE)\n";
        print XDS "!BEAM_DIVERGENCE_E.S.D.=   0.047 !half-width (Sigma) of BEAM_DIVERGENCE\n";
        print XDS "!REFLECTING_RANGE=  1.100 !for crossing the Ewald sphere on shortest route\n";
        print XDS "!REFLECTING_RANGE_E.S.D.=  0.169 !half-width (mosaicity) of REFLECTING_RANGE\n";
        print XDS "!NUMBER_OF_PROFILE_GRID_POINTS_ALONG_ALPHA/BETA=9 !used by: INTEGRATE\n";
        print XDS "!CUT=2.0    !defines the integration region for profile fitting\n";
        print XDS "!MINPK=75.0 !minimum required percentage of observed reflection intensity\n";
        print XDS "!DELPHI= 5.0!controls the number of reference profiles and scaling factors\n";
        print XDS "!PATCH_SHUTTER_PROBLEM=TRUE         !FALSE is default\n";
        print XDS "STRICT_ABSORPTION_CORRECTION=TRUE !TRUE  is default\n";
        print XDS "\n";

        print XDS "\n!==>PARAMETERS DEFINING BACKGROUND AND PEAK PIXELS\n";
        # print "Increase the level of strong spot from 3 to 5: STRONG_PIXEL=5.0\n";
        print XDS "STRONG_PIXEL= $strongpixel          !used by: COLSPOT\n";
        print XDS "!MAXIMUM_NUMBER_OF_STRONG_PIXELS=1500000       !used by: COLSPOT\n";
        print XDS "!SPOT_MAXIMUM-CENTROID=3.0                     !used by: COLSPOT\n";
        print XDS "!MINIMUM_NUMBER_OF_PIXELS_IN_A_SPOT=6          !used by: COLSPOT\n";
        print XDS "!NBX=3  NBY=3  !Define a rectangle of size (2*NBX+1)*(2*NBY+1)\n";
        print XDS "!BACKGROUND_PIXEL=6.0                          !used by: COLSPOT,INTEGRATE\n";
        print XDS "!SIGNAL_PIXEL=3.0\n";

        print XDS "\n!==>PARAMETERS CONTROLLING REFINEMENTS\n";
        print XDS "REFINE(IDXREF)= BEAM AXIS ORIENTATION CELL DISTANCE\n";
        print XDS "REFINE(INTEGRATE)= DISTANCE BEAM ORIENTATION CELL !AXIS\n";
        print XDS "REFINE(CORRECT)=DISTANCE BEAM ORIENTATION CELL AXIS\n";
        print XDS "\n!==>ADDITIONAL XDS COMMANDS by USER\n";
        print XDS "$xdscmds \n";
	close(XDS);
    } # end of XDS_INFO
     
}

sub ImgHeaderSearch {
    # given header string, and a keyword string, return the value after
    
    my $value=0.0;

    my $header=$_[0];
    my $keyword =$_[1];
    my @arr=split(/;\n/,$header);

    foreach my $el (@arr) {
        if ($el =~ m/$keyword/) {
          $value=$'; 
          # return $value;
        }
    }
    return $value;
}

sub parseIDXREF {
    my %lattice_order=(
      'aP'=>  [1, 'Primitive Triclinic'],
      'mP'=>  [2, 'Primitive Monoclinic'],
      'mC'=>  [3, 'C Centered Monoclinic'],
      'mI'=>  [3, 'C Centered Monoclinic'],
      'oP'=>  [4, 'Primitive Orthorhombic'],
      'oC'=>  [5, 'C Centered Orthorhombic'],
      'oF'=>  [6, 'F Centered Orthorhombic'],
      'oI'=>  [7, 'I Centered Orthorhombic'],
      'tP'=>  [8, 'Primitive Tetragonal'],
      'tI'=>  [9, 'I Centered Tetragonal'],
      'hP'=>  [10,'Primitive Hexagonal'],
      'hR'=>  [11,'Primitive Rhombohedral'],
      'cP'=>  [12,'Primitive Cubic'],
      'cF'=>  [13,'F Centered Cubic'],
      'cI'=>  [14,'I Centered Cubic']
    ); # hash of array to store lattice information

    my $cwdir=$_[0]; # figure out which IDXREF.LP to parse

    open(IDX,"$cwdir/IDXREF.LP") || die "ERROR in runIndex: cannot open $cwdir/IDXREF.LP: $!";
  
    local $/="\f";
    my $text=<IDX>;
    close(IDX);
    my $s1="CHARACTER  LATTICE";
    my $s2="AFTER ALL";
    my $block=extractLastBlock($text,$s1,$s2);
  
    my @lines=split(/\n/,$block);
  
    my(@arr);
    my $cutoff_index=20; # what is an acceptable score for good indexing solution
    my $npossible_solution=0;
    print "\n\n IDXREF.LP indexing result: \n";
    printf "%5s%25s%15s%10s%10s%10s%10s%10s%10s\n", "soln","Lattice","Penalty","a", "b","c","alpha","beta","gamma";
    my (@save_list);
    foreach my $line (@lines) {
       # here is what is looked like: 
       #  25        mC        999.0     183.4  183.4   97.4  74.8 105.2  64.2   -1  0  1  0  1  0  1  0  0  1  0  0
       next if ($line !~ m/\d+/);
       $line =~ s/^\s+\d+\s+//g; # remove empty space and first number and spaces after it
       @arr= split(/\s+/,$line);
       # first reject lines with high penalty value, the cutoff value is 20 for now
       next if ($arr[1] > $cutoff_index); 
       # can also reject impossible cell dimensions, wait till later
       #
       $npossible_solution++;
       printf "%5d%25s%15s%10s%10s%10s%10s%10s%10s\n", $npossible_solution, $lattice_order{$arr[0]}[1], $arr[1],
               $arr[2],$arr[3],$arr[4],$arr[5],$arr[6],$arr[7];
       # replace XDS lattice notation with numbers, for example, cI will become 14 instead
       $line =~ s/$arr[0]/$lattice_order{$arr[0]}[0]/;
       
       # save possible solution into an array of array
       push (@save_list, [ split(/\s+/,$line) ]);
    }
    print " possible solution found: $npossible_solution \n\n";
    if ($npossible_solution <1) {
       print "ERROR in parseIDXREF: index failed, cannot continue\n";
       return (0,0);
    }

    # this is a trick to sort a 2-dimensional arrary twice, 
    # first by first column (lattice, high->low), then by second column (penalty: low->high)
    my @sorted_list=sort {$b->[0] <=> $a->[0] || $a->[1] <=> $b->[1]} @save_list;

    for(my $i=0; $i<$npossible_solution; $i++) {
       if ($sorted_list[$i][0] == 2 || $sorted_list[$i][0] == 3 ) { # monoclinic
          $sorted_list[$i][5] = $sorted_list[$i][7] =90;  #alpha and gamma =90
       }  elsif ($sorted_list[$i][0] >= 4 && $sorted_list[$i][0] <= 7 ) { # Orthorhombic
          $sorted_list[$i][5]=$sorted_list[$i][6]=$sorted_list[$i][7] =90; #alpha=beta=gamma=90
       }  elsif ($sorted_list[$i][0] == 8 || $sorted_list[$i][0] == 9) { #tetragonal
          my $avcell= sprintf("%.3f",($sorted_list[$i][2]+$sorted_list[$i][3])/2.0);
          $sorted_list[$i][2]=$sorted_list[$i][3]=$avcell;
          $sorted_list[$i][5]=$sorted_list[$i][6]=$sorted_list[$i][7] =90; #alpha=beta=gamma=90
       }  elsif ($sorted_list[$i][0] ==10 ) { #hexagonal a=b<>c
          my $avcell= sprintf("%.3f",($sorted_list[$i][2]+$sorted_list[$i][3])/2.0);
          $sorted_list[$i][2]=$sorted_list[$i][3]=$avcell;
          $sorted_list[$i][5]=$sorted_list[$i][6]=90; #alpha=beta=gamma=90
          $sorted_list[$i][7]=120;
       }  elsif ($sorted_list[$i][0] ==11 ) {
          my $avcell= sprintf("%.3f",($sorted_list[$i][2]+$sorted_list[$i][3])/2.0);
          $sorted_list[$i][2]=$sorted_list[$i][3]=$avcell;
          $sorted_list[$i][5]=$sorted_list[$i][6]=90; #alpha=beta=gamma=90
          $sorted_list[$i][7]=120;
       }  elsif ($sorted_list[$i][0] >=12 ) { # cubic
          my $avcell= sprintf("%.3f",($sorted_list[$i][2]+$sorted_list[$i][3]+$sorted_list[$i][4])/3.0);
          $sorted_list[$i][2]=$sorted_list[$i][3]=$sorted_list[$i][4]= $avcell;
          $sorted_list[$i][5]=$sorted_list[$i][6]=$sorted_list[$i][7] =90; #alpha=beta=gamma=90
       } elsif ($sorted_list[$i][0] ==1 ) {
          ; # triclinic doing nothing
       } else {
          print "ERROR in parseIDXREF: no such lattice $sorted_list[$i][0]!\n";
          exit(1);
       }
   }
   return ($npossible_solution, \@sorted_list);
} # end of sub parseIDXREF

sub runIndex1 { # this is similar to runindex2, but the run are INDEX and INTEG are 
                # separated so that it can run regardless, which is UNSAFE!!!
                # !!! USE WITH PRECAUTION
                # this sub selects a run and integrate in reduced cell in space group p1 and 
                # then figure out the the space group, which represents another strategy
    my @latt_spg=(
                # data structure, array or array
                # store space lattice number and their corresponding space group number, lattice
                # number is stored in index, for example, latt_spg[9] corresponding to
                # tetragonal spgs, an artificial lattice 0 was added such that
                # the count of lattice number can start from 1 
       [1], # lattice 0
       [1], # triclinic
       [3], # p2      
       [5], # c2
       [16], # p222
       [21], # c222 
       [22], # f222
       [23], # i222   
       [89,75], # p4 and p422
       [97,79], #i4,i422
       [177,168,150,149,143], # p3,p312,p321,p6,p622        
       [155,146], # r3 , r32  
       [207,195], # p23,p432
       [209,196], #f23,f432       
       [211,197] #i23,i432
    );

    my $aref_collect = $_[0];
    my $aref_indexlist;

    createXDS($aref_collect);
    # right now it always pick the highest lattice with penalty the cutoff (<20)
    my $nfailed = @$aref_collect; # maximum time it can fails its number of runs
    my ($soln,$nsolidx,$penalty);
    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);
       next if ($href->{totalimages}<$MINIMAGESFORINDEX);

       my $wdir=$href->{processing_directory};
       my $proot=$href->{processing_root};

       modifyXDS($wdir,"JOB="," XYCORR  INIT  COLSPOT  IDXREF"); # only run first part
       # P1 integration step, used to determine Laue spg, don't use full image area in order to avoid out of memory error
       modifyXDS($wdir,"INCLUDE_RESOLUTION_RANGE=","$CUTOFF_LOWRESOL 2.0");
       print "Indexing ", $href->{first_image}, " in $wdir... \n";
       print "Logfile in $wdir/index-step1.log.\n";
       #runXDSprogs($wdir,"$XDS","index-step1.log");
       system("cd $wdir; $XDS > $wdir/index-step1.log");
       # ($nsolidx,$aref_indexlist)=parseIDXREF($wdir);
    
       hasERROR("$wdir/IDXREF.LP"); # check whether index performed OK
 
       print "Integration in reduce cell in space group p1 ", $href->{first_image}, " in $wdir...\n";
       print "Logfile in $wdir/index-step2.log.\n";
       modifyXDS($wdir,"JOB=","DEFPIX INTEGRATE"); # only run second part
       #runXDSprogs($wdir,"$XDS","index-step2.log");
       system("cd $wdir; $XDS > $wdir/index-step2.log");

       if ( hasERROR("$wdir/INTEGRATE.LP") )  { 
          print "Warning: running runIndex1 in p1 for run ", $href->{first_image}, " failed in $wdir.\n";
          print "         I will try next batch of images if possible.\n\n";
          $nfailed--;
          if ($nfailed==0) {
             print "Warning: processing for p1 failed for all runs.\n";
             return 0;
          }
          next;
       }

       print "\nSuccessfully completed processing in p1 for run ", $href->{first_image}, " in $wdir.\n";
       $href->{MAD_referencetag}="*"; # this one will be used as reference for MAD scaling
       my ($spg, $rmerge, $correlation,$cell,$nrefl, $resol2sig,$resolxsig, $data_aref_quality);
       print "\nNow trying to determine space group for run ", $href->{first_image}, "\n";
       print "      working directory: $wdir, using data up to $RESOL4SPG A.\n";

       modifyXDS($wdir,"JOB=","CORRECT");
       modifyXDS($wdir,"INCLUDE_RESOLUTION_RANGE=","$CUTOFF_LOWRESOL $RESOL4SPG");
       # rerun XDS with CORRECT, check Rmerge etc
       system("cd $wdir; $XDS > $wdir/spg.scale.log");
       if ( hasERROR("$wdir/CORRECT.LP") )  { 
            print "ERROR: failed to run CORRECT while trying to test spg. Abort.\n";
            exit(1);
       }
       ($spg, $rmerge, $correlation,$cell,$nrefl, $resol2sig, $data_aref_quality)=parseCORRECT2($wdir); 
       #move the resulting reflection file to images directory and used as further reference datasets
       # this is necessary since XDS cannot deal with long file names
       if (-f "$proot/images/REF.HKL" ) {
            print "File $proot/images/REF.HKL already exists. Deleting.\n";
            unlink("$proot/images/REF.HKL");
       }
       move("$wdir/XDS_ASCII.HKL","$proot/images/REF.HKL");
       # run pointless to get a list of candidate spgs
       system("$POINTLESS xdsin $proot/images/REF.HKL >$wdir/pointless.log");
       my $spglist=parsePOINTLESS("$wdir");
       # saved the p1 processed data in a new location
       move("$wdir","$wdir-p1"); mkpath($wdir);

       print "XDS determined spg as $spg.\n"; # found the correct space group
       # update collect and return 
       foreach my $xhref ( @{$aref_collect}) {
            $xhref->{spacegroup}=$spg;
            $xhref->{cell}=$cell; # try refined cell
            $xhref->{integ_spg}=$spg;
            $xhref->{pointless_spgs}=$spglist;
            $xhref->{reference_dataset}="../../images/REF.HKL";
       }
       return 1;
       
       #
       # !!! jun2008 xds makes the following section obselete!!
       #foreach my $soln (@$aref_indexlist) {
       #   my $clattice=$soln->[0];
       #   print "\n==>lattice number is ", $clattice,"\n"; 
       #   my @spg_list= @{$latt_spg[$clattice]};
       #   my $cells=join(" ", @{$soln}[2 .. 7]); # cell from index step
       #   my $reidx=join(" ", @{$soln}[8 .. 19]);
       #   print "cell= $cells reidx= $reidx\n";
       #   
       #   foreach my $spg (@spg_list) {
       #       print "Testing scaling in space group $spg ...\n";
       #       # update the current XDS.INP with above four strings 
       #       modifyXDS($wdir,"UNIT_CELL_CONSTANTS=",$cells);
       #       modifyXDS($wdir,"SPACE_GROUP_NUMBER=",$spg);
       #       modifyXDS($wdir,"JOB=","CORRECT");
       #       modifyXDS($wdir,"REIDX=",$reidx);
       #       modifyXDS($wdir,"INCLUDE_RESOLUTION_RANGE=","$CUTOFF_LOWRESOL $RESOL4SPG");
 
       #       # rerun XDS with CORRECT, check Rmerge etc
       #       #runXDSprogs($wdir,"$XDS","spg-$spg.scale.log");
       #       system("$BSUBfast -o $wdir/spg-$spg.scale.log \"cd $wdir; $XDS\"");
       #       if ( hasERROR("$wdir/CORRECT.LP") )  { 
       #          print "ERROR: failed to run CORRECT while trying to test spg $spg. Abort.\n";
       #          exit(1);
       #       }
       #       ($rmerge, $correlation,$cell,$nrefl, $resol2sig, $data_aref_quality)=parseCORRECT($wdir); 
       #       if($rmerge < $CUTOFF_RSYM ) {
       #          # move the resulting reflection file to images directory and used as further reference datasets
       #          # this is necessary since XDS cannot deal with long file names
       #          if (-f "$proot/images/REF.HKL" ) {
       #             print "File $proot/images/REF.HKL already exists. Deleting.\n";
       #             unlink("$proot/images/REF.HKL");
       #          }
       #          move("$wdir/XDS_ASCII.HKL","$proot/images/REF.HKL");
       #          # saved the p1 processed data in a new location
       #          move("$wdir","$wdir-p1"); mkpath($wdir);

       #          print "will accept the current spg # $spg, based on Rmerge.\n"; # found the correct space group
       #          # update collect and return 
       #          foreach my $xhref ( @{$aref_collect}) {
       #             $xhref->{spacegroup}=$spg;
       #             $xhref->{cell}=$cell; # try refined cell
       #             $xhref->{integ_spg}=$spg;
       #             $xhref->{reference_dataset}="../../images/REF.HKL";
       #          }
       #          return 1;
       #       } else {
       #          print "***reject current space group # $spg since Rmerge is >$CUTOFF_RSYM%.\n\n"
       #       }  
       #   }
       #}
    }

} # end of sub runIndex1

sub modifyXDS { 
    # modify XDS file, needs which directory in which XDS.INP resides, 
    # target keyword (with = ) to be changed and its new value
    my $wdir=$_[0];
    my $keyword=$_[1]; # contains keyword to be replaced
    my $value=$_[2]; # target string
    
    open(XDS,"$wdir/XDS.INP") || die "ERROR in modifyXDS: cannot open file $wdir/XDS.INP: $!";
    local $/="\f";
    my $xds=<XDS>; # reading the file in one go
    close(XDS);
    $xds =~ s/$keyword.*//g;
    $xds = $xds."\n!===modified keyword===\n".$keyword.$value."\n";
    open(XDSNEW,">$wdir/XDS.INP") || die "ERROR in modifyXDS: cannot create file $wdir/XDS.INP: $!";
    print XDSNEW $xds;
    close(XDSNEW);
}

sub runInteg  { # take current information and run integration for all runs
    my $aref_collect = $_[0];

    # create XDS.INP
    createXDS($aref_collect);

    # integration
    my $pm = new Parallel::ForkManager($NMACHINES2);
    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $wdir=$href->{processing_directory};

       sleep $SLEEP;
       $pm->start and next;

       print "\n\nIntegrate ", $href->{first_image}, " in $wdir...\n";
       modifyXDS($wdir,"JOB="," XYCORR  INIT  COLSPOT  IDXREF"); # only run first part
       print "Index and integration in space group ",$href->{spacegroup}, " for run ", $href->{first_image}, " in $wdir...\n";
       print "Logfile for index in $wdir/integ-step1.log.\n";
       #runXDSprogs($wdir,"$XDS","integ-step1.log");
       system("cd $wdir; $XDS >$wdir/integ-step1.log");

       hasERROR("$wdir/IDXREF.LP"); # check whether index performed OK

       print "Logfile for integration in $wdir/integ-step2.log.\n";
       modifyXDS($wdir,"JOB=","DEFPIX INTEGRATE CORRECT"); # only run second part
       #runXDSprogs($wdir,"$XDS","integ-step2.log");
       system("cd $wdir; $XDS >$wdir/integ-step2.log");

       $pm->finish; # do the exit in the child process
    }
    $pm->wait_all_children;

    # parsing results for first integration + correct 
    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $wdir=$href->{processing_directory};
       if ( hasERROR("$wdir/CORRECT.LP") ) {
          print "Warning: running integration for run ", $href->{first_image}, " in $wdir failed.\n";
          print "         I will try next batch of images if possible.\n";
          $href->{integration_status}=0;   # set integration status for failed run
       } else {
          print "\nSuccessfully completed integration for run ", $href->{first_image}, " in $wdir.\n";
          my ($rmerge, $correlation,$cell,$nrefl, $resol2sig,$resolxsig, $data_aref_quality);
          ($rmerge, $correlation,$cell,$nrefl, $resol2sig,$resolxsig, $data_aref_quality)=parseCORRECT($wdir); 
          print "Correlation with reference data (-99 means no reference set given) =", $correlation, "\n";
          # now update collect for this run, this will be useful for following steps
          $href->{cell}=$cell; # try refined cell
          $href->{correlation}=$correlation; 
          $href->{resol2sigma}=$resol2sig; 
          $href->{resolxsigma}=$resolxsig; 
          $href->{reflections}=$nrefl; 
          $href->{rmerge}=$rmerge; 
       }
    }

    # run CORRECT 2nd time to prevent overestimation of resolution
    #my $pm = new Parallel::ForkManager($NMACHINES2);
    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $wdir=$href->{processing_directory};
       my $resolcut=$href->{resol2sigma};
       
       sleep $SLEEP;
       $pm->start and next;

       print "\n\nrun CORRECT 2nd time with improved resolution estimate, log in $wdir/correct.log.\n";
       modifyXDS($wdir,"JOB=","CORRECT"); # only run CORRECT
       modifyXDS($wdir,"INCLUDE_RESOLUTION_RANGE=","$CUTOFF_LOWRESOL $resolcut"); # set the resolution cutoff
       #runXDSprogs($wdir,"$XDS","correct.log");
       system("cd $wdir; $XDS > $wdir/correct.log ");
      
       $pm->finish; # do the exit in the child process
    }
    $pm->wait_all_children;

    # parsing 2nd correct results
    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $wdir=$href->{processing_directory};
       my $resolcut=$href->{resol2sigma};
       if ( hasERROR("$wdir/CORRECT.LP") ) {
          print "Warning: running 2nd CORRECT for run ", $href->{first_image}, " in $wdir failed.\n";
          print "         I will try next batch of images if possible.\n";
          $href->{integration_status}=0;   # set integration status for failed run
       } else {
          print "\nSuccessfully completed CORRECT (to $resolcut A) for run ", $href->{first_image}, " in $wdir.\n";
          my ($rmerge, $correlation,$cell,$nrefl, $resol2sig,$resolxsig, $data_aref_quality);
          ($rmerge, $correlation,$cell,$nrefl, $resol2sig,$resolxsig, $data_aref_quality)=parseCORRECT($wdir);
          print "Correlation with reference data (-99 means no reference set given) =", $correlation, "\n";
          # now update collect for this run, this will be useful for following steps
          $href->{cell}=$cell; # try refined cell
          $href->{correlation}=$correlation;
          $href->{resol2sigma}=$resol2sig;
          $href->{resolxsigma}=$resolxsig;
          $href->{reflections}=$nrefl;
          $href->{rmerge}=$rmerge;
       }
    }

    return;
} # end of runInteg

sub runXDSprogs  { # running xds,xscale,xdsconv command, needs directory, program name and logfile name
  my $wdir=$_[0];
  my $prog=$_[1];
  my $logf=$_[2];

  system("cd $wdir; $prog >$logf");
  return;
} # end of runXDSprogs

### 
## the utility subroutines, general use
##

sub calResol {
    # calculate resolution on detector edge given distance, size of
    # detector (in mm) and wavelength (in A)
    #print "edge resolution....", calResol($distance,$diameter,$wavelength),"\n";

    my $distance=$_[0];
    my $diameter=$_[1];  # diameter of detector
    my $wavelength=$_[2];
    
    my $max_theta=atan2($diameter/2.0, $distance)/2.0;
    my $edge_resol=$wavelength/2.0/sin($max_theta);
    return $edge_resol; # return resolution at the edge
} # end of calResol

sub extractIDXREFReindexVect {# extract reindex vector from end of IDXREF.LP

   my $cwdir=$_[0];
   local $/="\f";
   open(IDXREF,"$cwdir/IDXREF.LP") || die "ERROR extractIDXREFReindexVect: cannot open $cwdir/IDXREF.LP: $!";
   my $text=<IDXREF>;
   close(IDXREF);

   my $str1="REINDEXING CARD           QUALITY";
   my $str2="cpu time used";

   my $block=extractLastBlock($text,$str1,$str2);

   my @reindexvects=();
   if (! $block ) { # if block is empty
      return \@reindexvects; # if there is no such block
   }

   my @lines=split(/\n/,$block);

   foreach my $line (@lines) {
      next if ($line !~ m/\d+/);
      $line =~ s/^\s+//;    # remove beginning empty spaces
      push @reindexvects,$line # remember last number quality is also present
   };

   return \@reindexvects;

} # end of extract reindex vector

# the utility subroutines, general use
sub extractLastBlock { # give a text block and two strings $str1 and $str2, extract last text
                       # block between $str1 and $str2, !!!! NOTE: does not work for s1==s2
    my $text=$_[0];
    my $str1=$_[1];
    my $str2=$_[2];

    my @lines=split(/\n/,$text);

    my $i=0;
    my $i1=99999999;
    my $i2=0;
    foreach my $line (@lines) {
       if ($line =~ m/$str1/) {
          $i1=$i;
       }
       if ($line =~ m/$str2/) {
          $i2=$i;
       }
       $i++;
    }

    my $last_block;
    if ($i1 <= $i2) {
       $last_block=join("\n",@lines[$i1+1 .. $i2-1] )."\n";
    } else {
       print "WARNING: unable to extract wanted textblock between s1 and s2.\n";
       print "         s1=$str1\n";
       print "         s2=$str2\n";
       $last_block='';
    }
    return $last_block;

} # end of extractLastBlock

sub runReindex { # reindex if necessary so that all runs can be directly scalable to reference data
    my $aref_collect = $_[0];

    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       print "\n\nCompare ",$href->{first_image}," and ",$href->{reference_dataset}, "\n";
       my $wdir=$href->{processing_directory};
       my $ok=$href->{integration_status};
       
       if( $ok ) { # if integration is successful for this run
          my $corr=$href->{correlation};
          print "Correlation between run ", $href->{first_image}," and REFERENCE is $corr.\n"; 
          next if $corr>$REINDEX_CORRELATION;             # no need to reindex if the correlation is already good
          print "Since the correlation is low, will try to reindex.\n"; 
          my $reindexvec_ref = extractIDXREFReindexVect($wdir);
          my @vects = @$reindexvec_ref;
          if( @vects ) { # if reindex vectors are not empty, run reindex one by one
               foreach my $v (@vects) {
                   modifyXDS($wdir, "REIDX=",$v); 
                   modifyXDS($wdir, "JOB=","CORRECT"); 

                   # rerun xds
                   print "Reindex by rerun XDS(CORRECT) again using reindex vector: $v.\n";
                   #runXDSprogs($wdir, "$XDS", "reindex.log");
                   system(" cd $wdir; $XDS > $wdir/reindex.log");
                   
                   # check correlation again
                   my ($rmerge, $correlation,$cell,$nrefl, $resol2sig,$resolxsig, $data_aref_quality);
                   ($rmerge, $correlation,$cell,$nrefl, $resol2sig,$resolxsig, $data_aref_quality)=parseCORRECT($wdir); 
                   print "Correlation with reference dataset is $correlation for idx=$v.\n"; 
                   if ( $correlation > $REINDEX_CORRELATION ) {
                      print "Correlation $correlation is acceptable, so keep this result for this run.\n";
                      $href->{correlation}=$correlation; # update using the correlation
                      $href->{reindex}=$v;         # save reindex matrix and its quality (last number)
                      last; # find correct reindex matrix and quit this loop
                   }
               } # foreach $v
          } else {
             print "NOTE in runReindex: no need to run reindex.\n";
          } # if @vects
       } else { # integration successful
             print "Integration this run is not successful. Skip.\n";
       } # integration unsuccessful 
       
    } # for each run

    return;
} # end of reindex subroutine

sub parseCORRECT {
  my @latt_spg_complete=( 
                 # data structure, array or array
                 # store space lattice number and their corresponding space group number, lattice
                 # number is stored in index, for example, latt_spg[9] corresponding to 
                 # tetragonal spgs, an artificial lattice 0 was added such that 
                 # the count of lattice number can start from 1
       [1], # lattice 0
       [1], # triclinic 
       [3, 4], # p2
       [5], # c2
       [5], # c2
       [16, 17, 18, 19], # p222
       [21, 20], # c222
       [22],     # f222
       [23, 24], # i222
       [75, 76, 77, 78, 89, 90, 91, 92, 93, 94, 95, 96], # p4 and p422
       [79, 80, 97, 98], #i4,i422
       [143, 144, 145, 149, 150, 151, 152,153,154,168,169,170,171,172,173, 177, 178,179,180,181,182], # p3,p312,p321,p6,p622
       [146, 155], # r3 , r32
       [195, 198, 207, 208, 212, 213], # p23,p432
       [196, 209, 210], #f23,f432
       [197,199,211,214] #i23,i432
  );

  my $cwdir=$_[0]; # figure out which CORRECT.LP to parse

  print "Analysing $cwdir/CORRECT.LP ...\n";
  open(CORRECT,"$cwdir/CORRECT.LP") || die "ERROR in parseCORRECT: cannot open $cwdir/CORRECT.LP: $!";


  local $/="\f";
  my $log=<CORRECT>;
  close(CORRECT); 
  my $str1='SUBSET OF INTENSITY DATA WITH SIGNAL/NOISE >= -3.0 AS FUNCTION OF RESOLUTION';
  #my $str2='SUBSET OF INTENSITY DATA WITH SIGNAL/NOISE >=  0.0 AS FUNCTION OF RESOLUTION';
  my $str2='NUMBER OF REFLECTIONS IN SELECTED SUBSET OF IMAGES';

    
  my $last_block=extractLastBlock($log,$str1,$str2);
  my @lines=split(/\n/,$last_block);

  my @data_quality; # array of array storing extracted table
  print "                                 SUMMARY OF DATA SET STATISTICS\n";
  print " ***********************************************************************************************************************************\n";
  #print "  RESOLUTION     NUMBER OF REFLECTIONS    COMPLETENESS R-FACTOR  R-FACTOR COMPARED I/SIGMA   R-meas  Rmrgd-F  Anomal  SigAno   Nano\n";
  #print "    LIMIT     OBSERVED  UNIQUE  POSSIBLE     OF DATA   observed  expected                                      Corr                \n";
  print "  RESOLUTION     NUMBER OF REFLECTIONS    COMPLETENESS R-FACTOR  R-FACTOR COMPARED I/SIGMA   R-meas  CC(1/2)  Anomal  SigAno   Nano\n";
  print "    LIMIT     OBSERVED  UNIQUE  POSSIBLE     OF DATA   observed  expected                                      Corr                \n";

  foreach my $line (@lines) {
     next if ($line !~ m/\d+/);
     next if ($line =~ m/^\s+RE/);
     print $line, "\n";
     $line =~ s/^\s+//g;    # remove beginning empty spaces
     $line =~ s/\%//g;      # remove % sign
     $line =~ s/\*//g;      # remove * sign
     push @data_quality, [ split(/\s+/,$line) ];
  };

  my $rmerge=$data_quality[-1][5];
  print "Rmerge (overall) =", $rmerge, "\n";
 
  my $resol2sig=$data_quality[-2][0]; # set it to highest resolution possible
  for my $i ( 0 .. $#data_quality-1 ) {
     # print $data_quality[$i][8],"\n"; # i over sigma i is at column 8 right now
     if ( $data_quality[$i][8] - 1.5 < 0.01 ) {
       # $resol2sig=$data_quality[$i-1][0]; # option 1
       # $resol2sig=$data_quality[$i][0];   # option 2
       my $wc=$data_quality[$i][8];   # current weight, suggested by Mitch
       my $wp=$data_quality[$i-1][8]; # previous weight
       $resol2sig =  sprintf("%.2f",($wp*$data_quality[$i-1][0]+$wc*$data_quality[$i][0])/($wp+$wc) );
       last;
     }
  }
  print "Resolution @ at least 2.0 sigma  =", $resol2sig, "\n";

  # obtain a higher i/sigma cutoff resolution, may be this is better for MAD experiements
  #
  my $resolxsig=$data_quality[-2][0]; # set it to highest resolution possible
  for my $i ( 0 .. $#data_quality-1 ) {
     if ( $data_quality[$i][8] - $SIGMAMAD < 0.01 ) {
       # $resolxsig=$data_quality[$i-1][0];
       my $wc=$data_quality[$i][8];   # current weight, suggested by Mitch
       my $wp=$data_quality[$i-1][8]; # previous weight
       $resolxsig =  sprintf("%.2f",($wp*$data_quality[$i-1][0]+$wc*$data_quality[$i][0])/($wp+$wc) );
       last;
     }
  }
  print "Resolution @ at least x.0 sigma (x=$SIGMAMAD)  =", $resolxsig, "\n";

  # parse refined new cell after COLLECT.LP step
  $log =~ /UNIT CELL PARAMETERS\s+(.*)/;
  my $cell=$1; 
  print "refined new unit cell  =", $cell, "\n";

  # parse from number of reflections
  $log =~ /USING\s+(\d+)\s+INDEXED SPOTS/;
  my $nrefl=$1;
  print "number of indexed spots used to refine cell  =", $nrefl, "\n";
  
  # parse for correction between reference set if any
  $str1="CORRELATIONS BETWEEN INPUT DATA SETS AFTER CORRECTIONS";
  $str2="Factor applied to intensities";

  my $block=extractLastBlock($log,$str1,$str2);

  my $correlation=-99;
  if ( $block ) { # if block is not empty
    my @lines=split(/\n/,$block);

    foreach my $line (@lines) {
      next if ($line !~ m/\d+/);
      $line =~ s/^\s+//;    # remove beginning empty spaces
      my @arr=split(/\s+/,$line);
      $correlation=$arr[3];
      if ($correlation =~ m/^([+-]?)(?=\d|\.\d)\d*(\.\d*)?([Ee]([+-]?\d+))?$/) { # check if it is a real number
         last; # exit the loop if found a number
      }
    }
  }

  return ($rmerge,$correlation, $cell,$nrefl, $resol2sig,$resolxsig,\@data_quality);

} # end of sub parseCORRECT

sub createXSCALE  { # take current information and run integration for all runs
    my $aref_collect = $_[0];
    my %dataset;
    my $spg;
    my $root;

    my $FRIEDELLAW='FALSE';
    if (!$ANOMALOUS) {
       $FRIEDELLAW='TRUE';
    } 

    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $wav4=$href->{wavelength4}; 
       $root=$href->{processing_root};
       my $integ_ok = $href->{integration_status};
       if($integ_ok) {
          $dataset{$wav4} = createDataSet($aref_collect,$wav4);
       }
    }

    # creating XSCALE.INP for each wavelength, and multiple wavelength scaling
    foreach my $w  (keys (%dataset) ) {  # foreach wavelength, looping
        my $maxwavres=20; #maxresolution for this wavelength
        my $dir=$root."/w".$w; # get scaling dir for this wavelength

        print "\n\ncreating XSCALE.INP for w=$w in $dir scaled data set\n";
        open(XSCALE,">$dir/XSCALE.INP") || die "cannot create XSCALE.INP in $dir: $!\n";
        print XSCALE "MAXIMUM_NUMBER_OF_PROCESSORS= $NPROCESSOR\n";
        print XSCALE "\n\n!Defines w=$w scaled data set\n";
        #print XSCALE "OUTPUT_FILE=w$w-nomerge.ahkl FRIEDEL'S_LAW=$FRIEDELLAW MERGE=FALSE\n";
        print XSCALE "OUTPUT_FILE=w$w-nomerge.ahkl \nFRIEDEL'S_LAW=TRUE \nMERGE=FALSE\n";

        my $ac=0.0; my $bc=0.0; my $cc=0.0; my $aa=0.0; my $ba=0.0; my $ca=0.0; my $nw=0;
        my $xtalid;
        foreach my $href (@{$aref_collect}) {
            next if ($href->{process}==0);
            my $integ_ok = $href->{integration_status};
            my $wavlen= sprintf("%.4f",$href->{wavelength});
            my $runid=$href->{runid};
            my $rmerge=$href->{rmerge};
            my $corr=$href->{correlation};
            $xtalid=$href->{crystalid};
            if($integ_ok) {
                my $cell= $href->{cell};
                my $nref= $href->{reflections};
                $spg=$href->{spacegroup};

                my $resol = $href->{resol2sigma};
                
                if ($w==$wavlen) { # calculate average cell for this wavelength only
                     if ($resol < $maxwavres ) {
                       $maxwavres=$resol;
                     }

                     my @ar=split(/\s+/,$cell);
                     $ac=$ac+$ar[0]*$nref;
                     $bc=$bc+$ar[1]*$nref;
                     $cc=$cc+$ar[2]*$nref;
                     $aa=$aa+$ar[3]*$nref;
                     $ba=$ba+$ar[4]*$nref;
                     $ca=$ca+$ar[5]*$nref;
                     $nw=$nw+$nref;
                     if ($corr>$REINDEX_CORRELATION) {  # if the rmerge is acceptable
                       print XSCALE "INPUT_FILE= ./$runid/XDS_ASCII.HKL  XDS_ASCII \nINCLUDE_RESOLUTION_RANGE= $CUTOFF_LOWRESOL $resol\n";
                     } else { # correlation with reference is poor
                       print "\n\nWarning: correlation $corr to reference is low for run ", $href->{first_image},"!\n"; 
                       print " you need to investigate what is went wrong for this run.\n"; 
                       print " This run is not included in the scaling.\n"; 
                       print XSCALE "!INPUT_FILE= ./$runid/XDS_ASCII.HKL  XDS_ASCII \nINCLUDE_RESOLUTION_RANGE= $CUTOFF_LOWRESOL 3.0\n";
                     } #
                } 
            } else { # integration not work for this run
                print XSCALE "!INPUT_FILE= ./$runid/XDS_ASCII.HKL XDS_ASCII \nINCLUDE_RESOLUTION_RANGE= $CUTOFF_LOWRESOL 2.5 !turn it on when integ fixed\n";
            }
        } # end of this wavelength
        my $av= sprintf("%.4f %.4f %.4f %.4f %.4f %.4f", $ac/$nw,$bc/$nw,$cc/$nw,$aa/$nw,$ba/$nw,$ca/$nw);
        print XSCALE  "UNIT_CELL_CONSTANTS=$av !unit cell\n";
        print XSCALE  "SPACE_GROUP_NUMBER=$spg !space group\n";
        print "averaged unit cell for w=$w is $av\n";

        my $shells=calResolShells(20,$maxwavres,10);
        print XSCALE  "RESOLUTION_SHELLS=$shells !resolution shells\n";
        close(XSCALE);
        print "Maxium resolution for w=$w is $maxwavres\n"; 

        # save information for this wavelength
        $dataset{$w}{resolxsigma}=$maxwavres;
        $dataset{$w}{cell}=$av;
        $dataset{$w}{filename}="w$w.ahkl";
        $dataset{$w}{scaledir}="$root/w$w";
        $dataset{$w}{spacegroup}=$spg;
        $dataset{$w}{integ_spg}=$spg;
        $dataset{$w}{crystalid}=$xtalid;
    } # end of all wavelens
    
    return \%dataset; # Note: this is a hash of hash
} # end of creating XSCALE.INP

sub createMADXSCALE  { 
    my $aref_collect = $_[0];
    my %MADdataset;
    my $spg;
    my $root;

    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $wav4=$href->{wavelength4}; 
       $root=$href->{processing_root};
       my $inMADset = $href->{inMADset};
       if($inMADset) {
          $MADdataset{$wav4} = createDataSet($aref_collect,$wav4);
       }
    }

    my $nwav= keys(%MADdataset); # how many wavelength?

    # creating XSCALE.INP for each wavelength, and multiple wavelength scaling
    my $wdir="$root/MADscale/";
    if (! -e "$wdir") { mkpath("$wdir"); }
    open(MADXSCALE,"> $wdir/XSCALE.INP");
    my $maxMADres=20; # max resolution overall
    my $xac=0.0; my $xbc=0.0; my $xcc=0.0; my $xaa=0.0; my $xba=0.0; my $xca=0.0; my $na=0;
    foreach my $w  (keys (%MADdataset) ) {  # foreach wavelength, looping
        my $maxwavres=20; #maxresolution for this wavelength
        print MADXSCALE "\n\n!Defines w=$w scaled data set\n";
        print MADXSCALE "OUTPUT_FILE=w$w-nomerge.ahkl \nFRIEDEL'S_LAW=FALSE \nMERGE=FALSE\n";

        my $xtalid;
        foreach my $href (@{$aref_collect}) {
            next if ($href->{process}==0);
            my $inMADset = $href->{inMADset};
            my $wavlen= $href->{wavelength4};
            my $runid=$href->{runid};
            my $ref=$href->{MAD_referencetag};
            $xtalid=$href->{crystalid};
            if($inMADset) {
                my $cell= $href->{cell};
                my $nref= $href->{reflections};
                $spg=$href->{spacegroup};
                my $resolx = $href->{resolxsigma};
                
                if ($resolx < $maxMADres ) { $maxMADres=$resolx; }

                if ($w==$wavlen) { # calculate average cell for this wavelength only
                     if ($resolx < $maxwavres ) { $maxwavres=$resolx; }

                     my @ar=split(/\s+/,$cell);
                     $xac=$xac+$ar[0]*$nref;
                     $xbc=$xbc+$ar[1]*$nref;
                     $xcc=$xcc+$ar[2]*$nref;
                     $xaa=$xaa+$ar[3]*$nref;
                     $xba=$xba+$ar[4]*$nref;
                     $xca=$xca+$ar[5]*$nref;
                     $na=$na+$nref;
                     print MADXSCALE "INPUT_FILE= $ref../w$w/$runid/XDS_ASCII.HKL  XDS_ASCII \nINCLUDE_RESOLUTION_RANGE= $CUTOFF_LOWRESOL $resolx\n";
                } 
            } else { # rejected from MAD dataset
                if ($w==$wavlen) { # calculate average cell for this wavelength only
                   print MADXSCALE "!INPUT_FILE= ../w$w/$runid/XDS_ASCII.HKL XDS_ASCII \nINCLUDE_RESOLUTION_RANGE= $CUTOFF_LOWRESOL 2.5 !turn it on when integ fixed\n";
                }
            }
        } # end of this wavelength

        # save information for this wavelength
        $MADdataset{$w}{resolxsigma}=$maxwavres;
        $MADdataset{$w}{filename}="w$w.ahkl";
        $MADdataset{$w}{scaledir}=$wdir;
        $MADdataset{$w}{spacegroup}=$spg;
        $MADdataset{$w}{integ_spg}=$spg;
        $MADdataset{$w}{crystalid}=$xtalid;
        print MADXSCALE "!turn off zero dose extraction by default \n";
        print MADXSCALE "!CRYSTAL_NAME=XTALID !STARTING_DOSE=0.0  DOSE_RATE=1.0\n";
    } # end of all wavelens
    my $xav= sprintf("%.4f %.4f %.4f %.4f %.4f %.4f", $xac/$na,$xbc/$na,$xcc/$na,$xaa/$na,$xba/$na,$xca/$na);
    print "Overall averaged unit cell for all wavelength is $xav\n";
    print "Maxium resolution for all wavelength is $maxMADres\n"; 
    print MADXSCALE  "UNIT_CELL_CONSTANTS=$xav !unit cell\n";
    print MADXSCALE  "SPACE_GROUP_NUMBER=$spg !space group\n";
    my $shells=calResolShells(20,$maxMADres,10);
    print MADXSCALE  "RESOLUTION_SHELLS=$shells !resolution shells\n";
    print MADXSCALE  "!0-DOSE_SIGNIFICANCE_LEVEL=0.10\n";
    print MADXSCALE  "MAXIMUM_NUMBER_OF_PROCESSORS= $NPROCESSOR\n";
    close(MADXSCALE);
    
    foreach my $w  (keys (%MADdataset) ) {  # update with overall unit cell
       $MADdataset{$w}{cell}=$xav;
    }
    
    #runXDSprogs($wdir,"$XSCALE","xscale.log");
    my $cmd="cd $wdir;$XSCALE >$wdir/xscale.log ";
    system("$cmd");

    if ( hasERROR("$wdir/XSCALE.LP") ) {
       print "Multiple wavelength scaling faied in $wdir\n";
    }

    print "Multiple wavelength scaling successful, logfile: $wdir/XSCALE.LP\n";

    return (\%MADdataset, $maxMADres); # Note: this is a hash of hash
} # end of creating SCALE.INP

sub createRun { # create a hash with default values, this is the central data structure
  my %run=( directory=>'',
            dir_info=>cwd.'/info',
            template=>'',
            first_image=>'',
            last_image=>'',
            startseq=>'',
            endseq=>'',
            totalimages=>'',
            distance=>'',
            beamx=>'',
            beamy=>'',
            twotheta=>'',
            osc_range=>'',
            osc_start=>'',
            wavelength=>'',
            wavelength4=>'',
            sizex=>'',
            sizey=>'',
            pixel_size=>'',
            beamx_pixel=>'',
            beamy_pixel=>'',
            rotation_axis=>'',
            detector=>'',
            processing_directory=>'',
            processing_root=>'',
            scaling_directory=>'',
            project_name=>$PROJECT,
            dataset_name=>'default_dname',
            crystalid=>'default_xtalid',
            spacegroup=>'0',
            mosaic=>'0.5',
            detector=>'ADSC',
            gain=>'0.5',
            asu_content=>'500',
            excluded_frames=>'',
            XDScommands=>'',
            polarization=>'0.9',
            beam_divergence=>'0.01',
            beam_divergenceESD=>'',
            cell=>'1 1 1 90 90 90',
            integration_status=>1,
            xds_orgx=>'',
            xds_orgy=>'',
            highres=>'0.6', # high resolution cutoff, can be changed by user
            strongpixel=>'3.0', # STRONG_PIXEL used in COLSPOT
            rfactor=>1.0,  # 100% percent
            integ_spg=>'',      # space group used for integration
            pointless_spgs=>'',      # determined by pointless
            reindex=>'',        # matrix used for reindex
            runid=>'',
            inMADset=>'1', # decide it belongs to MAD sets, use to select runs to include in MAD data
            xdsjobs=>'ALL',  # what job xds is going to run ?
            reference_dataset=>'', # any known dataset in same xtal form can be used here
            MAD_referencetag=>'', # this tag is use to set the reference dataset in datasets with multiple runs
            resol2sigma=>0.0,
            resolxsigma=>0.0, # get the resolution which I/sigma is x
            rmerge=>10,
            correlation=>-99, # correlation with reference dataset
            reflections=>0,
            nmolASU=>'',
            sitesASU=>'',
            Vm=>'',
            solvent=>'',
            energytag=>'',
            xsolve_label=>'', # label used in xsolve "Description field"
            process=>1, # whether to process this run 
            imgprefix=>'',
            fp=>'0.0',
            fpp=>'0.0'
  ); # end of storing the new hash
  return \%run;
} # end of create run

sub createLinks {  # create links to overcome XDS filename limits
    my $dir = $_[0];

    my $ok=0;
    # define how file names looked like, only field really matters is the seq number
    my $imagepatten = '(\d\d\d\.)|(\.\d\d\d*)';
    if (! -e $dir) { print "Error in createLinks: $dir does not exit.\n"; exit(1); }
    if (! -r $dir) { print "Error in createLinks: $dir is not readable.\n"; exit(1); }
    print "parsing images in $dir\n";

    opendir(DATA, $dir) || die "Error: cannot open directory $dir\n";
    if (! -e "./images") { mkdir("./images",0755); }

    while ( my $name = readdir(DATA))  {
        # ignore some common named files such as .log .dkc .x .lp .scan .mtz .bip .hkl
        next if ($name =~ m/\.log$/);
        next if ($name =~ m/\.dkc$/);
        next if ($name =~ m/\.lp$/);
        next if ($name =~ m/\.x$/);
        next if ($name =~ m/\.scan$/);
        next if ($name =~ m/\.bip$/);
        next if ($name =~ m/\.mtz$/);
        next if ($name =~ m/\.hkl$/);
        next if ($name =~ m/\.tar$/);
        next if ($name =~ m/\.gz$/);
        next if ($name =~ m/\.mosflm/);
        next if ($name =~ m/\.map/);
        next if ($name =~ m/\.scan/);
        next if ($name =~ m/\.im0/);
        next if ($name =~ m/\.im1/);
        next if ($name =~ m/\.bk/);
        next if ($name =~ m/\.rouges$/);

        my $match=0;
        if(exists $href_userInputs->{ignore} ) {
            my $href_ignore=$href_userInputs->{ignore};
            foreach my $pattern (@$href_ignore) {
               if ( $name =~ m/$pattern/ ) {
                  print "Warning: will ignore image $name.\n";
                  $match=1;
                  last;
               }
            }
        }
        next if $match==1;

        # can turn it off so that it work faster
        my $fz = (-s $dir."/".$name); #get image file size
        next if $fz < 2048000; # skip if the filesize is less than 2MB

        # now select those matched the pattern defined
        if ( $name =~ m/$imagepatten/ ) {
           symlink("$dir/$name","./images/$name") || die "cannot symlink to $name: $!";
        }
    }
    $ok=1;
    closedir(DATA);
    return $ok;
} # end of creating links

sub calResolShells {  
   # calculate resolution shells give min, max and # of shells in N
   # this is my cooked version, may not be right theoretically !
   # return upper resolution bounds in a string
   #
   # print calResolShells(20.0,1.8,20), "\n";

   my $dmax=$_[0]; # min resolution, eg 20.0
   my $dmin=$_[1]; # max resolution, eg 2.0
   my $n=$_[2];
   
   my @shels;

   my $dmax3=1.0/($dmax**3);
   my $dmin3=1.0/($dmin**3);
   my $upperbnd_resolshells='';
   for my $i (1 .. $n) {
      my $s3= $dmax3 + $i*($dmin3-$dmax3)/$n;
      $shels[$i]= sprintf("%.2f", 1.0/$s3**(1.0/3.0));
      $upperbnd_resolshells=$upperbnd_resolshells." ".$shels[$i];
   }
   return $upperbnd_resolshells;
} #end of calResolShells

sub is_empty_or_notexist { # check the directory is empty or not
   my $dir = $_[0];
   if (-d $dir) { #dir exists
     opendir(D,$dir) || die "opendir $dir: $!"; 
     my @dir_entries = readdir(D); 
     if (scalar(@dir_entries) == 2) {
        print "$dir is empty.\n";
        return 1; # empty
     } else {
       return 0;
     }
   } else {
     return 1; # not exist
   }
} # end of is_empty_or_notexist($dir) 


sub runXSCALE { # run xscale program 
   my $href_dataset=$_[0];

   my $pm = new Parallel::ForkManager($NMACHINES2);
   foreach my $w (keys %{$href_dataset} ) {
      sleep $SLEEP;
      $pm->start and next; # do the fork

      my $dir=$href_dataset->{$w}{scaledir};
      print "\nRunning scaling for wavelength $w in $dir\n";
      #runXDSprogs($dir,"$XSCALE","xscale.log");
      my $cmd="cd $dir;$XSCALE >$dir/xscale.log ";
      system("$cmd");
      if( hasERROR("$dir/XSCALE.LP") ) {
        print "Warning: Scaling failed for wavelength $w.\n";  
      } else {
        print "\nSuccessfully scaled the dataset w=$w, logfile $dir/XSCALE.LP.\n";
        my $dataquality = parseXSCALE($dir);
        $href_dataset->{$w}{quality_table}=$dataquality; # save data quality table
      } 
      $pm->finish;
   } # end of a scaling cycle
   $pm->wait_all_children;

   return;
}

sub createDataSet { # create a dataset, harvest data scaling statistics
   my $aref_collect=$_[0];
   my $wav=$_[1];

   my %dataset= ( 
       name=>'',
       rmerge=>'',
       scaledir=>'',
       rootdir=>'',
       filename=>'',
       completeness=>'',
       resolxsigma=>'',
       fp=>'',
       fpp=>'',
       nmolASU=>'',
       sitesASU=>'',
       quality_table=>'',
       cell=>'',
       cell_reindexed=>'',
       solvent=>'',
       Vm=>'',
       mtz_truncate=>'', 
       mtz_scala=>'',
       log_truncate=>'',
       log_scala=>'',
       log_xscale=>'',
       reindex=>'h,k,l',
       sca_unmerged=>'',
       crystalid=>'default_xtalid',
       project_name=>$PROJECT, 
       dataset_name=>'default_dname',
       spacegroup=>'',
       integ_spg=>'', # space group used in integration
       pointless_spgs=>'',
       HAsites=>'',
       hapdb_sharp=>'', # final sharp HA sites file
       energytag=>''  # for shelxc, to determine what label for 
                      # each wavelength 'peak,infl,lremo,hremo or sad'
   );
  
   # copy fp and fpp etc
   foreach  my $href ( @{$aref_collect}) {
      if ($href->{wavelength4} == $wav ) {
         $dataset{fp}=$href->{fp};
         $dataset{fpp}=$href->{fpp};
         $dataset{nmolASU}=$href->{nmolASU};
         $dataset{sitesASU}=$href->{sitesASU};
         $dataset{solvent}=$href->{solvent};
         $dataset{Vm}=$href->{Vm};
         $dataset{energytag}=$href->{energytag};
         $dataset{cell}=$href->{cell};
         $dataset{integ_spg}=$href->{integ_spg};
         $dataset{pointless_spgs}=$href->{pointless_spgs};
         $dataset{spacegroup}=$href->{spacegroup};
         last;
      }
   }

   return \%dataset;
} # end of createDataSet

sub getspgs { 
   # retrieve possible space groups from the integration space group
   #    return a reference to an array of possible spgs.
   #
   # similar table can be found in HKL manual as well
   # usage: print Dumper getspgs(149);
   my $spg=$_[0];

   my %spg_table2=( # hash of has to translate integration space to real space group
       1  =>[1], # p1
       3  =>[3, 4], # p2 and p21
       5  =>[5], # c2 
       16 =>[16, 17, 18, 19], # p222,p2221 p21212 p212121
       21 =>[20,21], #c2221 c222
       22 =>[22], #f222
       23 =>[23, 24], #i222 and i212121
       75 =>[75, 76, 77, 78], # p41-p43
       89 =>[89, 90, 91, 92, 93, 94, 95, 96], # p422 etc
       79 =>[79, 80], #i4 and i41
       97 =>[97, 98], # i422 and i4122
       143=>[143,144,145], # p3,p31,p32
       149=>[149,151,153], # p312 p3112 p3212
       150=>[150,152,154], # p321 p3121 p3221
       168=>[168,169,170,171,172,173], # p6 etc
       177=>[177,178,179,180,181,182], # p622 etc
       146=>[146], #r3
       155=>[155], #r32
       195=>[195,198], #p23 and p213
       207=>[207,208,212,213], # p432 etc
       196=>[196], #f23
       209=>[209,210], #f432 and f4132
       197=>[197,199], #i23/i213
       211=>[211,214]  # i432 and i4132
   );

   my %spg_table=( # hash of has to translate integration space to real space group
       1  =>['p1'], # p1
       3  =>['p21','p2'], # p2 and p21
       5  =>['c2'], # c2
       #16 =>['p212121','p2221','p21212','p222'], # p222,p2221 p21212 p212121
       16 =>['p212121','p2221','p2212','p2122','p21212','p21221','p22121','p222'], # p222,p2221 p21212 p212121
       21 =>['c2221','c222'], #c2221 c222
       22 =>['f222'], #f222
       23 =>['i222','i212121'], #i222 and i212121
       75 =>['p41','p42','p43','p4'], # p41-p43
       89 =>['p43212','p41212','p422','p4212','p4122','p4322','p4222','p42212'], # p422 etc
       79 =>['i4','i41'], #i4 and i41
       97 =>['i422','i4122'], # i422 and i4122
       143=>['p3','p31','p32'], # p3,p31,p32
       149=>['p312','p3112','p3212'], # p312 p3112 p3212
       150=>['p321','p3121','p3221'], # p321 p3121 p3221
       168=>['p6','p61','p62','p63','p64','p65'], # p6 etc
       177=>['p622','p6122','p6222','p6322','p6422','p6522'], # p622 etc
       #146=>['r3'], #r3
       #155=>['r32'], #r32
       146=>['h3'], #r3
       155=>['h32'], #r32
       195=>['p23','p213'], #p23 and p213
       207=>['p432','p4132','p4232','p4332'], # p432 etc
       196=>['f23'], #f23
       209=>['f432','f4132'], #f432 and f4132
       197=>['i23','i213'], #i23/i213
       211=>['i432','i4132']  # i432 and i4132
   );

   foreach my $k (keys %spg_table) {
     my $href=$spg_table{$k};
     if($k==$spg) {
        return $href;
     }
   }
   print "Something is seriously wrong, you shouldn't get here.\n";
} # end of space group retrieve

sub createBP3 {
   my $href_dataset=$_[0];
   my $href_target=$_[1];
   my $href_sharpset=$_[2];

   my $wdir=cwd()."/trace/bp3/";
   if (! -e $wdir ) {
      mkpath($wdir);
   }

   my $HAtype=ucfirst(lc($href_target->{HAtype})); # make sure HA is what SHARP expects, only first letter upper case
   my $naa=$href_target->{nAA};
   my $molw=$href_target->{MW};
   my $seq=$href_target->{sequence};

   my $nmolperasu = 1;
   my ($cell,$spg,$nsites,$solvent,$xyzblock);
   foreach my $w (keys %{$href_dataset} ) { # get needed numbers from first dataset
     $nmolperasu= $href_dataset->{$w}{nmolASU};
     $cell=$href_dataset->{$w}{cell};
     $spg=$href_dataset->{$w}{spacegroup};
     $nsites=$href_dataset->{$w}{sitesASU};
     $xyzblock=$href_dataset->{$w}{HAsites};
     #$solvent=sprintf("%.2f",$href_dataset->{$w}{solvent}/100.0);
     last;
   }
   $spg=uc $spg;
   my $nres=$href_target->{nAA};
   my $asucontent= $nres*$nmolperasu;

   symlink("../../MADscale/cad-all-truncate.mtz","$wdir/cad.mtz");
   
   open(BP3,">$wdir/bp3.csh");
   printf BP3 "#!/bin/csh\n\nbp3 hklin cad.mtz hklout bp3.mtz << eofbp3 >bp3.log \n";
   # sites...
   my $hapdb=$href_sharpset->{pdb_finalHA};
   printf BP3 "XTAL jcsg_xtalid\n";
   symlink($hapdb,"$wdir/ha.pdb");
   printf BP3 "MODL ha.pdb\n";

   #waveleng labels
   foreach my $w (sort keys %{$href_dataset} ) { # smallest wavelength will be always the reference
      my $edge=$href_dataset->{$w}{energytag};
      my $fp=$href_dataset->{$w}{fp};
      my $fpp=$href_dataset->{$w}{fpp};
      my $hatype=$href_target->{HAtype};
      printf BP3 "DNAME $edge\n  COLUmn F+=F-$w(+) SF+=SIGF-$w(+) F-=F-$w(-) SF-=SIGF-$w(-)\n";
      printf BP3 "  FORM $hatype FP $fp FPP $fpp\n";
   }

   printf BP3 "ALLIn\neofbp3\n";

   #add dm and tracing 
print BP3 <<EOFBP3;

$PARROT -stdin << eofp > parrot.log
pdbin-ref /home/sdcsoftware/linux/ccp4-6.2.0/lib/data/reference_structures/reference-1tqw.pdb
mtzin-ref /home/sdcsoftware/linux/ccp4-6.2.0/lib/data/reference_structures/reference-1tqw.mtz
colin-ref-fo /*/*/[FP.F_sigF.F,FP.F_sigF.sigF]
colin-ref-hl /*/*/[FC.ABCD.A,FC.ABCD.B,FC.ABCD.C,FC.ABCD.D]
seqin-wrk seq.pir
mtzin-wrk bp3.mtz
colin-wrk-fo /*/*/[FPHASED,SIGFPHASED]
colin-wrk-hl /*/*/[HLA,HLB,HLC,HLD]
pdbin-wrk-ha ha.pdb
colout parrot
solvent-flatten
histogram-match
ncs-average
cycles 8
mtzout parrot.mtz
eofp

/home/sdcsoftware/linux/tools/bin/python $BUCCANEERSCRIPT -stdin << eofbuc > buccaneer.log
seqin seq.pir
mtzin parrot.mtz
colin-fo FPHASED,SIGFPHASED
colin-hl parrot.ABCD.A,parrot.ABCD.B, parrot.ABCD.C,parrot.ABCD.D
cycles 5
jobs  $NPROCESSOR
eofbuc

$ARP workdir $wdir datafile $wdir/parrot.mtz seqin $dir_info/seq.pir cgr $nmolperasu residues $asucontent modelin $wdir/buccaneer.pdb fp FPHASED sigfp SIGFPHASED fbest parrot.F_phi.F phibest parrot.F_phi.phi >warp.log

EOFBP3

   close(BP3);
   copy("$dir_info/seq.pir","$wdir/seq.pir");
   chmod(0755,"$wdir/bp3.csh");
   my $cmd2run="cd $wdir; ./bp3.csh >$wdir/runbp3.log ";
   #system("$cmd2run &");
   print "NOTE: BP3 heavy atom refinement phasing @ $wdir, the heavy atom sites are from SHARP.\n";
   print "      parrot->buccaneer->warp results at: $wdir.\n";

   return; 
}

sub createAutoSHARP { # prepare autoSHARP inputs
   my $href_dataset=$_[0];
   my $href_target=$_[1];
   my $maxres=$_[2];

   my $wdir=cwd()."/sharp/";
   if (! -e $wdir ) { 
      mkpath($wdir); 
   }

   my @wavs=keys %{$href_dataset};

   my $nwav= @wavs;

   my $HAtype=ucfirst(lc($href_target->{HAtype})); # make sure HA is what SHARP expects, only first letter upper case
   my $naa=$href_target->{nAA};
   my $molw=$href_target->{MW};
   my $seq=$href_target->{sequence};

   my ($cell,$spg,$nsites,$solvent,$xyzblock);
   foreach my $w (keys %{$href_dataset} ) { # get needed numbers from first dataset
     $cell=$href_dataset->{$w}{cell};
     $spg=$href_dataset->{$w}{spacegroup};
     $nsites=$href_dataset->{$w}{sitesASU};
     $xyzblock=$href_dataset->{$w}{HAsites};
     #$solvent=sprintf("%.2f",$href_dataset->{$w}{solvent}/100.0);
     last;
   }
   $spg=uc $spg;

   # prepare sequence file
   my $seqtag='seq';
   if ( length($seq)<10) {
      $seqtag=''; 
      print "Warning: no sequence given in autoSHARP.\n";
   } else {
      copy("$dir_info/seq.pir","$wdir/seq.pir");
   }
   
   # prepare HA site file
   my $sitetag='HAsites';
   open(HA,">$wdir/HAsites.hatom");
   my @lines=split /\n/,$xyzblock;
   my $shelxdsites=0;
   foreach my $line (@lines) {
      my @a=split /\s+/,$line;
      if ($a[5]>0.0) { # only save good sites
         $shelxdsites++;
         my $x=$a[2];
         my $y=$a[3];
         my $z=$a[4];
         print HA "ATOM $HAtype $x $y $z\n"; 
      } 
   }
   close(HA);

   if ($shelxdsites == 0 ) {
      print "Error: no sites found using shelxd, aborting.\n";
      exit(1);
   }
   if (abs($shelxdsites-$nsites) >=2 ) {
      print "Warning: number sites saved in SHELXD: $shelxdsites does not match number of sites expected: $nsites.\n";
      $nsites = $shelxdsites+2; # resett nsites to shelxd sites, since nsites are likely overestimated! this should stop autoSHARP iterates more than necessary
   } 

   open(SHARP,">$wdir/dot.autoSHARP");

print SHARP <<EOF;
# ----------------------------------------
# General information about project
# ----------------------------------------
  autoSHARP_proj="autoSHARP"
  autoSHARP_jobi="0"
  autoSHARP_titl="autoSHARP MAD/SAD data"
  autoSHARP_type="MAD"
  autoSHARP_rate="5"
  autoSHARP_molw="0.0"
  autoSHARP_nres="0"
  autoSHARP_pirf="$seqtag"
#  autoSHARP_spgr="$spg"
  autoSHARP_spgr=""
  autoSHARP_resl="$CUTOFF_LOWRESOL"
  autoSHARP_resh="$maxres"
  autoSHARP_nset="$nwav"
  autoSHARP_user="$ENV{USER}"
  autoSHARP_ulvl="1"
  autoSHARP_chtm="no"
  autoSHARP_csum="no"
# ----------------------------------------
# autoSHARP protocol information
# ----------------------------------------
  autoSHARP_EntryPoint="2"
  autoSHARP_EntryPoint3_Path="3"
  autoSHARP_EntryPoint3_Path3_Opt="7"
  autoSHARP_DetectPgm="shelx"
# ----------------------------------------
# CCP4i specific
# ----------------------------------------
  autoSHARP_RunningType="ccp4i"
  autoSHARP_ccp4i_workdir="$wdir"
  autoSHARP_ccp4i_fileroot="$wdir/tmp"
  autoSHARP_ccp4i_log_file="$wdir/autoSHARP.log"
  autoSHARP_ccp4i_log_dir="autoSHARP"
  autoSHARP_ccp4i_job_id="1"
  autoSHARP_ccp4i_taskname="autoSHARP"
  autoSHARP_ccp4i_autoSHARP_dir="$wdir/autoSHARP"
  autoSHARP_viewer="NONE"
  CCP4I_DEFFILE="$wdir/autoSHARP.def"
EOF
   # now for each wavelength write a section 
   my $i=0;
   foreach my $w (sort keys %{$href_dataset} ) { # smallest wavelength will be always the reference
          my $edge=$href_dataset->{$w}{energytag};
          my $scdir=$href_dataset->{$w}{scaledir};
          my $fp=$href_dataset->{$w}{fp};
          my $fpp=$href_dataset->{$w}{fpp};
          my $nsites=$href_dataset->{$w}{sitesASU};
          my $mtzfile=$href_dataset->{$w}{mtz_truncate};
          my $hatype=$href_target->{HAtype};
          symlink($mtzfile, "$wdir/w$w-truncate.mtz");
          $i++;
print SHARP <<EOF1;
# ----------------------------------------
# data section for $w wavelength
# ----------------------------------------
  autoSHARP_iden_$i="w$w"
  autoSHARP_wave_$i="$w"
  autoSHARP_hatm_$i="$hatype"
  autoSHARP_nsit_$i="$nsites"
  autoSHARP_sitf_$i="$sitetag"
  autoSHARP_fone_$i="$fp"
  autoSHARP_ftwo_$i="$fpp"
  autoSHARP_fmid_$i="F"
  autoSHARP_smid_$i="SIGF"
  autoSHARP_dano_$i="DANO"
  autoSHARP_sano_$i="SIGDANO"
  autoSHARP_isym_$i="ISYM"
  autoSHARP_dtyp_$i="MTZ"
  autoSHARP_data_$i="w$w-truncate.mtz"
EOF1
   } # end of wavelength


   close(SHARP);

   copy("$wdir/dot.autoSHARP","$wdir/.autoSHARP");
   print "\nNow running sharp in $wdir on machine ", $ENV{HOST}," ... ";
   print "\n   NOTE: dataset corresponding to the SMALLEST wavelength is used as reference in autoSHARP.\n";
   print "\n   NOTE: if autoSHARP does not run correctly, all following steps will fail. \n";
   print "           Check the following:\n";
   print "         a. you are authorized to run sharp.\n";
   print "         b. you are on a machine with valid sharp licenses.\n\n";
   my $cmd="cd $wdir; $BDG_home/bin/sharp/detect.sh >$wdir/autosharp.html";
   system("$cmd");
   print "done\n\n";

   return;
}

sub FinalSharpDir { # locate where is the final sharp run
  my $file = $File::Find::name;
  return unless -f $file;
  return  unless $file =~ /eden_flat.*pc.mtz$/;

  my $dir=$File::Find::dir;

  opendir(DIR,$dir) || die "fail to opendir $!";
  my @flist=readdir DIR;
  closedir(DIR);

  my @fomlogs=();
  my @pplogs=();
  my @rclogs=();
  foreach my $file (@flist) {
     if ($file =~ m/^FOMstats/) { push(@fomlogs,$file); }
     if ($file =~ m/^PhasingPower/) { push(@pplogs,$file); }
     if ($file =~ m/^RCullis/) { push(@rclogs,$file); }
  }
  @fomlogs=sort @fomlogs; @pplogs=sort @pplogs;

  $href_sharpPhaseSet->{setname}='SHELX and autoSHARP';
  $href_sharpPhaseSet->{mtz_solomon}=$file;
  $href_sharpPhaseSet->{mtz_bestphs}="$dir/eden-unique.mtz";
  $href_sharpPhaseSet->{dir_final}="$dir";
  $href_sharpPhaseSet->{pdb_finalHA}="$dir/hatom.pdb";
  $href_sharpPhaseSet->{log_FOM}=    "$dir/".$fomlogs[-1]; 
  $href_sharpPhaseSet->{log_PP}=     "$dir/".$pplogs[-1];
  $href_sharpPhaseSet->{log_Rcullis}="$dir/".$rclogs[-1]; ;
 
  $file =~ m/(.*)(eden_flat_)(\d+.\d+)(pc.mtz)/;
  my $solv= $3;
  $href_sharpPhaseSet->{solv_solomon}=$solv;

  my $sinfile;
  my @tmparr=split(/\//,$dir);
  if ($tmparr[-1] eq '') {
    pop @tmparr;
    $sinfile=$dir."/../end_auto".$tmparr[-1].".sin";
  } else {
    $sinfile=$dir."/../end_auto".$tmparr[-1].".sin";
  }
  $href_sharpPhaseSet->{sin_final}=$sinfile;

  my $solomon_logfile=$dir."/solo_".$solv."pc.log.html";
  $href_sharpPhaseSet->{log_solomon}=$solomon_logfile;

  print "Final pass of SHARP was carried out in $dir\n";
  print "Final refined hatom site: $dir/hatom.pdb \n";
  print "Final autoSHARP mtz (no density modification): $dir/eden-unique.mtz\n";
  print "Final autoSHARP density modification mtz: $file\n";
  print "Final autoSHARP density modification log: ",$href_sharpPhaseSet->{log_solomon},"\n";
  print "Final autoSHARP solomon solvent content used: $solv\n";
  print "Final autoSHARP fom logfile: ", $href_sharpPhaseSet->{log_FOM}, "\n";
  print "Final autoSHARP phasing power logfile: ",$href_sharpPhaseSet->{log_PP},"\n";
  print "Final autoSHARP Rcullis logfile: ",$href_sharpPhaseSet->{log_Rcullis},"\n";
  print "Final autoSHARP sin file: ",$href_sharpPhaseSet->{sin_final},"\n";

}

sub createPhaseSet {
   my %phaseset=(
      'setname'=>'',     # method used
      # shelx
      'dir_shelx'=>'',
      'CCall_shelxd'=>'',
      'CCweak_shelxd'=>'',
      'CC_shelxe'=>'',
      'contrast_shelxe'=>'',
      'connect_shelxe'=>'',
      'spacegroup'=>'',
      'phs_shelxe'=>'',
      'log_shelxe'=>'',    
      'log_shelxd'=>'',    
      'pdb_shelxd'=>'',    
      'res_shelxd'=>'',    
      'log_shelxc'=>'',
      # sharp
      'mtz_solomon'=>'', # final solomon
      'log_solomon'=>'', # final solomon
      'mtz_bestphs'=>'', # final eden-unique.mtz
      'dir_final'=>'',   # final dir in which SHARP was run
      'sin_final'=>'',   # final sin file
      'pdb_finalHA'=>'', # final HA site in pdb
      'log_fom'=>'', # final fom log
      'log_PP'=>'', # final phasing power log
      'log_Rcullis'=>'',
      'solv_solomon'=>'0.5', # optimal solvent content used by autosharp-solomon
      # dm
      'mtz_dm'=>'',
      'log_dm'=>'',
      'dir_dm'=>'',
      # wARP
      'log_warp'=>'',
      'mtz_warp'=>'',
      'pdb_warp'=>'',
      'traced_warp'=>'',
      'traced_buccaneer'=>'',
      # resovle
      'log_resolve'=>'',
      'mtz_resolve'=>'',
      'pdb_resolve'=>'',
      'traced_resolve'=>'',
      'dummy'=>''
   );
   return \%phaseset;
}

sub getUserInputs {
  my %h=();
  GetOptions(\%h,'distance=f',
                 'anomalous!',
                 'help',
                 'stopafter=s',   # where to stop
                 'fast',     # determine how the integration is done
                 'recover',  # recover and debugging
                 'collectinfo', # creat links, do nothing else
                 'highres=s', # highres cutoff 
                 'rsym=s',  # let use define what is acceptable for spg acception, default 20
                 'CCmad=s', # let use define what is acceptable for shelxd cc,mad, default 40
                 'CCsad=s', # let use define what is acceptable for shelxd cc,sad, default 35
                 'CCreindex=s', 
                 'jcsg=s',
                 'xsolve=s', # to run inside xsolve tree; reading xsolve CollectData.xml
                 'gmcat!',
                 'beam=s',
                 'parallel=i', # number of parallel process
                 'seq=s',
                 'twotheta=f',
                 'peak=s',
                 'inflection=s',
                 'ispeak', 
                 'icerings=s',
                 'project=s',
                 'cell=s',
                 'spgnumber=i',
                 'heavyatom=s',
                 'nsites=i',
                 'nresidues=s',
                 'nmolASU=s',
                 "data=s@",
                 'ignore=s@', # allow user to ignore certain images by using patterns
                 'XDScommands=s',
                 'machines=s');	# will store in $h{length}  
  return \%h;
}

sub getJCSGtargetfromWWW {
   # retrieve jcsg target information from the web, return has reference to hash %targetinfo
   # ex: $href=getJCSGtarget($target);

   my $tid=$_[0];
   my $URL="http://www1.jcsg.org/cgi-bin/psat/targetinfo.cgi?acc=$tid";

   my $href_target=$_[1];

   my $content = get($URL);
   if (!defined $content) {
      print "Cannot retrieve http://www1.jcsg.org/cgi-bin/psat/targetinfo.cgi?acc=$tid\n";
      return $href_target;
   }
   getstore($URL,"$tid.html");
   $content =~ s/<[^>]*>//gs; # remove html tags
   $content =~ /Number of Met residues:\s+(\d+)/; 
   my $nMET=$1;

   if(! defined $nMET) {
      print "Warning: Error retrieve sequence. Please make sure to enter correct target name.\n";
      return $href_target;
   }
   
   $content =~ /Residues:\s+(\d+)\s+aa/;
   my $nAA=$1; 
   $content =~ /Molecule Weight:\s+(\d+.\d+)\s+Dalton/;
   my $mw=$1; 
   print "# of Met: $nMET, # of aa: $nAA, molecular weigh: $mw\n";
   #$content =~ /^SEQUENCE.+aa\n(.*?)^$/ms; # extract sequence from jcsg db
   $content =~ /SEQUENCE.+aa\n(.*?)^$/ms; # extract sequence from jcsg db
   my $seq1=$1;
   
   $seq1 =~ /\n/;
   my $header=$`;
   my $seq=$';
   # $seq=~s/\n//g; # remove tailing newline 
   $href_target->{nHA}=$nMET;
   $href_target->{nAA}=$nAA;
   $href_target->{MW}=$mw;
   $href_target->{sequence}=$seq;
   $href_target->{header}=$header;

   open(PIR,">$dir_info/$tid.pir");
   open(SEQ,">$dir_info/$tid.raw");
   print SEQ $seq;
   print PIR ">p1;$tid $header\n\n";
   print PIR $seq;
   print PIR "*\n";
   close(PIR);
   close(SEQ); 
   
   # my @arr=split //,$seq;  # turn seq into character array
   return $href_target;
}

sub updateTarget {
   # get target information: my $href_target=updateTarget($href_userInput);
   my $href=$_[0];

   my $href_target=createTarget();
   if (-e "$dir_info/TARGET.NFO") {
      $href_target=do "$dir_info/TARGET.NFO" or die "can't recreate href_target: $! $@";
   }
   if( exists $href->{jcsg} ) {
       print "Retrieve information from web for target: ",$href->{jcsg},"...\n";
       $href_target=getJCSGtargetfromWWW($href->{jcsg},$href_target);       
   }

   # update sequence 
   if( exists $href->{seq} || -e 'seq.dat' ) {
      my $seq='';
      my $seqfile= $href->{seq};
      if(-e 'seq.dat') { $seqfile="seq.dat"; }
      open SEQ, "$seqfile";
      while(<SEQ>) {
         next if  /^\s*$/;    # skip empty line
         next if /^\s*>/;  # skip lines with >
         next if /^\*/;  # skip lines with start with *
         next if /^\>[pP]1/;  # skip lines with start with >p1 or >P1
         s/\*//g; # remove * from sequence
         $seq=$seq.$_;
      }  
      close(SEQ);

      my $nMet = ($seq =~ tr/Mm//); # count of number of MET
      # add JCSG HIS tag
      #my $tag="MGSDKIHHHHHH";
      #if( exists $href->{tag} ) {
      #  $tag=$href->{tag};
      #} 
      #my $tag="MGSDKIHHHHHH";
      my $tag='';
      $seq=$tag.$seq;

      my $molw=length($seq)*110;
      $href_target->{sequence}=$seq;
      $href_target->{nHA}=$nMet;
      $href_target->{nAA}=length($seq);
      $href_target->{MW}=$molw;
   }

   open(SEQDAT, ">$dir_info/seq.raw"); # write a raw sequence called seq.raw
   print SEQDAT $href_target->{sequence},"\n";
   close(SEQDAT);
   open(SEQPIR, ">$dir_info/seq.pir"); # write a pir sequence called seq.pir
   print SEQPIR ">p1; target\n\n";
   print SEQPIR $href_target->{sequence};
   print SEQPIR "*";
   close(SEQPIR);

   if( exists $href->{heavyatom} ) {
       $href_target->{HAtype}=$href->{heavyatom};
       print "Note: user defined heavy atom type: ",$href->{heavyatom},".\n";
   }

   if( exists $href->{nresidues} ) {
       $href_target->{nAA}=$href->{nresidues};
       print "Note: user defined number of amino acid residues: ",$href->{nresidues},".\n";
   }

   if( exists $href->{nsites} ) {
       $href_target->{nHA}=$href->{nsites};
       print "Note: user defined number of heavy atom sites: ",$href->{nsites},".\n";
   }

   print "\n             SUMMARY of TARGET INFORMATION\n";
   print "====================================================================\n";
   print "Molecular weight = ",$href_target->{MW},"\n";
   print "Heavy atom type = ",$href_target->{HAtype},"\n";
   print "number of Heavy atom per mol = ",$href_target->{nHA},"\n";
   print "number of aa per mol = ",$href_target->{nAA},"\n";
   print "sequence:\n",$href_target->{sequence},"\n";

   return $href_target;
} # end of update target

sub createTarget {
   my %targetinfo=( 
       nHA=>'2', # number of HA sites per protein
       nAA=>'130', 
       MW=>'15000', 
       sequence=>'', 
       HAtype=>'Se',
       header=>'dummy header'
     ); 
   return \%targetinfo;
}

sub cal_matthews_Vm {
   ### now calculate matthew's coef to figure out additional information
   # we use matthew_coef in ccp4 to do the calculation, 
   #needs: dataset and target information
   #
   # returns: $nmolASU,$Vm,$solvent
   my $spg=$_[0];
   my $cell=$_[1];
   my $molw=$_[2];
   my $mode=$_[3]; # can be either 'AUTO' or number of moleculer per asu given by user input

   if ( $mode eq 'AUTO' ) { # automode if nmol per asu is unknown
      open(MATTHEWS,">$dir_info/matthews.com");
      print MATTHEWS "$MATTHEWS_COEF <<eof\n";
      print MATTHEWS "SYMM $spg\n";
      print MATTHEWS "CELL $cell\n";
      print MATTHEWS "AUTO\n";
      print MATTHEWS "MOLW $molw  \n";
      print MATTHEWS "END\neof\n";
      close (MATTHEWS);
      chmod(0755,"$dir_info/matthews.com");
      my $cmd2run="(cd $dir_info; ./matthews.com) >$dir_info/matthews.log";
      #system("./matthews.com >$dir_info/matthews.log");
      system($cmd2run);
   
      ## now parse log to determine #nmol and solvent
      local $/;
      open(MATTHEWSLOG,"$dir_info/matthews.log");
      my $mlog=<MATTHEWSLOG>;
      close(MATTHEWSLOG);
   
      my $s1='Nmol/asym  Matthews Coeff  %solvent';
      my $s2='___________________________________';
      my $block=extractLastBlock($mlog,$s1,$s2);
   
      my @lines=split(/\n/,$block);
      my @vmtable;
      foreach my $line (@lines) {
        next if ($line !~ m/\d+/);
        $line =~ s/^\s+//g;    # remove beginning empty spaces
        $line =~ s/\%//g;      # remove % sign
        push @vmtable, [ split(/\s+/,$line) ]; # vmtable is two dimensional array
      };
   
      # trying to figuring out a solution with closest to ~50% solvent
      my @sorted_list=sort { abs($a->[2]-50.0) <=> abs($b->[2]-50.0)} @vmtable;
   
      # after above sorting, now the first one is what I want
      my $nmolASU=$sorted_list[0][0];
      my $Vm=$sorted_list[0][1];
      my $solvent=$sorted_list[0][2];
      return($nmolASU,$Vm,$solvent);
   } else { # known nmol per asu, get Vm and solvent
      open(MATTHEWS,">./matthews.com");
      print MATTHEWS "$MATTHEWS_COEF <<eof\n";
      print MATTHEWS "SYMM $spg\n";
      print MATTHEWS "CELL $cell\n";
      print MATTHEWS "NMOL $mode\n";
      print MATTHEWS "MOLW $molw  \n";
      print MATTHEWS "END\neof\n";
      close (MATTHEWS);
      chmod(0755,"./matthews.com");
      system("./matthews.com >matthews.log");

      ## now parse log to determine #nmol and solvent
      local $/;
      open(MATTHEWSLOG,"./matthews.log");
      my $mlog=<MATTHEWSLOG>;
      close(MATTHEWSLOG);

      # The Matthews Coefficient is :  5.08
      # Assuming protein density is 1.34, the solvent % is: 75.59
      $mlog =~ /The Matthews Coefficient is :\s+(\d+.\d+)/;
      my $Vm=$1;
      $mlog =~ /Assuming protein density is 1.34, the solvent % is:\s+(\d+.\d+)/;
      my $solvent=$1;
      return ($mode,$Vm,$solvent);
   }
   ## end
} # end of matthew_coef

sub createMADXDSCONV2 {
   my $href_MADdataset=$_[0];
   my $solved=$_[1];

   # use integ_spg which is a number since XDS don't deal with symbols
   my ($MADcell, $ispg,$rspg,$MADdir,$reindex,$xtalid);
   foreach my $w (keys %{$href_MADdataset} ) { # get needed numbers from first dataset
     $ispg=$href_MADdataset->{$w}{integ_spg}; # integ space group number for XDSCONV
     $rspg=$href_MADdataset->{$w}{spacegroup}; #use symbol for mtz file, determined by SHELXD/SHELXE
     $MADcell=$href_MADdataset->{$w}{cell}; #
     $MADdir=$href_MADdataset->{$w}{scaledir}; #
     $reindex=$href_MADdataset->{$w}{reindex}; #
     $xtalid=$href_MADdataset->{$w}{crystalid}; #
     last;
   }

   my $pm = new Parallel::ForkManager($NMACHINES2);
   foreach my $w (keys %{$href_MADdataset} ) {
      my $dir=$href_MADdataset->{$w}{scaledir}."/w$w";
      $href_MADdataset->{$w}{mtz_truncate}="$dir/w$w-truncate.mtz";
      $href_MADdataset->{$w}{log_truncate}="$dir/w$w-truncate.log";
      $href_MADdataset->{$w}{mtz_scala}="$dir/w$w-scala.mtz";
      $href_MADdataset->{$w}{log_scala}="$dir/w$w-scala.log";
      $href_MADdataset->{$w}{log_xscale}="$dir/../XSCALE.LP";
      $href_MADdataset->{$w}{sca_unmerged}="SCALEPACK";
      $href_MADdataset->{$w}{dataset_name}="w$w";

      sleep $SLEEP;
      $pm->start and next; # do the fork

      print "Converting to mtz for w=$w using script $dir/mad-w$w.com ...\n";
      if (! -e $dir) {
         mkpath($dir);
      }

      open(DTMP,">$dir/w$w.ahkl") || die "Error: cannot open $dir/w$w.ahkl for writing: $!.\n";
      open(HKLDATA,"$MADdir/w$w-nomerge.ahkl") || die "Error: cannot open $MADdir/w$w.ahkl: $!.\n"; ; # data must be unmerged
      my $cell=$MADcell;
      my $spg=$ispg;
      my $maxI=-1;
      while(my $line=<HKLDATA>) {
         next if $line =~ /^\!/;
         my @a=split /\s+/,$line; 
         my $n= @a; 
         next if $n!=11;
         my $batch=int(1000*$a[10]+$a[8]+0.5);
         printf DTMP "%6d%6d%6d%6d%15.1f%15.1f\n", $a[1],$a[2],$a[3],$batch,$a[4],$a[5];
         if($a[4]>$maxI) { $maxI=$a[4]; } # maximum intensity
      }
      close(DTMP);
      close(HKLDATA);

      my $scale=sprintf("%.8f",500000.0/$maxI); # make sure it is not out of range in stupid SCALEPACK format f8.1
      if ($scale<0.0) {
         print "Scale is negative in createMADXDSCONV2. Abort.\n";
         exit(1);
      }

      open(JCSG,">$dir/mad-w$w.com");

      print JCSG <<EOF;
#!/bin/csh -f

setenv BINSORT_SCR \$HOME/scratch
cd $dir
set dir = $dir
set combat = $COMBAT
set reindex = $REINDEX
set scala = $SCALA
set truncate = $TRUNCATE
set sortmtz = $SORTMTZ 

\${combat} HKLIN \${dir}/w$w.ahkl HKLOUT \${dir}/tmp1.mtz <<eof-combat  >\${dir}/w$w-combat.log
title [No title given]
symmetry $rspg
cell $cell
scale $scale
wavelength $w
input USER
format '(4F6.0,2F15.1)'
label H K L BATCH I SIGI
NAME PROJECT $PROJECT CRYSTAL $xtalid DATASET w$w
end
eof-combat

\${reindex} hklin \${dir}/tmp1.mtz hklout \${dir}/tmp.mtz <<eof-reindex >\${dir}/w$w-reindex.log
symm $rspg
reindex $reindex
eof-reindex

\${sortmtz} hklin \${dir}/tmp.mtz hklout \${dir}/sort.mtz <<eof-sortmtz >\${dir}/w$w-sortmtz.log
H K L M/ISYM BATCH I SIGI
eof-sortmtz

# run scala to get SCALEPACK POLISH UNMERGED ORIGINAL
\${scala} hklin \${dir}/sort.mtz hklout \${dir}/w$w-scala.mtz <<eof-scala >\${dir}/w$w-scala0.log
run 1 batch 1 to 99999
title w=$w, processed by XDS/XSCALE 
scales constant
anomalous on
SDCORRECTION NOADJUST BOTH 1.0 0.0 0.0 
output POLISH UNMERGED ORIGINAL
eof-scala

# rerun to generate mtz file for truncation
\${scala} hklin \${dir}/sort.mtz hklout \${dir}/w$w-scala.mtz <<eof-scala >\${dir}/w$w-scala.log
run 1 batch 1 to 99999
title w=$w, processed by XDS/XSCALE 
scales constant
SDCORRECTION NOADJUST BOTH 1.0 0.0 0.0 
anomalous on
eof-scala

\${truncate} HKLIN \${dir}/w$w-scala.mtz HKLOUT \${dir}/w$w-truncate.mtz <<eof-trunc >\${dir}/w$w-truncate.log
anomalous yes
TRUNCATE YES
falloff yes
noharvest
eof-trunc

EOF
      close(JCSG);
      chmod(0755,"$dir/mad-w$w.com");
      system(" cd $dir; ./mad-w$w.com >$dir/mad-w$w.log ");
      move("$MADdir/SCALEPACK","$MADdir/w$w.sca");
      #system("gzip $MADdir/w$w.sca");
      unlink("$dir/sort.mtz","$dir/tmp.mtz","$dir/tmp1.mtz");
  
      $pm->finish;
   } # end of a scaling cycle
   $pm->wait_all_children;

   if ($solved != 1) { return;}
   #combine all truncate data into  a single file
   open(CAD,">$MADdir/cad.com"); 
   printf CAD "#!/bin/csh\n\n"; 

   my $cmdline="$CAD ";
   my $nfile=0;
   foreach my $w (keys %{$href_MADdataset} ) {
      $nfile++;
      my $currtruncatefile=$href_MADdataset->{$w}{mtz_truncate};
      $cmdline=$cmdline." hklin$nfile $currtruncatefile ";
   }
   $cmdline=$cmdline." hklout cad-all-truncate.mtz <<eof-cad >cad-all-truncate.log \n";
   printf CAD $cmdline;
   $nfile=0;
   foreach my $w (keys %{$href_MADdataset} ) {
      $nfile++;
      printf CAD "labin file_number $nfile E1=F E2=SIGF E3=DANO E4=SIGDANO E5=F(+) E6=SIGF(+) E7=F(-) E8=SIGF(-)\n";
      printf CAD "labout file_number  $nfile E1=F-$w E2=SIGF-$w E3=DANO-$w E4=SIGDANO-$w E5=F-$w(+) E6=SIGF-$w(+) E7=F-$w(-) E8=SIGF-$w(-)\n";
   }
   printf CAD "eof-cad\n";
   close(CAD);
   chmod(0755,"$MADdir/cad.com");
   #system(" cd $MADdir; csh ./cad.com > $MADdir/cad.log ");
   #done cad

   return;
}

sub parseSHELXD {
    # parse .res file to see how shelxd is doing
    # ex: parseSHELXD("p321_fa.res");

    my $resfile=$_[0];
    if ( ! -e $resfile ) {
       print "$resfile does not exist. SHELXD score too low!";
       return (0.0,0.0,"",0,0);    
    }
    local $/;
    open(RES,"$resfile");
    my $res=<RES>;
    close(RES);
     
    $res =~ /REM TRY\s+\d+\s+CC(.*)/;
    $res =~ /REM TRY\s+\d+\s+CC\s+(\d+.\d+)\s+CC\(weak\)\s+(\d+.\d+)/;
    my $CCall=$1;
    my $CCweak=$2;

    # now generate HA sites file only xyz block is extracted
    my $s1="UNIT ";
    my $s2="HKLF ";
    my $xyz=extractLastBlock($res,$s1,$s2);

    # now check the occupancy column of shelxd sites
    my @sites=split(/\n/,$xyz);
    my @occ_sites;
    my $ngoodsite=0;
    foreach my $site (@sites) {
      my @flds=split(/\s+/,$site);
      my $nflds=@flds;
      if( $nflds == 7 ) {
         push(@occ_sites,$flds[-2]);
      }
      if($flds[-2] >= 0.3) { # if occ >0.3 then counted as a good site
         $ngoodsite++;
      }
    }

    my $nsites=@occ_sites;
    my @diffocc;
    my $avgdiff=0.0; # average difference between two neighbour occ
    my $max=-1;
    my $maxposition; # maximum value in the array and its position
    for (my $i=0;$i<$nsites-1;$i++) {
       my $diff=$occ_sites[$i]-$occ_sites[$i+1];
       if ($diff >$max) {
          $max=$diff;
          $maxposition=$i;
       }
       $avgdiff=$avgdiff+$diff;
       push(@diffocc,$diff);
    }
    $avgdiff=($avgdiff-$max)/($nsites-1.0); # avg diff occ is calculated without the largest number

    @diffocc=sort {$b <=> $a} @diffocc;
    # normalize difference occupancy array
    @diffocc=map $_/$avgdiff,@diffocc;
    #print Dumper \@diffocc;
    # print "maxdiff $max,maxposition=$maxposition\n";
    # measure how the site are distributed using occupancy
    # for the sorted array, sitehealth=diff between 1st best and 2nd best
    my $sitehealth=0;
    my $nsites_optimal=$maxposition+1;
    if ($nsites>2) {
      $sitehealth = $diffocc[0]-$diffocc[1];
    }
    # using number of sites and found sites to adjust the site score
    $sitehealth=($sitehealth*$nsites_optimal)/$nsites;
    $sitehealth=sprintf("%.2f",$sitehealth);
    #print "sitehealth= $sitehealth good sites=$nsites_optimal\n";
    if ($sitehealth <3.0) { # if the distribution of occ is not indicative, then use >0.3 occ sites
       print "Since the occ distibution score is not indicative, use occ>=0.3 site instead;\n"; 
       $nsites_optimal=$ngoodsite;
    }

    return ($CCall, $CCweak,$xyz,$sitehealth,$nsites_optimal);
}

sub parseSHELXE { 
    # open a file and extract the following, pseudo CC, connectivity and contrast
    # parseSHELXE("e.log","name.hat");
    my $elogfilename=$_[0];
    my $hatfilename=$_[1];
    open(ELOG,"$elogfilename");
    # $enantimorph_spg is used to keep track of whether /Space group converted to enantiomorph/ 
    # when HA hand is reversed
    my ($lastline, $pCC,$connectivity,$contrast,$enantimorph_spg);
    $enantimorph_spg=0;
    while ( <ELOG> ) {
       if ( /Pseudo-free CC =\s+(\d+.\d+)/ ) {
            $pCC=$1;
       }
       if (/Contrast/) {
            $lastline=$_;
       }
       if (/Space group converted to enantiomorph/) {
            $enantimorph_spg=1;
       }
    }
    close(ELOG);

    # <wt> = 0.279, Contrast = 0.144, Connect. = 0.859 for dens.mod. cycle 20
    #$lastline =~ /Contrast =\s+(\d+.\d+),\s+Connect. =\s+(\d+.\d+)/;
    $lastline =~ /Contrast =\s+(\d+.\d+),\s+Connect. =\s+(\d+.\d+)/;
    $contrast=$1; $connectivity=$2;

    # now opens name.hat file to look at refined HA site
    # same format as .res file 
    open(HAT,"$hatfilename") || die "failed to open $hatfilename.\n";
    local $/="\f";
    my $text=<HAT>;
    close(HAT);
    my $s1="UNIT ";
    my $s2="HKLF ";
    my $xyz=extractLastBlock($text,$s1,$s2);

    my @sites=split(/\n/,$xyz);
    my $newxyz='';
    my $ngoodsite=0;
    foreach my $site (@sites) {
      my @flds=split(/\s+/,$site);
      if($flds[-2] >= 0.3) { # if occ >=0.3 then count as a good site
         $ngoodsite++;
         $newxyz=$newxyz.$site."\n";
      }
    }
    close(HAT);

    return ($pCC,$contrast,$connectivity,$enantimorph_spg,$ngoodsite,$newxyz);
}

sub runSHELX {
   # prepare shelx, this version will run all space group possibities
   # will run shelxcde in one place,
   # only useful when one is desparate?
   my $href_dataset=$_[0];
   my $resolution=$_[1];

   my @wavs=keys %{$href_dataset};
   my $nwav= @wavs;

   my ($cell,$integ_spg,$aref_pointless_spgs,$nsites,$fsolvent,$nmolasu); # 
   foreach my $w (keys %{$href_dataset} ) { # get needed numbers from first dataset
     $cell=$href_dataset->{$w}{cell};
     $integ_spg=$href_dataset->{$w}{spacegroup};
     $aref_pointless_spgs=$href_dataset->{$w}{pointless_spgs};
     $nsites=$href_dataset->{$w}{sitesASU};
     $nmolasu=$href_dataset->{$w}{nmolASU};
     if($nsites <1) {
         print "\n\n!!!WARNING: # of heavy atom sites equals to 0. Check your input.\n";
         print "\n\n   # of sites was set to default.\n";
         # better guess here needed, using asu content 
         $nsites=2;
     }
     $fsolvent=sprintf("%.2f",$href_dataset->{$w}{solvent}/100.0);
     last;
   }
   if ($fsolvent > 1.0 || $fsolvent < 0.01 ) {
     print "WARNING: solvent content incorrect: $fsolvent.\n\n";
     $fsolvent = 0.5;
   }

   my $CC_cutoff= $SHELXD_CUTOFF_MAD; # don't bother with CC lower then 40 with MAD case
   if ($nwav ==1 ) {
       $CC_cutoff=$SHELXD_CUTOFF_SAD; # reduce the threshold if SAD case
   }

   my $ntry=$SHELXD_CYCLES;
   my $shelxd_aux='';
   # if there are a lot of sites, increase # of shelxd cycles
   if ( $nsites >= 40 ) { # if too many sites, run shelxd slightly differently
      $ntry=$ntry*5; 
      $shelxd_aux="WEED 0.3\nSKIP 0.5\n";  
   }

   my @spgs=@{getspgs($integ_spg)};
   my $nspg = @spgs;
   my @pointless_spgs=@{$aref_pointless_spgs};
 
   my $wdir=cwd()."/shelx/";
   if (! -e $wdir ) { mkpath($wdir); }

   my $pm = new Parallel::ForkManager($NMACHINES2);
   #foreach my $xspg (@spgs) {
   my $cell_index=$cell;
   foreach my $xspg (@pointless_spgs) {
      $cell=$cell_index;
      #prepare reflection files for shelx
      #skip space groups rejected by pointless
      #my $find=0;
      #foreach my $p_spg (@pointless_spgs) {
      #   if ($xspg eq $p_spg) {
      #      $find=1;
      #      last;
      #   }
      #}
      #if ($find==0) {print "$xspg was not suggested by pointless, skipped.\n" };
      #next if $find==0; 

      # end of pointless section
      my @cells=split (/\s+/,$cell);

      my $iucrspg=$xspg;
      my $reindex='h, k, l';
      if($xspg eq 'p22121' or $xspg eq 'p2122') { # c a b
          $reindex='k,l,h';
          #$cell=$cells[2]." ".$cells[0]." ".$cells[1]." 90 90 90";
          $cell=$cells[1]." ".$cells[2]." ".$cells[0]." 90 90 90";

          $iucrspg='p2221';
          if($xspg eq 'p22121') { 
             $iucrspg='p21212';
          }
      } elsif ($xspg eq 'p21221' or $xspg eq 'p2212') { # b c a
          $reindex='l,h,k';
          #$cell=$cells[1]." ".$cells[2]." ".$cells[0]." 90 90 90";
          $cell=$cells[2]." ".$cells[0]." ".$cells[1]." 90 90 90";

          $iucrspg='p2221';
          if($xspg eq 'p21221') {
              $iucrspg='p21212';
          }
      }

      # the child does the following
      my $wdir=cwd()."/shelx/$xspg/";
      if (! -e $wdir ) { mkpath($wdir); }

      sleep $SLEEP;
      $pm->start and next;

      # foreach wavelength, run combat/mtzutils to generate shelxd files
      foreach my $w (keys %{$href_dataset} ) { 
          my $datafile=$href_dataset->{$w}{mtz_truncate};

          open(REINDEX,">$wdir/reindex-w$w.com");
print REINDEX <<EOF4;
#!/bin/csh 

# reindex swap cell parameters order, so no need to change cell explicitly
$REINDEX hklin $datafile hklout $wdir/w$w-reindex.mtz <<eof-reindex
reindex $reindex
symm $iucrspg
end
eof-reindex

$MTZ2VARIOUS hklin $wdir/w$w-reindex.mtz hklout $wdir/w$w.sca <<eof-mtzutils
OUTPUT SCAL
LABIN  I(+)=I(+) SIGI(+)=SIGI(+) I(-)=I(-) SIGI(-)=SIGI(-)
SCALE 0.1
end
eof-mtzutils
EOF4
          close(REINDEX); 

          chmod(0755,"$wdir/reindex-w$w.com");
          my $cmd2run="(cd $wdir; ./reindex-w$w.com) >$wdir/w$w-reindex.log";
          system($cmd2run);
      } # end of data conversion
      
      open(SHELX,">$wdir/$xspg-shelxcd.com");
      print SHELX "#!/bin/csh\n\ncd $wdir\n\n";
      print SHELX "$SHELXC $xspg <<EOF >$xspg-shelxc.log\n";
      my $peak=0; # keep track of what has already used
      my $lrem=0;
      my $hrem=0;
      my $infl=0;
      my %he=('PEAK'=>0,'INFL'=>0,'HREM'=>0,'LREM'=>0);
      foreach my $w (keys %{$href_dataset} ) { # what about there are two energy remotes? etc
          my $edge=$href_dataset->{$w}{energytag};
          $he{$edge}++;
          if( $he{$edge} >= 2 ) {
             print "WARNING: more than one $edge. Skipped wavelength $w.\n";
             next;
          }
          print SHELX "$edge w$w.sca\n";
      }
      print SHELX "SPAG $iucrspg\n";
      print SHELX "CELL $cell\n";
      print SHELX "FIND $nsites\n";
      if ($resolution > 1.0 ) { # run SHELX with different resolution cutoff
         print SHELX "SHEL 99 $resolution\n";
      }

      print SHELX "NTRY $ntry\n";
      print SHELX "EOF\n";
      my $tag=$xspg."_fa";
      print SHELX "$SHELXD $tag >$xspg-shelxd.log\n";
      close(SHELX);
      chmod(0755,"$wdir/$xspg-shelxcd.com");
      # starting all shelxc,d, e job all at once;

      print "\nStarting shelx jobs in $wdir for space group $xspg ...\n";
      system(" cd $wdir; ./$xspg-shelxcd.com ");
      
      print "\nDone shelx c/d jobs in $wdir for space group $xspg.\n";
      my ($CCall,$CCweak,$xyz,$sitehealth,$nsites_optimal)=parseSHELXD("$wdir/$tag.res"); 
      if ($CCall > $CC_cutoff) { # if CC is reasonable run SHELXE
          open(SHELXE,">$wdir/$xspg-shelxe.com");
          my $log_shelxe_i="$wdir/$xspg-shelxe_i.log";
          my $log_shelxe="$wdir/$xspg-shelxe.log";
          my $cmd2run1="$SHELXE $xspg $tag -i -s$fsolvent -h -l10 -b -m$SHELXE_CYCLES >$log_shelxe_i  & ";
          my $cmd2run2="$SHELXE $xspg $tag    -s$fsolvent -h -l10 -b -m$SHELXE_CYCLES > $log_shelxe   & ";
          my $cmd2run3="wait\n"; # this wait is used for waiting the above two background processes to finish
          print SHELXE "#!/bin/csh\n\ncd $wdir\n\n";
          print SHELXE "$cmd2run1\n";
          print SHELXE "sleep $SLEEP\n";
          print SHELXE "$cmd2run2\n";
          print SHELXE "$cmd2run3\n";
          close(SHELXE);
          chmod(0755,"$wdir/$xspg-shelxe.com");
          my $cmd2run="(cd $wdir; ./$xspg-shelxe.com) >/dev/null";
          system($cmd2run); # there is a wait in above code!
          print "\nDone shelx e job in $wdir for space group $xspg.\n";
      }

      $pm->finish; # Terminates the child process
   } # end of for each space group
   $pm->wait_all_children; # blocking wait, for all children to finish

   # parse shelx results for all spgs, not done in parallel
   my @shelx_result=();
   my ($CCall,$CCweak,$xyz,$sitehealth,$nsites_optimal);
   my ($pCC,$contrast,$connectivity,$enantimorph_spg);
   my ($pCCi,$contrasti,$connectivityi,$enantimorph_spgi);
   #my $CC_cutoff= $SHELXD_CUTOFF_MAD; # don't bother with CC lower then 40 with MAD case
   #if ($nwav ==1 ) {
   #    $CC_cutoff=$SHELXD_CUTOFF_SAD; # reduce the threshold if SAD case
   #}

   #foreach my $xspg (@spgs) {
   foreach my $xspg (@pointless_spgs) {
      my $wdir=cwd()."/shelx/$xspg/";

      print "\n\nShelx results for space group $xspg:\n";
      my @cells=split (/\s+/,$cell);
      my $iucrspg=$xspg;
      my $reindex='h, k, l';
      if($xspg eq 'p22121' or $xspg eq 'p2122') { # c a b
          $reindex='k,l,h';
          $cell=$cells[1]." ".$cells[2]." ".$cells[0]." 90 90 90";

          $iucrspg='p2221';
          if($xspg eq 'p22121') {
             $iucrspg='p21212';
          }
      } elsif ($xspg eq 'p21221' or $xspg eq 'p2212') { # b c a
          $reindex='l,h,k';
          $cell=$cells[2]." ".$cells[0]." ".$cells[1]." 90 90 90";

          $iucrspg='p2221';
          if($xspg eq 'p21221') {
              $iucrspg='p21212';
          }
      }

      my $tag=$xspg."_fa";
      ($CCall,$CCweak,$xyz,$sitehealth,$nsites_optimal)=parseSHELXD("$wdir/$tag.res"); 
      print "For spg $xspg, SHELXD best, CC(all)/CC(weak)=$CCall/$CCweak\n";
      print "SHELXD sites as:\n$xyz\n";
      print "The score for distribution of the sites is (the bigger the better): $sitehealth\n";
      my $ratio=($nsites_optimal+.0001)/$nsites;
      $ratio=sprintf("%.2f",$ratio);
      my $diffsite =$nsites_optimal-$nsites;
      print "SHELXD found $nsites_optimal sites vs expected $nsites sites, ratio=$ratio, difference=$diffsite\n";
      if ( abs($diffsite) <= 1  ) {
          print "Note: number of sites found match expected number of sites.\n";
      } elsif ( $diffsite >= 2 ) {
          print "Note: number of sites found more than expected number of sites, nmol is probably larger than expected.\n";
      } elsif ( $diffsite <= -2 ) {
          print "Note: number of sites found less than expected number of sites, nmol is probably smaller than expected.\n";
      }
      if ($CCall > $CC_cutoff) { # if CC is reasonable run SHELXE
          print "NOTE: A solution was likely found, based on shelxd CC values.\n";
          my $log_shelxe_i="$wdir/$xspg-shelxe_i.log";
          my $log_shelxe="$wdir/$xspg-shelxe.log";
          ($pCC,$contrast,$connectivity,$enantimorph_spg,$nsites_optimal,$xyz)=parseSHELXE($log_shelxe,"$wdir/$xspg".".hat"); # shelxe HA site supercede shelxd sites!
          my $pCCmod=$pCC+$sitehealth/10.0; # 10.0 is a random number 
          push @shelx_result, [$xspg, $CCall,$CCweak,$xyz,$pCCmod,$contrast,$connectivity,$enantimorph_spg,$iucrspg,$reindex,'',$nsites_optimal,$cell];
          ($pCCi,$contrasti,$connectivityi,$enantimorph_spgi,$nsites_optimal,$xyz)=parseSHELXE($log_shelxe_i,"$wdir/$xspg"."_i.hat");
          my $pCCimod=$pCCi+$sitehealth/10.0;
          if ( $enantimorph_spgi != 1 ) {  
             # only selects choices with no enantimorph swap, deal with cases with p3121/p3221 etc
             push @shelx_result, [$xspg, $CCall,$CCweak,$xyz,$pCCimod,$contrasti,$connectivityi,$enantimorph_spgi,$iucrspg,$reindex,'_i',$nsites_optimal,$cell];
          }
      }
   } # end of for each space group parsing
 
   my $n=scalar @shelx_result;
   if ( $n==0 ) { # no good solution 
      print "WARNING: No good solution found using shelxd.\n"; 
      print "         What to do? -- future work: different wavelength cobo, # of sites searched etc.\n";
      return 0;
   }
   
   # evaluation all candidates, sorting two dimensional array, using pCC and then CCall decreasing order high-->low
   my @sorted_list=sort {$b->[4] <=> $a->[4] || $b->[1] <=> $a->[1]} @shelx_result;

   # now pick the best one
   my $savedspg=$sorted_list[0][0];
   my $savedcell=$sorted_list[0][12];
   my $savedxyz=$sorted_list[0][3];
   my $savediucrspg=$sorted_list[0][8];
   my $savedreindex=$sorted_list[0][9];
   my $nshelxdsites=$sorted_list[0][11]; # number of sites SHELX find

   # revisit nmolASU, solvent, based on shelxd result
   print "\n\nrevisit nmol per ASU and solvent content estimation based on SHELXD results:\n";
   my $diff=abs($nsites-$nshelxdsites);
   my $new_nmolasu=$nmolasu;
   my $new_solvent=$fsolvent*100.0;
   if ($diff>2) { # only consider nmol/solvent change if there enough discrepancy between shelxd and estimate
      my $sites_per_mol=$nsites/$nmolasu;
      if  ($sites_per_mol == 1 ) { $sites_per_mol=2; }
      $new_nmolasu=$nshelxdsites/($sites_per_mol-1);
      $new_nmolasu= int($new_nmolasu+0.45); # round to nearest integer, or should add 1 instead ???
      my $vol_per_mol = (100.0-$new_solvent)/$nmolasu; # this is how many percent of volume supposed to be occupied by a mol.
      $new_solvent = (100.0-$new_nmolasu*$vol_per_mol);
      $new_solvent = sprintf("%.2f",$new_solvent);
      # need reevaluate Vm as well!
      print "updated number of mol per asu as $new_nmolasu, updated solvent content as: $new_solvent\n";
   } 

   # update dataset with SHELX information
   foreach my $w (keys %{$href_dataset} ) {
     $href_dataset->{$w}{spacegroup}=$savediucrspg;
     $href_dataset->{$w}{reindex}=$savedreindex;
     $href_dataset->{$w}{HAsites}=$savedxyz;
     $href_dataset->{$w}{cell_reindexed}=$cell;
     $href_dataset->{$w}{nmolASU}=$new_nmolasu;
     $href_dataset->{$w}{solvent}=$new_solvent;
   }
  
   my $tag_inv=$sorted_list[0][10]; # two values: '' or '_i'
   # save results into PhaseSet
   my $final_shelx_dir=cwd()."/shelx/$savedspg/";
   $href_sharpPhaseSet->{dir_shelx}=$final_shelx_dir;
   #
   $href_sharpPhaseSet->{phs_shelxe}="$final_shelx_dir/$savedspg"."$tag_inv.phs";
   $href_sharpPhaseSet->{log_shelxe}="$final_shelx_dir/$savedspg-shelxe$tag_inv.log";
   #
   $href_sharpPhaseSet->{log_shelxc}="$final_shelx_dir/$savedspg-shelxc.log";
   $href_sharpPhaseSet->{log_shelxd}="$final_shelx_dir/$savedspg-shelxd.log";
   $href_sharpPhaseSet->{res_shelxd}="$final_shelx_dir/$savedspg"."_fa.res";
   $href_sharpPhaseSet->{pdb_shelxd}="$final_shelx_dir/$savedspg"."_fa.pdb";
   $href_sharpPhaseSet->{CCall_shelxd}=$sorted_list[0][1];
   $href_sharpPhaseSet->{CCweak_shelxd}=$sorted_list[0][2];
   $href_sharpPhaseSet->{CC_shelxe}=$sorted_list[0][4];
   $href_sharpPhaseSet->{contrast_shelxe}=$sorted_list[0][5];
   $href_sharpPhaseSet->{connect_shelxe}=$sorted_list[0][6];
   $href_sharpPhaseSet->{spacegroup}=$savediucrspg;

   print "\n\nShelx summary: Shelxd CC(all)/CC(weak)=", $sorted_list[0][1], "/",$sorted_list[0][2],"\n";
   print "Shelx summary: Shelxe pseudo CC=", $sorted_list[0][4], " Contrast ",$sorted_list[0][5], " connect. ",$sorted_list[0][6],"\n";
   print "Shelx summary: Space group selected: $savedspg \n";
   print "Shelx summary: HA sites by SHELXD and verified by shelxe:\n$savedxyz \n";
   print "Shelx summary: All detailed SHELX results in directory $final_shelx_dir\n\n";
   # return $href_dataset;

   # now run shelxe free luch algorithm (FLA) and warp in background, so do a quick resolve pass
   # need to optimize the solvent in the future to get better results!

   print "Now executing shelxe free lunch algorithm in background in $final_shelx_dir.\n";
   print "Caution: The results may be only useful if the resolution is between 1.6-2.0A according to the manual.\n";
   my $cwdir=$final_shelx_dir;
   copy("$cwdir/$savedspg"."$tag_inv.phs", "$cwdir/$savedspg"."$tag_inv.phs.saved"); # save a copy of 1st pass shelxe phases
   my $shelxe_phs="$cwdir/$savedspg"."$tag_inv.phs"; # final shelxe phase file to be converted to mtz
   my $shelxe_to_use="$SHELXE";
   if ($tag_inv eq "_i") {
      $shelxe_to_use="$SHELXE -i"; 
   }
   my $tag_fa=$savedspg."_fa";
   my $new_fsolvent=sprintf("%.4f",$new_solvent/100.0);

   print "Now getting trace using shelxe map with resolve (quickie mode) in $cwdir/quick_resolve.\n";
   # quick resolve run using shelxe map
   my $qresolvedir="$cwdir/quick_resolve";
   if(! -e "$qresolvedir") {
       mkpath($qresolvedir);
   }
   copy("$dir_info/seq.raw","$cwdir/seq.dat");
   open(QR,">$qresolvedir/qresolve.csh");
   print QR <<EOFQR;
#!/bin/csh 

cd $qresolvedir

cp ../$tag_fa.pdb ha.pdb

# convert the resulting mtz file to mtz for use with resolve tracing program
$F2MTZ hklin $shelxe_phs.saved  HKLOUT shelxe_qr.mtz <<eof
TITLE ATTENTION: note SIGFP is fake (i.e. not real SIGFP) #SYMM $savedspg
CELL $savedcell
SYMM $savediucrspg
LABOUT H   K  L   FP  FOMDM PHIDM SIGFP
CTYPE  H   H  H   F   W     P     Q       
eof

$RESOLVE <<eof
seq_file seq.raw
hklin shelxe_qr.mtz
ha_file ha.pdb
superquick_build
resolution  2.0 200
labin FP=FP PHIB=PHIDM FOM=FOMDM
eof
EOFQR
   close(QR);
   chmod(0755, "$qresolvedir/qresolve.csh");
   my $cmd2run="cd $qresolvedir; ./qresolve.csh >$qresolvedir/qresolve.log";
   system("$cmd2run &");
   print "NOTE 1: Quick resolve job submitted to background, check you results in 30min in $cwdir/quick_resolve.\n";
   print "NOTE 2: You can also load the map $cwdir/quick_resolve/shelxe_qr.mtz right now to see how your map looks.\n\n";

   # now run shelxe free luch algorithm (FLA) and warp in background, so do a quick resolve pass
   # need to optimize the solvent in the future to get better results!

   print "Now executing shelxe free lunch algorithm in background in $final_shelx_dir/FLA.\n";
   print "Caution: The results may be only useful if the resolution is between 1.6-2.0A according to the manual. EXPERIMENTAL!\n";

   my $shelxe_fla_dir="$cwdir/FLA";
   if(! -e "$shelxe_fla_dir") {
       mkpath($shelxe_fla_dir);
   }

   # prepare wARP files
   my $nres=$href_target->{nAA};
   my $asucontent= $nres*$new_nmolasu;
   copy("$dir_info/seq.pir","$shelxe_fla_dir/seq.pir");
   #my $arpcmd="$ARP workdir $shelxe_fla_dir datafile $shelxe_fla_dir/shelxe_fla.mtz seqin $shelxe_fla_dir/seq.pir ncs $new_nmolasu residues $asucontent fp FP sigfp SIGFP phibest PHIDM fom FOMDM";
   my $arpcmd="$ARP workdir $shelxe_fla_dir datafile $shelxe_fla_dir/shelxe_fla.mtz seqin $shelxe_fla_dir/seq.pir cgr $new_nmolasu residues $asucontent fp FP sigfp SIGFP phibest PHIDM fom FOMDM";

   open(FLA,">$shelxe_fla_dir/fla.csh");
   print FLA <<EOFFLA;
#!/bin/csh 

source /home/sdcsoftware/linux/arp_warp_7.2/arpwarp_setup.csh
cp ../*.hkl .
cp ../*.res .

cd $shelxe_fla_dir

# run shelxe free lunch algorithm
# may only useful if the data is between 1.6-2.0A
$shelxe_to_use $savedspg $tag_fa  -s$new_fsolvent -e1.0 -b -h  -l10 -m$SHELXE_FLA_CYCLES

# convert the resulting mtz file to mtz for use with wARP tracing program
$F2MTZ hklin $shelxe_phs HKLOUT shelxe_fla.mtz <<eof
TITLE ATTENTION: note SIGFP is fake (i.e. not real SIGFP) #SYMM $savedspg
CELL $savedcell
SYMM $savediucrspg
LABOUT H   K  L   FP  FOMDM PHIDM SIGFP
CTYPE  H   H  H   F   W     P     Q       
eof

$arpcmd

EOFFLA
  close(FLA);

  print " Creating fla.csh in $shelxe_fla_dir ...\n";
  # run time-consuming iterative resolve
  print "\n run time consuming shelxe free lunch algorithm followed by wARP in $shelxe_fla_dir.\n";
  print " Job started in background.\n\n";
  chmod(0755, "$shelxe_fla_dir/fla.csh");
  $cmd2run="cd $shelxe_fla_dir; ./fla.csh";
  
  return 1;
}

sub updateCollectionfromCollectDataXML {
   # update Collection hash from Xsolve CollectData.xml
   my $aref_collect=$_[0];
   my $xmldata=$_[1];

   my $data_ref=$xmldata->{CollectData}->{DataList}->{Data};
  
   #print Dumper $data_ref;
   #exit;

   foreach my $href ( @{$aref_collect}) {
      next if ($href->{process}==0);

      my $start=sprintf("%4d",$href->{startseq});
      my $end=  sprintf("%4d",$href->{endseq});
      my $imgprefix=$href->{imgprefix};

      my $match=0;
      foreach my $el (@$data_ref) {
        my $xstart=sprintf("%4d", $el->{FrameNumber}->{start});
        my $xend  =sprintf("%4d", $el->{FrameNumber}->{end});
        if($start == $xstart && $end == $xend && $imgprefix eq $el->{ImagePrefix} ) {
           $href->{fp}=$el->{Anomalous}->{fprimv};
           $href->{fpp}=$el->{Anomalous}->{fprprv};
           $href->{xsolve_label}=$el->{Description};
           $href->{distance}=$el->{Distance};
           $href->{wavelength}=$el->{Wavelength};
           $href->{wavelength4}=sprintf("%.4f",$el->{Wavelength}); 
           $match=1;
           last;
        }
        #print "xsolve_label ",$el->{Description},"\n";
        #print "fp ",$el->{Anomalous}->{fprimv},"\n";
        #print "fpp ",$el->{Anomalous}->{fprprv},"\n";
        #print "prefix ",$el->{ImagePrefix},"\n";
        #print "wavelength ",$el->{Wavelength},"\n";
        #print "distance ",$el->{Distance},"\n";
        #print "\n\n";
      }
      if($match==0) {
         print "Warning: run with image from ", $href->{first_image}, " to ", $href->{first_image},  " does not have a match in xsolve CollectData.xml\n";
         $href->{process}=0; # this run will be ignored in the following processing
      }
   }

   return $aref_collect;
}

sub updateCollectionfromUserInput1 {
   # update Collection hash from user input, this allows user input of prior knowledge
   my $aref_collect=$_[0];
   my $href_userInputs=$_[1];

   if (exists $href_userInputs->{XDScommands} )  {
      my @cmds=split /;/, $href_userInputs->{XDScommands};
      print "NOTE: user has specified additional XDS commands as follows:\n";
      my $xdscmds=join("\n",@cmds);
      print $xdscmds,"\n";
      foreach my $href ( @{$aref_collect}) {
        $href->{XDScommands}=$xdscmds; 
      }
   }

   if (exists $href_userInputs->{highres} )  {
      print "NOTE: user has specified a high resol cutoff as (applied to all runs): ", $href_userInputs->{highres}, "A.\n";
      foreach my $href ( @{$aref_collect}) {
        $href->{highres}=$href_userInputs->{highres}; 
      }
   }
   
   return $aref_collect;
}

sub updateCollectionfromUserInput2 { 
   my $aref_collect=$_[0];
   my $href_userInputs=$_[1];
   my %wavs=();

   my %HAedges = (
      'U'=>17165.3,  #  92 L3 w=0.72227
      'Y'=>17038.2,  #  39 K w=0.72766
      'SR'=>16106.9, #  38 K w=0.76973
      'RB'=>15202.2, #  37 K w=0.81554
      'KR'=>14324.3, #  36 K w=0.86552
      'BR'=>13470.2, #  35 K w=0.92040
      'BI'=>13426.5, #  83 L3 w=0.92340
      'PB'=>13040.5, #  82 L3 w=0.95073
      'SE'=>12654.4, #  34 K w=0.97974
      'HG'=>12286.2, #  80 L3 w=1.00910
      'AU'=>11921.2, #  79 L3 w=1.04000
      'AS'=>11864.1, #  33 K w=1.04500
      'PT'=>11562.1, #  78 L3 w=1.07230
      'IR'=>11211.8, #  77 L3 w=1.10580
      'GE'=>11103.5, #  32 K w=1.11658
      'OS'=>10867.8, #  76 L3 w=1.14080
      'RE'=>10530.9, #  75 L3 w=1.17730
      'GA'=>10368,   #  31 K w=1.19580
      'W'=>10199.9,  #  74 L3 w=1.21550
      'TA'=>9876.52, #  73 L3 w=1.25530
      'ZN'=>9660.28, #  30 K w=1.28340
      'HF'=>9557.51, #  72 L3 w=1.29720
      'LU'=>9248.79, #  71 L3 w=1.34050
      'CU'=>8980.22, #  29 K w=1.38059
      'YB'=>8943.88, #  70 L3 w=1.38620
      'TM'=>8649.37, #  69 L3 w=1.43340
      'ER'=>8357.26, #  68 L3 w=1.48350
      'NI'=>8331.6,  #  28 K w=1.48807
      'HO'=>8067.41, #  67 L3 w=1.53680
      'DY'=>7789.65, #  66 L3 w=1.59160
      'CO'=>7709.48, #  27 K w=1.60815
      'TB'=>7515.31, #  65 L3 w=1.64970
      'GD'=>7243.09, #  64 L3 w=1.71170
      'FE'=>7111.15, #  26 K w=1.74346
      'EU'=>6980.46, #  63 L3 w=1.77610
      'SM'=>6717.23  #  62 L3 w=1.84570
   );

   my $hatype='SE';
   if( exists $href_userInputs->{heavyatom} ) {
     $hatype=uc($href_userInputs->{heavyatom});
   }
   
   my $haedge=12654.4;
   if (exists $HAedges{$hatype} ) { $haedge=$HAedges{$hatype}; }

   foreach my $href ( @{$aref_collect}) {
       my $wav4=$href->{wavelength4};
       $wavs{$wav4} ++;
   }

   my @a_wavs=keys %wavs;
   my $nwav= @a_wavs;

   my $e_infl=99999;
   my $Erange=26.0;
   my $belowEdge=$haedge-$Erange;
   my $aboveEdge=$haedge+$Erange;
   my $nedges=0; # count number of energies around the edge
   foreach my $el (@a_wavs) {
      my $energy=12398.00/$el;
      if ($energy > $belowEdge && $energy < $aboveEdge ) {
         $nedges++;
         if ($energy < $e_infl ) {
            $e_infl=$energy;
         }
      }
   }

   if ($nedges==0) {
      print "Warning: No data were collected around the wedge $haedge for heavy atom $hatype.\n";
      print "         Make sure that heavy atom type is correct.\n";
   } elsif ($nedges==1) {
      print "Warning: only 1 dataset inside the edge (expecting 2), assuming inflection point by default.\n";
      print "         use -peak=fp,fpp to override the default.\n";
   }

   ############ NOTE: only works for Se #############################
   # update f' and f"
   foreach my $href ( @{$aref_collect}) {
        next if ($href->{process}==0);

        # only one wavelength
        if ($nwav==1 ) {
          $href->{energytag}="SAD";
          if( $href->{fpp} < 0.05 ) {
             $href->{fp}=-7.5;
             $href->{fpp}=5.5;
          }
          last;
        } 
       
        # >1 wavelength, assuming MAD
        my $w=$href->{wavelength4};
        my $energy=12398.00/$w;
        if ($energy > $aboveEdge) {
          $href->{energytag}="HREM";
          if( $href->{fpp} < 0.05 ) {
            $href->{fp}=-1.8;
            $href->{fpp}=3.4;
          }
        } elsif ($energy < $belowEdge ) {
          $href->{energytag}="LREM";
          if( $href->{fpp} < 0.05 ) {
             $href->{fp}=-3.4;
             $href->{fpp}=0.5;
          }
        } elsif (abs($energy-$e_infl) < 0.01 && !(exists $href_userInputs->{ispeak})  ) {
          $href->{energytag}="INFL";
          if (exists $href_userInputs->{inflection} ) {
              my $in=$href_userInputs->{inflection};
              my @arr=split /,/, $in;
              $href->{fp}=$arr[0];
              $href->{fpp}=$arr[1];
          } else {
            if( $href->{fpp} < 0.05 ) {
               $href->{fp}=-10.0;
               $href->{fpp}=3.0;
            }
          }
        } else {
          $href->{energytag}="PEAK";
          if (exists $href_userInputs->{peak} ) {
              my $in=$href_userInputs->{peak};
              my @arr=split /,/, $in;
              $href->{fp}=$arr[0];
              $href->{fpp}=$arr[1];
          } else {
            if( $href->{fpp} < 0.05 ) {
              $href->{fp}=-7.5;
              $href->{fpp}=5.5;
            }
          }
        }
   } # end of for each wavelength
   return $aref_collect;
}

sub updateCollectionfromUserInput3 { 
   my $aref_collect=$_[0];
   my $href_userInputs=$_[1];
   my $href_target=$_[2];
   my %wavs=();

   my $cell;
   my $spg;
   foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $wav4=$href->{wavelength4};
       my $integ_ok = $href->{integration_status};
       if($integ_ok) {
          $wavs{$wav4} ++;
          $cell=$href->{cell};
          $spg =$href->{integ_spg};
       }
   }
   my @a_wavs=keys %wavs;
   my $nwav= @a_wavs;

   ## calculate nmol per asu automatically
   my $molw=$href_target->{'MW'};
   my ($nmolASU,$Vm,$solvent);

   if (exists $href_userInputs->{nmolASU} )  {
       $nmolASU=$href_userInputs->{nmolASU};
       ($nmolASU,$Vm,$solvent)=cal_matthews_Vm($spg,$cell,$molw,$nmolASU);
   } else {
       print "Automatic guessing how many mols per asu.\n";
       ($nmolASU,$Vm,$solvent)=cal_matthews_Vm($spg,$cell,$molw,'AUTO');
       print "Based on matthews_coeff, the following parameters were estimated as: \n"; 
       print "Vm              = $Vm\n"; 
       print "solvent content = $solvent\n"; 
       print "nmol per asu    = $nmolASU\n"; 
       print "Note: This values are estimated from molecular weight of: $molw\n"; 
   }

   foreach my $href ( @{$aref_collect}) {
      $href->{nmolASU}=$nmolASU;
      $href->{solvent}=$solvent;
      $href->{Vm}=$Vm;
      $href->{sitesASU}=$nmolASU*$href_target->{nHA};
   }

   # now made a final pass of all runs to decide whether a run 
   # should be include into the MAD scaled data set
   #  use only correlation to reference as cutoff standard, needs improve
   foreach my $href ( @{$aref_collect}) {
      next if ($href->{process}==0);

      my $corr =  $href->{correlation} ;
      if ($corr < $CUTOFF_CORRELATION_MAD ) {
        print "Warning: run for ", $href->{first_image}, " will not be include in MAD set.\n";
        print "         since its correlation to reference ($RESOL4SPG) $corr < $CUTOFF_CORRELATION_MAD \n";
        $href->{inMADset}=0;
      }
   }

   # reject runs with really bad Rmerge
   foreach my $href ( @{$aref_collect}) {
      next if ($href->{process}==0);

      my $rmerge =  $href->{rmerge} ;
      if ($rmerge > $CUTOFF_RMERGE_MAD ) {
        print "Warning: run for ", $href->{first_image}, " be cut to $RESOL4SPG A to be included into MAD set (experimental).\n";
        print "         since its Rmerge $rmerge > $CUTOFF_RMERGE_MAD \n";
        # $href->{inMADset}=0;         # if rmerge is too bad, reject it from MAD set or
        $href->{resol2sigma}= $RESOL4SPG;     # cut it so only low resolution data is used ???
        $href->{resolxsigma}= $RESOL4SPG;     # cut it so only low resolution data is used ???
      }
   }

   # now check to make sure not all remote wavelength are thrown out
   if ($nwav==1) { return $aref_collect; }

   my $i=0;
   foreach my $href ( @{$aref_collect}) {
      next if ($href->{process}==0);

      my $energytag = $href->{energytag};
      if ($energytag eq 'HREM' || $energytag eq 'LREM' ) {
        if ( $href->{inMADset} != 0 ) { $i++; }
      }
   }

   if ($i==0) { # if no remo in MAD, make sure it has one; temporary solution, any better idea?
      print "Since all remote are rejected, needs to include remote...\n";
      foreach my $href ( @{$aref_collect}) {
         next if ($href->{process}==0);

         my $energytag = $href->{energytag};
         if ($energytag eq 'HREM' || $energytag eq 'LREM' ) {
             print "include ", $href->{first_image}, " set into MAD scaled set ...\n";
             $href->{inMADset}=1;
         }
      }
   }

   return $aref_collect;
}

sub usage {
   
print <<EOF;
    
    autoXDS Yet Another Automatic Protein Structure Solver 
            using XDS, SHELX, autoSHARP, wARP and RESOLVE 
 
    usage: autoXDS expects data in a subdirectory named ./images.
           If no such directory exists, it can create links 
           to raw images with -data=/path_to_images.
           Ideally, a sequence file is provided. Data 
           processing will run regardless a sequence is given.
           For jcsg target, sequence can be automatically
           retrieved from the web with the target name.
             
           It assumes that the data is at least good to $RESOL4SPG A 
           resolution. But it may be still useful for lower
           resolution data (such as setting up directories
           and creating XDS.INP etc). It supports only ADSC and MARCCD 
           detectors and image header must be correct in default mode! 

           IMPORTANT NOTICE: It is your responsibility to check
           the validity of the result. This tool only mean to make 
           your life easier.
           
           examples:

           autoXDS -data=...  -seq=...
           autoXDS -seq=... 
           autoXDS -jcsg=TM0415 -data= ...
          
           -help 
              print this screen
 
           -xml=/full_path/CollectData.xml
              with this option, all STSS uploads will be created in directory STSS 
              catagorized by index.xml

           -data=/path_to_raw_image_directory
              specifies the location(s) of x-ray images, you can use
              -data multiple times to specify images at more than one place
              as long as they have the same unit cell and space group.
                                                                                                                                          
           -seq=sequence_file
              specifies the protein sequence file

           -noanomalous  
              turns off anomalous, processes native data
 
           -fast  
              integrates everything in p1, useful for one batch with 
              lots of images (save time),

           -ignore
              this may be useful if you want ignore images with filenames 
              matching supplied pattern(s), can use this flag more than once.
              eg. -ignore "22533_[234]" will ignore all images containg
              22533_2,22533_3,22533_4.
           
           -nmolASU=integer 
              defines number of molecules per asu

           -heavyatom=string
              defines heavy atom type, e.g. Fe

           -nsites=integer
              numbers of heavy atom sites per molecule 

           -peak=fp,fpp
           -inflection=fp,fpp
              defines fp and fpp values for peak and inflection

           -ispeak
              if two wavelengths were collected and one of
              them is near edge, then it will be assumed to be
              inflection by default, you can use -ispeak
              to override this behaviour.

           -collectinfo  
              just creates links/info and store in COLLECT.NFO, 
              USEFUL if you want to modify/set parameters 
              individually for each batch, just edit COLLECT.NFO
              and rerun with -recover

           -recover
              allows failure recovery/rerun, there are five
              recover points: before integration (COLLECT.NFO), 
              after integration (INTEG.NFO), after scaling 
              (DATASETS.NFO), before sharp (MAD.NFO) and before
              tracing (PHASE.NFO).

              Ex. 1, parameters collected from images header
              are incorrect, you can first generate COLLECT.NFO 
              with -collectinfo, edit it and rerun with -recover. 
                    
              Ex. 2, if you want to exclude certain batches 
              of images from scaling/phasing, you can editting 
              parameter inMADset in INTEG.NFO. 
            
              IMPORTANT: recovery is file-dependent, in order
              to recover at correct stage, all NFO files after
              recovery point must be deleted. For example, you 
              want to rescale/phasing, DATASETS.NFO and subsequent 
              .NFOs needs to be deleted.
            
           -highres=float
              applies high resolution cutoff to integration

           -beam=float,float
              set beam center manually (use MOSFLM/HKL beam center)

           -XDScommands=s 
              additional XDS.INP commands for user more commands 
              must be separated by ; e.g. -XDScommands="SEPMIN=1.0"

           -jcsg=jcsg_target_number
              for JCSG only, works only on machines with LWP and on internet
            

           The following parameters can be tweaked if necessary (rarely needed):

           -parallel=i
              number of machines to use for XDS integration (default=2)


           -rsym (default 20.0)
              laue group selection, data were scaled/merged to $RESOL4SPG
              from high to lower laue group, if rmerge is less rsym, then 
              current Laue group will be selected.

           -CCreindex (default 0.9)
              each sweep of data is compared to the reference sweep, 
              if Correlation is less than CCreindex, reindex will be 
              tried, the default works fine in most cases

           -CCmad (default 40.0)
           -CCsad (default 35.0)
              SHELXD correlation values for selection correct solutions
              the default works fine in most cases (MAD or SAD)

           Credits: please reference these packages below
                    for any useful information generated by 
                    this program

           -XDS by  Wolfgang Kabsch
           -SHELX by George Sheldrick
           -SHARP/autoSHARP by Global Phasing Ltd. 
           -ARP/wARP by Victor S. Lamzin and Anastassis Perrakis
           -CCP4 by CCP4 project (SCALA etc)
           -Resolve by Tom Terwilliger
           -Parrot/DM/Buccaneer by Kevin Cowtan

EOF

}

sub printSummary { # print summary of parsed data 
    my $aref_collect=$_[0];
    print "                                   SUMMARY OF DATA TO BE PROCESSED                  \n";
    print "========================================================================================================================\n";
    printf "%10s%20s%20s%7s%10s%10s%30s\n", "runid","first_image","last_image","total"," wavelength","distance","processing directory";
    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $pdir=$href->{processing_directory};
       my $nimg=$href->{totalimages};
       my $fimg=$href->{first_image};
       my $limg=$href->{last_image};
       my $runi=$href->{runid};
       my $wave=$href->{wavelength};
       my $dist=$href->{distance};
       printf "%10s%25s%25s%7d%10.4f%10.1f %30s\n",$runi,$fimg,$limg,$nimg,$wave,$dist,$pdir;
    }
    print "========================================================================================================================\n\n";
    return;
}

sub createSummary { # $href_dataset,$href_MADdataset,$href_target
   my ($MADcell,$rspg,$reindex);
   my $anomalous1='off'; my $anomalous2='no';
   if ($ANOMALOUS) {
      $anomalous1='on'; $anomalous2='yes';
      foreach my $w (keys %{$href_MADdataset} ) { # get needed numbers from first dataset
        $rspg=$href_MADdataset->{$w}{spacegroup}; #use symbol for mtz file, determined by SHELXD/SHELXE
        $reindex=$href_MADdataset->{$w}{reindex}; #
        $MADcell=$href_MADdataset->{$w}{cell}; #
        last;
      }
   }

   my $wdir=cwd()."/datasummary";
   if (! -e $wdir) { mkpath($wdir); }
   print "All data generated are under directory: $wdir, tagged by its wavelength in the filename.\n";
   my $pm = new Parallel::ForkManager($NMACHINES2);
   foreach my $w (keys %{$href_dataset} ) {
      # save locations of the files
      $href_dataset->{$w}{mtz_truncate}="$wdir/w$w/w$w-truncate.mtz";
      $href_dataset->{$w}{log_truncate}="$wdir/w$w/w$w-truncate.log";
      $href_dataset->{$w}{mtz_scala}="$wdir/w$w/w$w-scala.mtz";
      $href_dataset->{$w}{log_scala}="$wdir/w$w/w$w-scala.log";
      $href_dataset->{$w}{log_xscale}="$wdir/w$w/w$w-XSCALE.log";
      $href_dataset->{$w}{sca_unmerged}="$wdir/w$w/w$w.sca";
      $href_dataset->{$w}{dataset_name}="w$w";

      sleep $SLEEP;
      $pm->start and next; # do the fork      

      # start conversion, each wavelength in its own directory
      $wdir=$wdir."/w$w";
      if(! -e $wdir) {
         mkpath($wdir);
      } 
      my $dir=$href_dataset->{$w}{scaledir};
      if ( -e "$dir/XSCALE.LP") {
         copy("$dir/XSCALE.LP","$wdir/w$w-XSCALE.log");
      } else {
         print "Warning: cannot find $dir/XSCALE.LP.\n";
      }
      my $xtalid=$href_dataset->{$w}{crystalid};
      open(DTMP,">$wdir/w$w.ahkl") || die "Error: cannot open $wdir/w$w.ahkl for writing: $!.\n";
      open(HKLDATA,"$dir/w$w-nomerge.ahkl") || die "Error: cannot open $dir/w$w-nomerge.ahkl: $!.\n"; ; # data must be unmerged
      my $cell=$href_dataset->{$w}{cell};
      if(!$ANOMALOUS) { # if native, there will be no MADdataset, so use default values of spg and reindex
         $rspg=$href_dataset->{$w}{spacegroup};
         $reindex=$href_dataset->{$w}{reindex};
      }
      my $maxI=-1;
      while(<HKLDATA>) {
         next if /^\!/;
         my @a=split /\s+/; my $n= @a; next if $n!=11;
         my $batch=int(1000*$a[10]+$a[8]+0.5);
         printf DTMP "%6d%6d%6d%6d%15.1f%15.1f\n", $a[1],$a[2],$a[3],$batch,$a[4],$a[5];
         if($a[4]>$maxI) { $maxI=$a[4]; } # maximum intensity
      }
      close(DTMP); 
      close(HKLDATA);
       
      my $scale=sprintf("%.8f",500000.0/$maxI); # make sure it is not out of range in stupid SCALEPACK format f8.1
      if ($scale<0.0) {
         print "Scale is negative in createSummary! Abort.\n";
         exit(1);
      }

      print "Converting jcsg uploads for w=$w using script $wdir/jcsg-w$w.com ...\n";
      open(JCSG,">$wdir/jcsg-w$w.com");

      print JCSG <<EOF;
#!/bin/csh -f

cd $wdir

set dir = $wdir
set combat = $COMBAT
set reindex = $REINDEX
set scala = $SCALA
set truncate = $TRUNCATE
set sortmtz = $SORTMTZ 

\${combat} HKLIN \${dir}/w$w.ahkl HKLOUT \${dir}/tmp1.mtz <<eof-combat >\${dir}/w$w-combat.log
title [No title given]
symmetry $rspg
cell $cell # use orginal integration cell
scale $scale
wavelength $w
input USER
format '(4F6.0,2F15.1)'
label H K L BATCH I SIGI
NAME PROJECT $PROJECT CRYSTAL $xtalid DATASET w$w
end
eof-combat

\${reindex} hklin \${dir}/tmp1.mtz hklout \${dir}/tmp.mtz <<eof-reindex >\${dir}/w$w-reindex.log
symm $rspg
reindex $reindex
eof-reindex

\${sortmtz} hklin \${dir}/tmp.mtz hklout \${dir}/sort.mtz <<eof-sortmtz >\${dir}/w$w-sortmtzlog
H K L M/ISYM BATCH I SIGI
eof-sortmtz

# run scala to get SCALEPACK POLISH UNMERGED ORIGINAL
\${scala} hklin \${dir}/sort.mtz hklout \${dir}/w$w-scala.mtz <<eof-scala >\${dir}/w$w-scala0.log
run 1 batch 1 to 99999
title w=$w, processed by XDS/XSCALE 
scales constant
anomalous off
SDCORRECTION NOADJUST
output POLISH UNMERGED ORIGINAL
eof-scala

# rerun to generate mtz file for truncation
\${scala} hklin \${dir}/sort.mtz hklout \${dir}/w$w-scala.mtz <<eof-scala >\${dir}/w$w-scala.log
run 1 batch 1 to 99999
title w=$w, processed by XDS/XSCALE 
scales constant
SDCORRECTION NOADJUST
anomalous off
eof-scala

\${truncate} HKLIN \${dir}/w$w-scala.mtz HKLOUT \${dir}/w$w-truncate.mtz <<eof-trunc >\${dir}/w$w-truncate.log
anomalous yes
TRUNCATE YES
falloff yes
noharvest
eof-trunc

EOF
      close(JCSG);
      chmod(0755,"$wdir/jcsg-w$w.com");
      system(" cd $wdir; ./jcsg-w$w.com >  $wdir/jcsg-w$w.log");
      move("$wdir/SCALEPACK","$wdir/w$w.sca");
      #system("gzip $wdir/w$w.sca"); # compress file 
      unlink("$wdir/sort.mtz","$wdir/tmp.mtz","$wdir/tmp1.mtz");

      $pm->finish; # do the exit in the child process
   } # end of a scaling cycle
   $pm->wait_all_children;
   return;
}

sub spotrange2 {
   # return a list of possible spot ranges 
   # given: start number, end num and osc range and an option on how to choose spot range
   # wedge, choose images for IDXREF by wedge, pick images every wedge
   # it is not perfect and may have bugs!!!!
   # print spotrange2(1,21,0.5,22.5);
   my $start=$_[0];
   my $end=$_[1];
   my $osc=$_[2];

   my $binsize=$SPOTRANGE; #3.0 degree per bin
   # for fine slicing data, decrease the size of the bin
   if ($osc<0.4) { $binsize=5*$osc; }

   my $totalosc=($end-$start)*$osc;
   my $totalimg=$end-$start+1;

   my $wedge=22.5; # degree, pick 3 degree of data every 22.5 degree
   # attempt to determine sampling size automatically
   if ($totalosc > 90.0 ) {
      $wedge= int($totalosc/10);
   } else {
      $wedge= int($totalosc/6);
   }

   if ($wedge<2*$binsize) {$wedge=2*$binsize;}

   my $framesperbin=int($binsize/$osc); # divide into bins of 3 degree each

   my $nwedge=int($totalosc/$wedge+0.5);
   my $nimgperwedge=int($wedge/$osc);
  
   my $spotrange;
   if ($totalosc < 20.0) {
      $spotrange="SPOT_RANGE= $start $end\n";
      return $spotrange;
   }

   my $count=0;
   for (my $i=0; $i<=$nwedge; $i++) {
      my $x1=$start+$i*$nimgperwedge;
      my $x2=$x1+$framesperbin;
      if ($x1 >= $end) { $x1=$end-$framesperbin};
      if ($x2 >= $end) { $x2=$end; }
      $spotrange=$spotrange."SPOT_RANGE= $x1 $x2\n";
      $count++; if ($count>=10) {last;} # xds only use 10 spot ranges, so no need to go further
   }
   
   return $spotrange;
}

##
sub parseXSCALE {
  my $cwdir=$_[0]; # figure out which XSCALE.LP to parse
  print "Analysing $cwdir/XSCALE.LP ...\n";
  open(XSCALE,"$cwdir/XSCALE.LP") || die "ERROR in parseXSCALE: cannot open $cwdir/XSCALE.LP: $!";
  local $/="\f";
  my $log=<XSCALE>;
  close(XSCALE); 
  my $str1='SUBSET OF INTENSITY DATA WITH SIGNAL/NOISE >= -3.0 AS FUNCTION OF RESOLUTION';
  #my $str2='SUBSET OF INTENSITY DATA WITH SIGNAL/NOISE >= -2.0 AS FUNCTION OF RESOLUTION';
  my $str2='========== STATISTICS OF INPUT DATA SET ==========';
    
  my $last_block=extractLastBlock($log,$str1,$str2);
  my @lines=split(/\n/,$last_block);

  my @data_quality; # array of array storing extracted table
  print "                                 SUMMARY OF DATA SET STATISTICS\n";
  print " *********************************************************************************************************************************\n";
  print "  RESOLUTION     NUMBER OF REFLECTIONS    COMPLETENESS R-FACTOR  R-FACTOR COMPARED I/SIGMA   R-meas  CC(1/2)  Anomal  SigAno   Nano\n";
  print "    LIMIT     OBSERVED  UNIQUE  POSSIBLE     OF DATA   observed  expected                                      Corr                \n";

  foreach my $line (@lines) {
     next if ($line !~ m/\d+/);
     next if ($line =~ m/^\s+RES/);
     print $line, "\n";
     $line =~ s/^\s+//g;    # remove beginning empty spaces
     $line =~ s/\%//g;      # remove % sign
     push @data_quality, [ split(/\s+/,$line) ];
  };

  my $overall_rmerge=$data_quality[-1][5];
  my $outer_rmerge=$data_quality[-2][5];

  my $overall_completeness=$data_quality[-1][4];
  my $outshell_completeness=$data_quality[-2][4];

  my $nobs=$data_quality[-1][1];
  my $nuniqobs=$data_quality[-1][2];
  my $overall_redundancy = sprintf("%.2f",$nobs/$nuniqobs);
  my $outer_redundancy = sprintf("%.2f",$data_quality[-2][1]/$data_quality[-2][2]);

  my $meanios = $data_quality[-1][8]; # mean I over sigma I 
  my $outshell_ios = $data_quality[-2][8]; # mean I over sigma I for highest resol shell
  my $outshell_res = "$data_quality[-3][0]"."-$data_quality[-2][0]";
  
  print "\n\n SUMMARY of data for publication purpose\n";
  print " ================================================================\n";
  print " Rmerge overall/outer shell = $overall_rmerge/$outer_rmerge\n";
  print " Redundancy overall/outer shell = $overall_redundancy/$outer_redundancy\n";
  print " I/sigmaI mean overall/outer shell = $meanios/$outshell_ios\n";
  print " Number of obsevations all/unique = $nobs/$nuniqobs\n";
  print " Outer resolution shell is defined as $outshell_res (Angstroms)\n";
  print " ================================================================\n";

  return \@data_quality;

} # end of sub parseXSCALE

sub hasERROR {
   # given a XDS .LP log file check whether these is an ERROR near the end of the 
   # logfile, if yes, return 1, else return 0, 1 is returned if cannot open the file
   # pattern to look for: !!! ERROR !!!
   # ex:  print hasERROR("./IDXREF.LP");

   my $err="!!! ERROR !!!";
   my $logf=$_[0];

   open(LOGF,$logf) || return 1; # return -1 if the file does not exist
   my @lines;
   while (<LOGF>) {
      push @lines, $_;
   }
   close(LOGF);

   my $nlines= @lines;

   my @reversedlines= reverse @lines;
   foreach (my $i=0;$i<20;$i++) { # only check last 20 lines
      my $line=$reversedlines[$i];
      if ( $line =~ /$err/ ) {
         print "\nERROR: An ERROR was found near the end of $logf as follows:\n";
         print "       It is highly RECOMMENDED to check for what is wrong!\n";
         print "       Error msg: $line\n";
         return 1;
      }
   }

   return 0;
} # end of sub hasERROR

sub runIntegP1 {  # this sub will integrate every run in p1, !- experimental -!
    my @latt_spg=(
                # data structure, array or array
                # store space lattice number and their corresponding space group number, lattice
                # number is stored in index, for example, latt_spg[9] corresponding to
                # tetragonal spgs, an artificial lattice 0 was added such that
                # the count of lattice number can start from 1 
       [1], # lattice 0
       [1], # triclinic
       [3], # p2      
       [5], # c2
       [16], # p222
       [21], # c222 
       [22], # f222
       [23], # i222   
       [89,75], # p4 and p422
       [97,79], #i4,i422
       [177,168,150,149,143], # p3,p312,p321,p6,p622        
       [155,146], # r3 , r32  
       [207,195], # p23,p432
       [209,196], #f23,f432       
       [211,197] #i23,i432
    );

    my $aref_collect = $_[0];

    createXDS($aref_collect);
    my $nfailed = @$aref_collect; # maximum time it can fails its number of runs
    my ($soln,$nsolidx,$penalty);
    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $wdir=$href->{processing_directory};
       my $proot=$href->{processing_root};

       modifyXDS($wdir,"JOB="," XYCORR  INIT  COLSPOT  IDXREF"); # only run first part
       print "Indexing ", $href->{first_image}, " in $wdir... \n";
       print "Logfile in $wdir/index.log.\n";
       #runXDSprogs($wdir,"$XDS","index.log");
       system("cd $wdir; $XDS > $wdir/index.log");
       my ($nsolidx,$aref_indexlist)=parseIDXREF($wdir);
    
       hasERROR("$wdir/IDXREF.LP"); # check whether index performed OK
 
       print "Integration in reduce cell in space group p1 ", $href->{first_image}, " in $wdir...\n";
       print "Logfile in $wdir/integ.log.\n";
       modifyXDS($wdir,"JOB=","DEFPIX INTEGRATE"); # only run second part except CORRECT
       #runXDSprogs($wdir,"$XDS","integ.log");
       system("cd $wdir; $XDS >$wdir/integ.log ");

       if ( hasERROR("$wdir/INTEGRATE.LP") )  { 
          print "Warning: running runIntegP1 in p1 for run ", $href->{first_image}, " failed in $wdir.\n";
          print "         I will try next batch of images if possible.\n\n";
          $nfailed--;
          $href->{integration_status}=0;
          if ($nfailed==0) {
             print "Warning: processing for p1 failed for all runs.\n";
             return 0;
          }
          next;
       }
    }

    # figure out space group using a successful run
    my $idx_save;
    my ($rmerge, $correlation,$cell,$nrefl, $resol2sig,$resolxsig, $data_aref_quality);
    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $integ_ok = $href->{integration_status};
       my $wdir=$href->{processing_directory};
       my $proot=$href->{processing_root};
       if ($integ_ok) {
          $href->{MAD_referencetag}="*"; # this one will be used as reference for MAD scaling
          print "\nNow trying to determine space group for run ", $href->{first_image}, " in $wdir using data to $RESOL4SPG A.\n";
          my ($nsolidx,$aref_indexlist)=parseIDXREF($wdir);
          foreach my $soln (@$aref_indexlist) {
             my $clattice=$soln->[0];
             print "\n==>lattice number is ", $clattice,"\n"; 
             my @spg_list= @{$latt_spg[$clattice]};
             my $cells=join(" ", @{$soln}[2 .. 7]); # cell from index step
             my $reidx=join(" ", @{$soln}[8 .. 19]);
             print "cell= $cells reidx= $reidx\n";
             
             foreach my $spg (@spg_list) {
                 print "Testing scaling in space group $spg ...\n";
                 # update the current XDS.INP with above four strings 
                 modifyXDS($wdir,"UNIT_CELL_CONSTANTS=",$cells);
                 modifyXDS($wdir,"SPACE_GROUP_NUMBER=",$spg);
                 modifyXDS($wdir,"JOB=","CORRECT");
                 modifyXDS($wdir,"REIDX=",$reidx);
                 modifyXDS($wdir,"INCLUDE_RESOLUTION_RANGE=","$CUTOFF_LOWRESOL $RESOL4SPG");
    
                 # rerun XDS with CORRECT, check Rmerge etc
                 #runXDSprogs($wdir,"$XDS","spg-$spg.scale.log");
                 system("cd $wdir; $XDS> $wdir/spg-$spg.scale.log ");
                 if ( hasERROR("$wdir/CORRECT.LP") )  { 
                    print "ERROR: failed to run CORRECT while trying to test spg $spg. Abort.\n";
                    exit(0);
                 }
                 ($rmerge, $correlation,$cell,$nrefl, $resol2sig,$resolxsig, $data_aref_quality)=parseCORRECT($wdir); 
                 if($rmerge < $CUTOFF_RSYM) {
                    # move the resulting reflection file to images directory and used as further reference datasets
                    # this is necessary since XDS cannot deal with long file names
                    if (-f "$proot/images/REF.HKL" ) {
                       print "File $proot/images/REF.HKL already exists. Deleting.\n";
                       unlink("$proot/images/REF.HKL");
                    }
                    $idx_save=$reidx;
                    move("$wdir/XDS_ASCII.HKL","$proot/images/REF.HKL");
                    print "will accept the current spg # $spg, based on Rmerge.\n"; # found the correct space group
                    # update collect and return 
                    foreach my $xhref ( @{$aref_collect}) {
                       $xhref->{spacegroup}=$spg;
                       $xhref->{cell}=$cell; # use refined cell
                       $xhref->{integ_spg}=$spg;
                       $xhref->{reference_dataset}="../../images/REF.HKL";
                    }
                    goto CORRECT;
                 } else {
                    print "***reject current space group # $spg since Rmerge is >$CUTOFF_RSYM%.\n\n"
                 }  
             }
          }
       }
    }

    CORRECT:
    print "\n\nRunning XDS/CORRECT for every run successfully integrated.\n";
    createXDS($aref_collect); # rewrite XDS.INP and run CORRECT for every run
    foreach my $href ( @{$aref_collect}) {
       next if ($href->{process}==0);

       my $integ_ok = $href->{integration_status};
       my $wdir=$href->{processing_directory};
       print "Correcting ", $href->{first_image}, " in $wdir... \n";
       if ($integ_ok) {
          modifyXDS($wdir,"REIDX=",$idx_save);
          modifyXDS($wdir,"JOB=","CORRECT");
          modifyXDS($wdir,"INCLUDE_RESOLUTION_RANGE=","$CUTOFF_LOWRESOL 1.0");
          #runXDSprogs($wdir,"$XDS","correct.log");
          system(" cd $wdir; $XDS >$wdir/correct.log ");
          ($rmerge, $correlation,$cell,$nrefl, $resol2sig,$resolxsig, $data_aref_quality)=parseCORRECT($wdir); 
          if ( hasERROR("$wdir/CORRECT.LP") )  { 
               print "ERROR: Failed to run XDS/CORRECT in $wdir.\n";
               exit(0);
          }
          modifyXDS($wdir,"JOB=","IDXREF"); # run IDXREF to get possible reindex vectors
          #runXDSprogs($wdir,"$XDS","index2.log");
          system("cd $wdir; $XDS >$wdir/index2.log ");
          print "Correlation with reference data (-99 means no reference set given) =", $correlation, "\n";
          # now update collect for this run, this will be useful for following steps
          $href->{cell}=$cell; # try refined cell
          $href->{correlation}=$correlation;
          $href->{resol2sigma}=$resol2sig;
          $href->{resolxsigma}=$resolxsig;
          $href->{reflections}=$nrefl;
          $href->{rmerge}=$rmerge;
       }
    }
    return;
} # end of sub runIntegP1

sub checkprog { # check a whether a program program 
    my $prog=$_[0];
    my $pnam=$_[1];
    if (!-e $prog) {
      usage();
      print "\n\nProgram $pnam does not exist or not found. Quit.\n";
      exit(1);
    }
}

sub createResolveWarpBuccaneer {
  my $href_sharpPhaseSet=$_[0];
  my $href_target=$_[1];
  my $href_dataset=$_[2];

  # run BP3 based on sharp output
  #print "create BP3 files based on SHARP sites.\n";
  #createBP3($href_dataset,$href_target,$href_sharpPhaseSet);

  # other trace options
  my $mtzin=$href_sharpPhaseSet->{mtz_bestphs};
  my $hapdb=$href_sharpPhaseSet->{pdb_finalHA};

  if (! -e $mtzin ) {
     print "Phase file $mtzin does not exist. Aborting.\n";
     return;
  } 
  if (! -e $hapdb ) {
     print "Warning: heavy atom file $hapdb does not exist.\n";
     return;
  } 

  my $highres=10.0;
  my $fsolvent=0.50;
  my $nmolperasu = 1;
  foreach my $w (keys %{$href_dataset} ) { # get needed numbers from first dataset
     #print "===> $w solvent ", $href_dataset->{$w}{solvent},"\n";
     $fsolvent=sprintf("%.2f",$href_dataset->{$w}{solvent}/100.0);
     $nmolperasu= $href_dataset->{$w}{nmolASU};
     if ( $href_dataset->{$w}{resolxsigma} < $highres ) {
        $highres=$href_dataset->{$w}{resolxsigma}; # best common resolution to run IRP
     }
  }
  print "Solvent content used in resolve: $fsolvent\n";
  if ($fsolvent > 1.0 || $fsolvent < 0.01 ) {
     print "WARNING: solvent content incorrect: $fsolvent.\n\n";
     $fsolvent = 0.5;
  }

  if (! -e $IRPSCRIPT) {
     print "$IRPSCRIPT does not exist. Will not create irp.csh script. Aborting.\n";
     return;
  }
  ## Resolve procedure ###
  my $wdir=cwd()."/trace/irp";
  if (! -e $wdir) { mkpath($wdir); }
  # prepare sequence file
  my $seq=$href_target->{sequence};
  my $seqtag='seq';
  if ( length($seq)<10) {
      $seqtag='';
      print "Warning: no sequence given in RESOLVE.\n";
  } else {
      copy("$dir_info/$seqtag.raw","$wdir/seq.dat");
  }

  copy($mtzin,"$wdir/eden-unique.mtz");
  copy($hapdb,"$wdir/ha.pdb");

  local $/ ="\f";
  open(IRP,"$IRPSCRIPT") || die "error copy iterative resolve script $!"; 
  my $irpscript=<IRP>; 
  close(IRP);

  if($highres < 2.0) {  
     print " NOTE: resolve auto_build is run at 2.0A resolution despite true resolution is $highres.\n";
     $highres=2.0;
  }
  $irpscript =~ s/set\s+dmin\s+=.*/set dmin = $highres/;
  $irpscript =~ s/set\s+solvent_content\s+=.*/set solvent_content = $fsolvent/;

  print " Creating irp.csh in $wdir...\n";
  open(IRP,">$wdir/irp.csh"); print IRP $irpscript; close(IRP);
  chmod(0755,"$wdir/irp.csh");
  # run time-consuming iterative resolve
  print "\n run time consuming iterative resolve, \n";
  print "    i.e. resolve_autobuild script in directory $wdir.\n";
  print " Job started in background.\n\n";
  my $cmd2run1="cd $wdir; ./irp.csh >$wdir/irp-autoxds.log ";
  system("$cmd2run1 &");
##########################################
## PHENIX AUTOBUILD
  $wdir=cwd()."/trace/phenix.autobuild";
  if (! -e $wdir) { mkdir($wdir); }
  copy($mtzin,"$wdir/eden-unique.mtz");
  copy($hapdb,"$wdir/ha.pdb");
  copy("$dir_info/seq.raw","$wdir/seq.dat");
  #need to check program availability
  print "\n run phenix.autobuild in background at $wdir. \n";
  my $cmd2runx="cd $wdir; $PHENIXAUTOBUILD data=eden-unique.mtz seq_file=seq.dat input_ncs_file=ha.pdb >$wdir/phenix_autobuild.log";
  system("$cmd2runx &");
##########################################
## PARROT/BUCCANEER
  $wdir=cwd()."/trace/parrot-buccaneer";
  if (! -e $wdir) { mkdir($wdir); }
  copy($mtzin,"$wdir/eden-unique.mtz");
  copy($hapdb,"$wdir/ha.pdb");
  copy("$dir_info/seq.pir","$wdir/seq.pir");

  print " Creating parrot & buccaneer files in $wdir...\n";
  open(PBFILE,">$wdir/parrot-buccaneer.csh");
print PBFILE <<EOF;
#!/bin/csh 

$PARROT -stdin <<eofp > parrot.log
pdbin-ref /home/sdcsoftware/linux/ccp4-6.2.0/lib/data/reference_structures/reference-1tqw.pdb
mtzin-ref /home/sdcsoftware/linux/ccp4-6.2.0/lib/data/reference_structures/reference-1tqw.mtz
colin-ref-fo /*/*/[FP.F_sigF.F,FP.F_sigF.sigF]
colin-ref-hl /*/*/[FC.ABCD.A,FC.ABCD.B,FC.ABCD.C,FC.ABCD.D]
seqin-wrk seq.pir
mtzin-wrk eden-unique.mtz
colin-wrk-fo /*/*/[FP,SIGFP]
colin-wrk-hl /*/*/[HLA,HLB,HLC,HLD]
pdbin-wrk-ha ha.pdb
colout parrot
solvent-flatten
histogram-match
ncs-average
cycles 8
mtzout parrot.mtz
eofp

/home/sdcsoftware/linux/tools/bin/python  $BUCCANEERSCRIPT -stdin << eofbuc > buccaneer.log
seqin seq.pir
mtzin parrot.mtz
colin-fo FP,SIGFP
colin-hl parrot.ABCD.A,parrot.ABCD.B, parrot.ABCD.C,parrot.ABCD.D
colin-free FreeR_flag
cycles 5
jobs  $NPROCESSOR
eofbuc

cp buccaneer/refine.mtz buccaneer.mtz

EOF

  close(PBFILE);
  chmod(0755,"$wdir/parrot-buccaneer.csh");
  my $cmd2run2="cd $wdir; ./parrot-buccaneer.csh > $wdir/run-parrot-buccaneer.log";
  system("$cmd2run2");
  print " Tracing results:\n";
  my $buc=parseBUCCANEER($wdir); 
  $href_sharpPhaseSet->{traced_buccaneer}=$buc;
  
  print "Running warp based on buccaneer trace...\n";
  my $nres=$href_target->{nAA};
  my $asucontent= $nres*$nmolperasu;
  my $arpcmd;
  if (-e "$wdir/parrot.mtz" && -e "$wdir/buccaneer.pdb") {
    $arpcmd="$ARP workdir $wdir datafile $wdir/parrot.mtz seqin $dir_info/seq.pir cgr $nmolperasu residues $asucontent ";
    $arpcmd=$arpcmd."modelin $wdir/buccaneer.pdb fbest parrot.F_phi.F phibest parrot.F_phi.phi ";
    my $cmd2run3="cd $wdir; $arpcmd > $wdir/warp.log &";
    print "warp cmd: $cmd2run3.\n";
    #system("$cmd2run3");
    #my $warp=parseWARP($wdir);
    #$href_sharpPhaseSet->{traced_warp}=$warp; 
  } else {
    print " Warning: parrot-buccaneer failed!\n";
  }

  #############################
  ## DM and WARP procedures ###
  $wdir=cwd()."/trace/dm-warp";
  if (! -e $wdir) { mkdir($wdir); }
  copy($mtzin,"$wdir/eden-unique.mtz");

  # start DM+wARP
  print " Run dm and then wARP on dm results in directory $wdir.\n";
  $arpcmd="$ARP workdir $wdir datafile $wdir/dm.mtz seqin $dir_info/seq.pir cgr $nmolperasu residues $asucontent phibest PHIDM fom FOM";
  # prepare dm script, it is simple, don't use NCS, so may be get best results
  # should be sufficient for most of MAD cases
  open(DMFILE,">$wdir/dm-warp.csh");
print DMFILE <<EOF;
#!/bin/csh 

source /home/sdcsoftware/linux/arp_warp_7.2/arpwarp_setup.csh

$DMX HKLIN $wdir/eden-unique.mtz HKLOUT $wdir/dm.mtz <<eof > dm-autoxds.log
MODE SOLV HIST MULT
COMBINE PERT
NCYCLE AUTO
SCHEME ALL
SOLC $fsolvent
LABIN  FP=FP SIGFP=SIGFP PHIO=PHIB FOMO=FOM HLA=HLA HLB=HLB HLC=HLC HLD=HLD
LABOUT  FDM=FDM PHIDM=PHIDM FOMDM=FOMDM HLADM=HLADM HLBDM=HLBDM HLCDM=HLCDM HLDDM=HLDDM
noharvest
END
eof

$arpcmd >$wdir/warp.log
EOF
  close(DMFILE);
  chmod(0755,"$wdir/dm-warp.csh"); 
  my $cmd2run4="cd $wdir; ./dm-warp.csh >$wdir/run-dm-warp.log ";
  system("$cmd2run4 &");

  #############################################
  ## run buccaneer/warp on sharp solomon map
  $mtzin=$href_sharpPhaseSet->{mtz_solomon};
  $hapdb=$href_sharpPhaseSet->{pdb_finalHA};

  $wdir=cwd()."/trace/solomon-warp";
  if (! -e $wdir) { mkdir($wdir); }
  copy($mtzin,"$wdir/solomon.mtz");
  copy("$dir_info/seq.pir","$wdir/seq.pir");

  open(TRFILE,">$wdir/trace.csh");
print TRFILE <<EOF;
#!/bin/csh 

source /home/sdcsoftware/linux/arp_warp_7.2/arpwarp_setup.csh

/home/sdcsoftware/linux/tools/bin/python  $BUCCANEERSCRIPT -stdin << eofbuc > $wdir/buccaneer.log 
seqin seq.pir
mtzin solomon.mtz
colin-fo FPsha,SIGFPsha
colin-hl HLAshasol,HLBshasol,HLCshasol,HLDshasol
colin-free FreeR_flag
cycles 5
jobs  $NPROCESSOR
eofbuc

cp buccaneer/refine.mtz buccaneer.mtz

$ARP workdir $wdir datafile $wdir/solomon.mtz seqin $dir_info/seq.pir cgr $nmolperasu residues $asucontent fp FPsha sigfp SIGFP fbest FBshasol phibest PHIBshasol >warp.log
$ARP workdir $wdir datafile $wdir/solomon.mtz seqin $dir_info/seq.pir cgr $nmolperasu residues $asucontent fp FPsha sigfp SIGFP fbest FBshasol phibest PHIBshasol modelin $wdir/buccaneer.pdb >buccaneer-warp.log

EOF
  close(TRFILE);
  chmod(0755,"$wdir/trace.csh");
  my $cmd2run5="cd $wdir; ./trace.csh >$wdir/trace.log ";
  system("$cmd2run5 &");

  return;
}

sub createSTSS { # $href_dataset,$href_MADdataset,$href_sharpPhaseSet,$href_target

  # scaling and truncating for each individual wavelength
  print "\n\nSCALING AND TRUNCATION RESULTS FOR EACH INDIVIDUAL WAVELENGTH:\n";
  foreach my $w (keys %{$href_dataset} ) { 
     print "log file for lamda=$w scaling is: ",$href_dataset->{$w}{log_scala},"\n";
     print "mtz file for lamda=$w scaling is: ",$href_dataset->{$w}{mtz_scala},"\n";
     print "log file for lamda=$w truncation is: ",$href_dataset->{$w}{log_truncate},"\n";
     print "mtz file for lamda=$w truncation is: ",$href_dataset->{$w}{mtz_truncate},"\n";
     print "unmerged scalepack file for lamda=$w is: ",$href_dataset->{$w}{sca_unmerged},"\n";
  }


  print "\n\nSCALING AND TRUNCATION RESULTS FOR MULTIPLE WAVELENGTH:\n";
  # multiple wavelength version of scaling /truncating
  foreach my $w (keys %{$href_MADdataset} ) { # get needed numbers from first dataset
     print "log file for lamda=$w scaling is: ",$href_MADdataset->{$w}{log_scala},"\n";
     print "mtz file for lamda=$w scaling is: ",$href_MADdataset->{$w}{mtz_scala},"\n";
     print "log file for lamda=$w truncation is: ",$href_MADdataset->{$w}{log_truncate},"\n";
     print "mtz file for lamda=$w truncation is: ",$href_MADdataset->{$w}{mtz_truncate},"\n";
     print "unmerged scalepack file for lamda=$w is: ",$href_MADdataset->{$w}{sca_unmerged},"\n";
  }

  # shelxcde phasing and sharp phasing
  print "\n\nSHELX AND AUTOSHARP RESULTS:\n";
  print "\nSHELX:\n";
  print "Final SHELX directory: ",$href_sharpPhaseSet->{dir_shelx},"\n";
  print "Final SHELXD log: ",$href_sharpPhaseSet->{log_shelxd},"\n";
  print "Final SHELXE log: ",$href_sharpPhaseSet->{log_shelxe},"\n";
  print "Final SHELXE phases: ",$href_sharpPhaseSet->{phs_shelxe},"\n";
  print "Final SHELXD sites (pdb): ",$href_sharpPhaseSet->{pdb_shelxd},"\n";
  print "Final SHELXD sites (res): ",$href_sharpPhaseSet->{res_shelxd},"\n";
  print "\n\nSHARP:\n";
  print "Final pass of SHARP was carried out in ",$href_sharpPhaseSet->{dir_final},"\n";
  print "Final autoSHARP best experimental phases in mtz: ",$href_sharpPhaseSet->{mtz_bestphs},"\n";
  print "Final autoSHARP density modification mtz: ",$href_sharpPhaseSet->{mtz_solomon}, "\n";
  print "Final autoSHARP density modification log: ",$href_sharpPhaseSet->{log_solomon},"\n";
  print "Final autoSHARP refined HA sites:",$href_sharpPhaseSet->{pdb_finalHA},"\n";
  print "Final autoSHARP fom logfile: ", $href_sharpPhaseSet->{log_FOM}, "\n";
  print "Final autoSHARP phasing power logfile: ",$href_sharpPhaseSet->{log_PP},"\n";
  print "Final autoSHARP Rcullis logfile: ",$href_sharpPhaseSet->{log_Rcullis},"\n";
  print "Final autoSHARP sin file: ",$href_sharpPhaseSet->{sin_final},"\n";

  # extract autosharp summary into mmcif
 
  open(MMCIF,">sharp/autosharp_extract.csh");
  print MMCIF <<EOF;
#!/bin/csh

$PDB_EXTRACT -o autosharp.mmcif -e MAD -p SHELXD -iPDB $href_sharpPhaseSet->{pdb_shelxd} -ilog $href_sharpPhaseSet->{log_shelxd} -p SHARP -iPDB $href_sharpPhaseSet->{pdb_finalHA} -ilog $href_sharpPhaseSet->{log_FOM}  $href_sharpPhaseSet->{log_Rcullis} $href_sharpPhaseSet->{log_PP} $href_sharpPhaseSet->{sin_final}  >& pdb_extract.log
EOF
  close(MMCIF);
  my $pwd=cwd()."/sharp";
  chmod (0755,"$pwd/autosharp_extract.csh");
  print "Making sharp summary mmcif file. The mmcif file is called autosharp.mmcif under $pwd and can be treated as a brief sharp log file.\n";
  system("(cd $pwd; ./autosharp_extract.csh)>/dev/null");
  
  # create stss.xml for jcsgupload
  if (! exists $href_userInputs->{xml} ) { 
     return;
  }
  
  # date and crystalid
  my @d=split /\s+/, localtime();
  my $date=join("-", $d[2],$d[1],$d[-1]);
  my $xtalid;
  foreach my $href ( @{$aref_collect}) {
    $xtalid=$href->{crystalid};
    last;
  }

  my $wdir=cwd()."/STSS";
  if (! -e $wdir) { mkpath($wdir); }

  open(STSS,">$wdir/index.xml");

  print STSS "<STSS program='autoXDS' date='$date'>\n";
  print STSS "<ProcessingData target='$PROJECT' crystal='$xtalid'>\n";

  # collection section
  print STSS "<DataCollection>\n";
  print STSS "<File type='CollectData.xml' file='CollectData.xml'/>\n";
  print STSS "<File type='Beamline.xml' file='beamline.xml'/>\n";
  print STSS "<File type='Notes.txt' file='notes.txt'/>\n";
  print STSS "</DataCollection>\n\n";

  my $collectxml=$href_userInputs->{xml};
  my @p=split /\//,$collectxml;
  pop(@p);
  my $bicpath=join("/",@p);
  copy($collectxml,$wdir) or die "Warning: copy $collectxml failed.\n";
  copy("$bicpath/beamline.xml",$wdir) or die "Warning: copy $bicpath/beamline.xml failed.\n";
  copy("$bicpath/notes.txt",$wdir) or die "Warning: copy $bicpath/notes.txt failed.\n";

  # integrate section
  print STSS "<Integration program='XDS'>\n";
  my $iiset=0; # number of integration sets
  my $ref_wavelength=3.0;
  my $ref_wavelength4;
  foreach my $href ( @{$aref_collect}) {
      next if ($href->{process}==0); # not processed 
      next if ($href->{integration_status} == 0 ); # failed integration
      $iiset++;
      my $name=$href->{xsolve_label};
      if ($href->{wavelength} < $ref_wavelength) {
          $ref_wavelength=$href->{wavelength};
          $ref_wavelength4=$href->{wavelength4};
      }
      print STSS "<IntegrateSet number='$iiset' name='$name'>\n";
      print STSS "<Comments> block from ", $href->{first_image}, " to ",$href->{last_image},"</Comments>\n";
      print STSS "<File type='Integrate.log' file='$name-INTEGRATE.LP'/>\n";
      print STSS "<File type='Integrate.hkl' file='$name-INTEGRATE.HKL'/>\n";
      print STSS "</IntegrateSet>\n";
      my $pdir=$href->{processing_directory};
      copy("$pdir/INTEGRATE.LP","$wdir/$name-INTEGRATE.LP") or die "Warning: copy $pdir/INTEGRATE.LP failed.\n";
      copy("$pdir/INTEGRATE.HKL","$wdir/$name-INTEGRATE.HKL") or die "Warning: copy $pdir/INTEGRATE.HKL failed.\n";
  }
  print STSS "</Integration>\n\n";

  # scaling section
  my $isset=0;
  print STSS "<Scale program='XSCALE'>\n";
  print STSS "<NAT>\n";
  foreach my $w (keys %{$href_dataset} ) {
     copy($href_dataset->{$w}{log_xscale},$wdir);
     copy($href_dataset->{$w}{mtz_scala},$wdir);
     copy($href_dataset->{$w}{sca_unmerged},$wdir);
     $isset++; 
     print STSS "<ScaleSet number='$isset' name='w$w'>\n";
     print STSS "<File type='Scale.log' file='w$w-XSCALE.log'/>\n";
     print STSS "<File type='Scale.hkl' file='w$w-scala.mtz'/>\n";
     print STSS "<File type='ScaleUnmerged.hkl' file='w$w.sca'/>\n";
     # figure out integration sets used
     print STSS "<IntegrateSetUsedList>\n";
     foreach my $href ( @{$aref_collect}) {
        next if ($href->{process}==0); # not processed 
        next if ($href->{integration_status} == 0 ); # failed integration
        my $name=$href->{xsolve_label};
        my $imgprefix=$href->{imgprefix};
        my $w4=$href->{wavelength4};
        if ($w eq $w4) {
            print STSS "<IntegrateSetUsed name='$name' imgPrefix='$imgprefix' wavelength='$w4'/>\n";
        }
     }
     print STSS "</IntegrateSetUsedList>\n";
     print STSS "</ScaleSet>\n";
  }
  print STSS "</NAT>\n";

  # MAD scaling section
  if ( $ANOMALOUS ) { 
    print STSS "<MAD>\n";
    foreach my $w (keys %{$href_MADdataset} ) {
      copy($href_MADdataset->{$w}{log_xscale},"$wdir/MAD-w$w-XSCALE.log");
      copy($href_MADdataset->{$w}{mtz_scala},"$wdir/MAD-w$w.mtz");
      copy($href_MADdataset->{$w}{sca_unmerged},"$wdir/MAD-w$w.sca");
      $isset++;
      print STSS "<ScaleSet number='$isset' name='MAD-w$w'>\n";
      print STSS "<File type='Scale.log' file='MAD-w$w-XSCALE.log'/>\n";
      print STSS "<File type='Scale.hkl' file='MAD-w$w-scala.mtz'/>\n";
      print STSS "<File type='ScaleUnmerged.hkl' file='MAD-w$w.sca'/>\n";
      # figure out integration sets used
      print STSS "<IntegrateSetUsedList>\n";
      foreach my $href ( @{$aref_collect}) {
         next if ($href->{process}==0); # not processed 
         next if ($href->{integration_status} == 0 ); # failed integration
         my $name=$href->{xsolve_label};
         my $imgprefix=$href->{imgprefix};
         my $w4=$href->{wavelength4};
         if ($w eq $w4) {
            print STSS "<IntegrateSetUsed name='$name' imgPrefix='$imgprefix' wavelength='$w4'/>\n";
         }
      }
      print STSS "</IntegrateSetUsedList>\n";
      print STSS "</ScaleSet>\n";
    }
    print STSS "</MAD>\n\n";
  }
  print STSS "</Scale>\n\n";

  # truncate section
  my $itset=0;
  print STSS "<Truncate program='truncate'>\n";
  print STSS "<NAT>\n";
  foreach my $w (keys %{$href_dataset} ) {
     copy($href_dataset->{$w}{log_truncate},$wdir);
     copy($href_dataset->{$w}{mtz_truncate},$wdir);
     $itset++; 
     print STSS "<TruncateSet number='$itset' name='w$w'>\n";
     print STSS "<File type='Truncate.log' file='w$w-truncate.log'/>\n";
     print STSS "<File type='Truncate.hkl' file='w$w-truncate.mtz'/>\n";
     print STSS "<ScaleSetUsed name='w$w'/>\n";
     print STSS "</TruncateSet>\n";
  }
  print STSS "</NAT>\n";

  if ( !$ANOMALOUS ) { 
     print STSS "</Truncate>\n\n";
     print STSS "</ProcessingData>\n";
     print STSS "</STSS>\n";
     return;
  }

  # MAD truncate section
  print STSS "<MAD>\n";
  my $nwav=0;
  foreach my $w (keys %{$href_MADdataset} ) {
     copy($href_MADdataset->{$w}{log_truncate},"$wdir/MAD-w$w-truncate.log");
     copy($href_MADdataset->{$w}{mtz_truncate},"$wdir/MAD-w$w-truncate.mtz");
     $itset++;
     print STSS "<TruncateSet number='$itset' name='MAD-w$w'>\n";
     print STSS "<File type='Truncate.log' file='MAD-w$w-truncate.log'/>\n";
     print STSS "<File type='Truncate.hkl' file='MAD-w$w-truncate.mtz'/>\n";
     print STSS "<ScaleSetUsed name='w$w'/>\n";
     print STSS "</TruncateSet>\n";
     $nwav++;
  }
  print STSS "</MAD>\n\n";
  print STSS "</Truncate>\n\n";

  # phasing (shelxd/sharp) section
  copy("$wdir/../sharp/autosharp.mmcif",$wdir);
  copy($href_sharpPhaseSet->{pdb_finalHA},$wdir);
  copy($href_sharpPhaseSet->{mtz_bestphs},$wdir);
  print STSS "<Phasing program='SHELXD SHARP'>\n";
  print STSS "<PhaseSet number='1' name='shelx-autosharp'>\n";
  print STSS "<File type='Phase.log' file='autosharp.mmcif'/>\n";
  print STSS "<File type='Phase.hkl' file='eden-unique.mtz'/>\n";
  print STSS "<File type='Phase.pdb' file='hatom.pdb'/>\n";
  print STSS "<TruncateSetUsedList>\n";
  foreach my $w (keys %{$href_dataset} ) {
     print STSS "<TruncateSetUsed name='MAD-w$w'/>\n";
  }
  print STSS "</TruncateSetUsedList>\n";
  if($nwav>1) {
     print STSS "<PhasingMethod method='MAD' reference_wavelength='$ref_wavelength'/>\n";
  } else {
     print STSS "<PhasingMethod method='SAD'/>\n";
  }
  print STSS "</PhaseSet>\n";
  print STSS "</Phasing>\n\n";
  
  # dm section
  copy($href_sharpPhaseSet->{log_solomon},"$wdir/solomon_log.html");
  copy($href_sharpPhaseSet->{mtz_solomon},"$wdir/solomon.mtz");
  print STSS "<DensityModification program='autoSHARP solomon'>\n";
  print STSS "<DMSet number='1' name='autosharp-solomon'>\n";
  print STSS "<File type='DM.log' file='solomon_log.html'/>\n";
  print STSS "<File type='DM.hkl' file='solomon.mtz'/>\n";
  print STSS "<PhaseSetUsedList>\n";
  print STSS "   <PhaseSetUsed name='shelx-autosharp'/>\n";
  print STSS "</PhaseSetUsedList>\n";
  print STSS "<TruncateSetUsedList>\n";
  print STSS "   <TruncateSetUsed name='MAD-w$ref_wavelength4'/>\n";
  print STSS "</TruncateSetUsedList>\n";
  print STSS "</DMSet>\n";
  print STSS "</DensityModification>\n\n";

  # tracing section

  print STSS "</ProcessingData>\n";
  print STSS "</STSS>\n";
  close(STSS);

  return;
}

sub STOP {
   print "STOPPED. Signal caught: @_\n";
   kill('HUP', -$$);
   die;
}

sub parseCORRECT2 {
  my $cwdir=$_[0]; # figure out which CORRECT.LP to parse

  print "Analysing $cwdir/CORRECT.LP ...\n";
  open(CORRECT,"$cwdir/CORRECT.LP") || die "ERROR in parseCORRECT: cannot open $cwdir/CORRECT.LP: $!";


  local $/="\f";
  my $log=<CORRECT>;
  close(CORRECT); 

  my $str1='SELECTED SPACE GROUP AND UNIT CELL FOR THIS DATA SET';
  my $str2='REFINEMENT OF DIFFRACTION PARAMETERS USING ALL IMAGES';
  my $spg_block=extractLastBlock($log,$str1,$str2);
  $spg_block =~ /SPACE_GROUP_NUMBER=\s+(\d+)/;
  my $spg=$1;

  $str1='SUBSET OF INTENSITY DATA WITH SIGNAL/NOISE >= -3.0 AS FUNCTION OF RESOLUTION';
  #$str2='SUBSET OF INTENSITY DATA WITH SIGNAL/NOISE >=  0.0 AS FUNCTION OF RESOLUTION';
  $str2='NUMBER OF REFLECTIONS IN SELECTED SUBSET OF IMAGES';
    
  my $last_block=extractLastBlock($log,$str1,$str2);
  my @lines=split(/\n/,$last_block);

  my @data_quality; # array of array storing extracted table
  print "                                 SUMMARY OF DATA SET STATISTICS\n";
  print " ***********************************************************************************************************************************\n";
  print "  RESOLUTION     NUMBER OF REFLECTIONS    COMPLETENESS R-FACTOR  R-FACTOR COMPARED I/SIGMA   R-meas  CC(1/2)  Anomal  SigAno   Nano\n";
  print "    LIMIT     OBSERVED  UNIQUE  POSSIBLE     OF DATA   observed  expected                                      Corr                \n";

  foreach my $line (@lines) {
     next if ($line !~ m/\d+/);
     next if ($line =~ m/^\s+RE/);
     print $line, "\n";
     $line =~ s/^\s+//g;    # remove beginning empty spaces
     $line =~ s/\%//g;      # remove % sign
     $line =~ s/\*//g;      # remove * sign
     push @data_quality, [ split(/\s+/,$line) ];
  };

  my $rmerge=$data_quality[-1][5];
  print "Rmerge (overall) =", $rmerge, "\n";
 
  my $resol2sig=$data_quality[-2][0]; # set it to highest resolution possible
  for my $i ( 0 .. $#data_quality-1 ) {
     # print $data_quality[$i][8],"\n"; # i over sigma i is at column 8 right now
     if ( $data_quality[$i][8] - 1.5 < 0.01 ) {
       # $resol2sig=$data_quality[$i-1][0]; # option 1
       # $resol2sig=$data_quality[$i][0];   # option 2
       my $wc=$data_quality[$i][8];   # current weight, suggested by Mitch
       my $wp=$data_quality[$i-1][8]; # previous weight
       $resol2sig =  sprintf("%.2f",($wp*$data_quality[$i-1][0]+$wc*$data_quality[$i][0])/($wp+$wc) );
       last;
     }
  }
  print "Resolution @ at least 2.0 sigma  =", $resol2sig, "\n";

  # obtain a higher i/sigma cutoff resolution, may be this is better for MAD experiements
  #
  my $resolxsig=$data_quality[-2][0]; # set it to highest resolution possible
  for my $i ( 0 .. $#data_quality-1 ) {
     if ( $data_quality[$i][8] - $SIGMAMAD < 0.01 ) {
       # $resolxsig=$data_quality[$i-1][0];
       my $wc=$data_quality[$i][8];   # current weight, suggested by Mitch
       my $wp=$data_quality[$i-1][8]; # previous weight
       $resolxsig =  sprintf("%.2f",($wp*$data_quality[$i-1][0]+$wc*$data_quality[$i][0])/($wp+$wc) );
       last;
     }
  }
  print "Resolution @ at least x.0 sigma (x=$SIGMAMAD)  =", $resolxsig, "\n";

  # parse refined new cell after COLLECT.LP step
  $log =~ /UNIT CELL PARAMETERS\s+(.*)/;
  my $cell=$1; 
  print "refined new unit cell  =", $cell, "\n";

  # parse from number of reflections
  $log =~ /USING\s+(\d+)\s+INDEXED SPOTS/;
  my $nrefl=$1;
  print "number of indexed spots used to refine cell  =", $nrefl, "\n";
  
  # parse for correction between reference set if any
  $str1="CORRELATIONS BETWEEN INPUT DATA SETS AFTER CORRECTIONS";
  $str2="Factor applied to intensities";

  my $block=extractLastBlock($log,$str1,$str2);

  my $correlation=-99;
  if ( $block ) { # if block is not empty
    my @lines=split(/\n/,$block);

    foreach my $line (@lines) {
      next if ($line !~ m/\d+/);
      $line =~ s/^\s+//;    # remove beginning empty spaces
      my @arr=split(/\s+/,$line);
      $correlation=$arr[3];
      if ($correlation =~ m/^([+-]?)(?=\d|\.\d)\d*(\.\d*)?([Ee]([+-]?\d+))?$/) { # check if it is a real number
         last; # exit the loop if found a number
      }
    }
  }

  return ($spg,$rmerge,$correlation, $cell,$nrefl, $resol2sig,$resolxsig,\@data_quality);

} # end of sub parseCORRECT2

sub createXIA2 {
    my $ref_collect = $_[0];
    my $ref_target = $_[1];

    my $wdir=cwd."/xia";
    if (! -e $wdir) { mkpath($wdir); }
    open(XIA,">$wdir/xia2.info");
    printf XIA "BEGIN PROJECT $PROJECT\n";    

    my %xtalids;
    foreach my $href ( @{$aref_collect}) {
      my $xtalid=$href->{crystalid};
      $xtalids{$xtalid}=1;
    }
    if (keys(%xtalids) > 1) { 
        print "Note: more than 1 xtals\n"; 
    } 

    foreach my $xtal (keys %xtalids) {
       print XIA "BEGIN CRYSTAL $xtal\n";
       # seq
       my $seq=$ref_target->{sequence};
       print XIA "BEGIN AA_SEQUENCE\n";
       print XIA "$seq";
       print XIA "END AA_SEQUENCE\n\n";
       
       #ha info
       my $ha=uc($ref_target->{HAtype});
       print XIA "BEGIN HA_INFO\n";
       print XIA "ATOM $ha\n";
       my $nha=$ref_target->{nHA};
       print XIA "NUMBER_PER_MONOMER  $nha\n";
       print XIA "END HA_INFO\n";

       # wavelength 
       my $previousw4="";
       foreach my $href ( @{$aref_collect}) {
          if ( $href->{crystalid} eq $xtal ) {
             my $wav4=$href->{wavelength4};
             if ($wav4 ne $previousw4) {
                my $fp=$href->{fp};
                my $fpp=$href->{fpp};
                print XIA "BEGIN WAVELENGTH w$wav4\n";
                print XIA "WAVELENGTH $wav4\n";
                print XIA "F\' $fp\n";
                print XIA "F\" $fpp\n";
                print XIA "END WAVELENGTH w$wav4\n\n";
                $previousw4=$wav4;
             }
          }
       }

       # sweep info
       foreach my $href ( @{$aref_collect}) {
          if ( $href->{crystalid} eq $xtal ) {
             my $wav4=$href->{wavelength4};
             my $swid=$href->{runid};
             my $beamx=$href->{beamx};
             my $beamy=$href->{beamy};
             my $fimg=$href->{first_image};
             print XIA "BEGIN SWEEP $swid\n"; 
             print XIA "WAVELENGTH  w$wav4\n";
             print XIA "DIRECTORY ".cwd."/images\n";
             print XIA "BEAM $beamx $beamy\n";
             print XIA "IMAGE $fimg\n";
             print XIA "END SWEEP $swid\n\n";
          }
       }
       print XIA "END CRYSTAL $xtal\n";
    }

    printf XIA "END PROJECT $PROJECT";
    close(XIA);
}

sub updateSDCPROC {
    my $dataid=$_[0];
    my $href_sharpPhaseSet=$_[1];
    my $href_target=$_[2];
    my $href_dataset=$_[3];

    my $spg;
    my $nmolperasu;
    my $sitesASU;
    my $cell;
    my $xtalid;
    my $reso;
    foreach my $w (keys %{$href_dataset} ) { 
       $nmolperasu= $href_dataset->{$w}{nmolASU};
       $cell=$href_dataset->{$w}{cell};
       $spg=$href_dataset->{$w}{spacegroup};
       $sitesASU=$href_dataset->{$w}{sitesASU};
       $xtalid=$href_dataset->{$w}{crystalid};
       $reso=$href_dataset->{$w}{resolxsigma};
    }

    $spg=$href_sharpPhaseSet->{spacegroup};
    my $ccw=$href_sharpPhaseSet->{CCweak_shelxd};
    my $cc=$href_sharpPhaseSet->{CCall_shelxd};
    my $cce=$href_sharpPhaseSet->{CC_shelxe};
    my $warp=$href_sharpPhaseSet->{traced_warp};
    my $buc=$href_sharpPhaseSet->{traced_buccaneer};
    my $nHA=$href_target->{nHA};
    my $nAA=$href_target->{nAA};

    my $comment="autoXDSp results, automatically generated:\n";
    $comment=$comment."space group: $spg \n";
    $comment=$comment."cell: $cell\n";
    $comment=$comment."max resolution: $maxres\n"; # maxres is a global variable...
    $comment=$comment."No. HA per monomer: $nHA\n";
    $comment=$comment."No. of HA per asu: $sitesASU \n";
    $comment=$comment."Estimated monomer per asu: $nmolperasu\n";
    $comment=$comment."SHELXD CC(weak): $cc($ccw)\n";
    $comment=$comment."SHELXE CC: $cce\n";
    $comment=$comment."Residues per monomer: $nAA\n";
    $comment=$comment."Tracing results:\n";
    $comment=$comment."$buc\n";
    $comment=$comment."$warp\n";

    my $dbh = DBI->connect("dbi:Oracle:host=smbdb1;sid=SDC","jcsg","jcsg_sdc");
    my $update_handle = $dbh->prepare_cached("UPDATE prc_dataprocessing SET AUTOXDS= ? WHERE data_id = ?");
    die "Couldn't prepare queries; aborting" unless defined $update_handle;
    my $success = 1;
    $success &&= $update_handle->execute($comment,$dataid);
    mail2me($comment,$xtalid);
    return $success;
}

sub parseBUCCANEER {
  my $cwdir=$_[0]; # figure out which CORRECT.LP to parse

  #print "Analysing $cwdir/buccaneer.log ...\n";
  open(BUCLOG,"$cwdir/buccaneer.log ") || die "ERROR in parseBUCCANEER: cannot open $cwdir/buccaneer.log: $!";


  local $/="\f";
  my $log=<BUCLOG>;
  close(BUCLOG);

  my $count=0; #find last match 
  $count++ while $log =~ /(\d+)\s+residues were built in\s+(\d+)\s+chains, the longest having\s+(\d+)\s+residues/g;
  my $traced=$1;
  my $chains=$2;
  my $continuous=$3;
  $count++ while $log =~ /(\d+)\s+residues were sequenced, after pruning/g;
  my $docked=$1;
  $count++ while $log =~ /\s+R factor\s+(0\.\d+)\s+(0\.\d+)/g;
  my $rfact=$2;

  print "Buccaneer trace: ", $traced,"aa traced, in ",$chains," chains, with longest chain ",$continuous," aa, ",$docked," aa docked, rfact=",$rfact,"\n";
  my $bucresult="BUCCANEER trace: ".$traced."aa traced, in ".$chains." chains,\n  with longest chain ".$continuous." aa ".$docked." aa docked, rfact=".$rfact;
  #return $traced,$chains,$continuous,$docked,$rfact;
  return $bucresult;
}

sub parseWARP {
  my $cwdir=$_[0]; # figure out which CORRECT.LP to parse

  #print "Analysing $cwdir/warp.log ...\n";
  open(WARPLOG,"$cwdir/warp.log ") || die "ERROR in parseWARP: cannot open $cwdir/warp.log: $!";
  local $/="\f";
  my $log=<WARPLOG>;
  close(WARPLOG);

  my $warpresult="ARP/wARP failed.\n";
  my $count=0; #find last match 
  my $find;
  my $traced;
  my $chains;
  $count++ while  $find=$log =~ /Chains\s+(\d+), Residues\s+(\d+), Estimated correctness of the model/g;
  if ($find) {
    $traced=$2;
    $chains=$1;
  } else {
    print "ARP/wARP failed\n";
    return $warpresult;
  }
  $count++ while $log =~ /The longest chain comprises\s+(\d+)\s+peptides/g; # not perfect but close.
  my $continuous=$1;
  $count++ while $log =~ /(\d+)\s+chains \((\d+)\s+residues\) have been docked in sequence/g;
  my $docked=$2;
  $count++ while $log =~ /Cycle\s+\d+:  After refmac, R =\s+(0.\d+) \(Rfree/g;
  my $rfact=$1;

  print "ARP/wARP trace: ", $traced,"aa traced, in ",$chains," chains, with longest chain ",$continuous," aa ",$docked," aa docked, rfact=",$rfact,"\n";
  #return $traced,$chains,$continuous,$docked,$rfact;
  $warpresult="ARP/wARP trace: ".$traced."aa traced, in ".$chains." chains,\n  with longest chain ".$continuous." aa ".$docked." aa docked, rfact=".$rfact;
  #return $traced,$chains,$continuous,$docked,$rfact;
  return $warpresult;

}

sub mail2me { 
   my $message=$_[0];
   my $xtalid=$_[1]; 

   my $smtp = Net::SMTP->new('bl712.slac.stanford.edu');
   $smtp->mail('jcsg');
   $smtp->to('qxu@slac.stanford.edu');
   $smtp->data();
   $smtp->datasend("Subject: $PROJECT $xtalid solved");
   $smtp->datasend("\n");
   $smtp->datasend("$message\n");
   $smtp->dataend();
   $smtp->quit;
}

sub parsePOINTLESS {
  my $cwdir=$_[0]; # figure out which POINTLESS.LP to parse
  print "Analysing $cwdir/pointless.log ...\n";
  open(POINTLESS,"$cwdir/pointless.log") || die "ERROR in parsePOINTLESS: cannot open $cwdir/pointless.log: $!";
  if(! -e "$cwdir/pointless.log") {
    print "$cwdir/pointless.log does not exist.\n";
  }
  local $/="\f";
  my $log=<POINTLESS>;
  close(POINTLESS); 
  my $str1='   Spacegroup         TotProb SysAbsProb     Reindex         Conditions';
  #my $str2='Choosing between possible best groups:';
  my $str2='Selecting ';
    
  my $last_block=extractLastBlock($log,$str1,$str2);
  my @lines=split(/\n/,$last_block);

  my @pointlessspgs; # array of array storing extracted table
  print "Pointless suggests space groups: ";
  foreach my $line (@lines) {
     next if ($line !~ m/\d+/);
     #print $line, "\n";
     $line =~ /\s+\<(.*)\>\s+\((.*)\)\s+([01].\d+)/g;
     if(defined $3 && $3>0.05) {
        print "$1 with prob of $3, ";
        my $spg=$1;
        $spg =~ s/\s+//g;
        $spg =~ tr/[A-Z]/[a-z]/;
        if ($spg eq 'i121') { $spg='c2'; }
        if ($spg eq 'p1211') { $spg='p21'; }
        if ($spg eq 'p121') { $spg='p2'; }
        push @pointlessspgs, $spg;
     } 
  }
  print "\n";

  return \@pointlessspgs;

} # end of sub parsePOINTLESS

